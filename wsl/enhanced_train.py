#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆWSL Ray RLlibè®­ç»ƒè„šæœ¬
- å®Œæ•´çš„æ—¶é—´ç»Ÿè®¡å’Œé¢„æµ‹
- ç®€æ´çš„æ£€æŸ¥ç‚¹ä¿¡æ¯
- ç”¨æˆ·å‹å¥½çš„è¿›åº¦æ˜¾ç¤º
- å®æ—¶æ€§èƒ½ç›‘æ§
"""

import os
import sys
import time
import json
from pathlib import Path
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).parent.absolute()
project_root = current_dir.parent
sys.path.append(str(project_root))

# WSLç¯å¢ƒä¼˜åŒ–
os.environ['RAY_DISABLE_IMPORT_WARNING'] = '1'
os.environ['RAY_USAGE_STATS_ENABLED'] = '0'
os.environ['RAY_DEDUP_LOGS'] = '0'

import ray
from ray.rllib.algorithms.ppo import PPOConfig
from ray.tune.registry import register_env
from environments.w_factory_env import WFactoryGymEnv

class TrainingTimer:
    """è®­ç»ƒæ—¶é—´ç®¡ç†å™¨"""
    
    def __init__(self, total_iterations):
        self.start_time = time.time()
        self.total_iterations = total_iterations
        self.iteration_times = []
        self.current_iteration = 0
    
    def start_iteration(self):
        """å¼€å§‹ä¸€è½®è®­ç»ƒ"""
        self.iteration_start = time.time()
    
    def end_iteration(self):
        """ç»“æŸä¸€è½®è®­ç»ƒ"""
        duration = time.time() - self.iteration_start
        self.iteration_times.append(duration)
        self.current_iteration += 1
        return duration
    
    def get_stats(self):
        """è·å–æ—¶é—´ç»Ÿè®¡"""
        elapsed = time.time() - self.start_time
        
        if not self.iteration_times:
            return {
                'elapsed_time': elapsed,
                'avg_iteration_time': 0,
                'estimated_remaining': 0,
                'estimated_finish': None
            }
        
        avg_time = sum(self.iteration_times) / len(self.iteration_times)
        remaining_iterations = self.total_iterations - self.current_iteration
        estimated_remaining = remaining_iterations * avg_time
        estimated_finish = time.time() + estimated_remaining
        
        return {
            'elapsed_time': elapsed,
            'avg_iteration_time': avg_time,
            'estimated_remaining': estimated_remaining,
            'estimated_finish': estimated_finish,
            'fastest_iteration': min(self.iteration_times),
            'slowest_iteration': max(self.iteration_times),
            'current_iteration_time': self.iteration_times[-1] if self.iteration_times else 0
        }

def format_time(seconds):
    """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
    if seconds < 60:
        return f"{seconds:.1f}ç§’"
    elif seconds < 3600:
        return f"{seconds/60:.1f}åˆ†é’Ÿ"
    else:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        return f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ"

def create_enhanced_config():
    """åˆ›å»ºå¢å¼ºçš„è®­ç»ƒé…ç½®"""
    config = (
        PPOConfig()
        .environment(
            env="w_factory",
            env_config={"debug_level": "WARNING"}
        )
        .framework("torch")
        .api_stack(
            enable_rl_module_and_learner=False,
            enable_env_runner_and_connector_v2=False,
        )
        .training(
            train_batch_size=2000,
            minibatch_size=128,
            num_epochs=5,
            lr=5e-4,
            gamma=0.99,
            lambda_=0.95,
            clip_param=0.2,
            vf_clip_param=10.0,
            entropy_coeff=0.01,
            vf_loss_coeff=0.5,
        )
        .env_runners(
            num_env_runners=0,
            rollout_fragment_length=200,
        )
        .resources(
            num_gpus=0,
        )
        .multi_agent(
            policies={"shared_policy": (None, None, None, None)},
            policy_mapping_fn=lambda agent_id, *args, **kwargs: "shared_policy",
        )
        .debugging(
            log_level="WARNING",
        )
    )
    return config

def run_enhanced_training(num_iterations=10):
    """è¿è¡Œå¢å¼ºç‰ˆè®­ç»ƒ"""
    print("ğŸš€ å¢å¼ºç‰ˆMARLè®­ç»ƒç³»ç»Ÿ")
    print("=" * 80)
    
    # æ˜¾ç¤ºè®­ç»ƒå¼€å§‹ä¿¡æ¯
    start_datetime = datetime.now()
    print(f"ğŸ“… å¼€å§‹æ—¶é—´: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ è®­ç»ƒè½®æ¬¡: {num_iterations}")
    print(f"ğŸ“ ç»“æœç›®å½•: enhanced_results")
    
    # åˆå§‹åŒ–è®¡æ—¶å™¨
    timer = TrainingTimer(num_iterations)
    
    try:
        # åˆå§‹åŒ–Ray
        print("\nğŸ”§ åˆå§‹åŒ–Rayç¯å¢ƒ...")
        if not ray.is_initialized():
            ray.init(
                num_cpus=4,
                num_gpus=0,
                object_store_memory=1000000000,
                ignore_reinit_error=True,
                log_to_driver=False,
            )
        print("âœ… Rayåˆå§‹åŒ–å®Œæˆ")
        
        # æ³¨å†Œç¯å¢ƒ
        register_env("w_factory", lambda config: WFactoryGymEnv(config))
        print("âœ… ç¯å¢ƒæ³¨å†Œå®Œæˆ")
        
        # åˆ›å»ºç®—æ³•
        print("ğŸ§  æ„å»ºè®­ç»ƒç®—æ³•...")
        config = create_enhanced_config()
        algo = config.build()
        print("âœ… ç®—æ³•æ„å»ºå®Œæˆ")
        
        # åˆ›å»ºç»“æœç›®å½•
        results_dir = r"D:\MPU\æ¯•ä¸šè®ºæ–‡\MARL_FOR_W_Factory\wsl\ray_result"
        os.makedirs(results_dir, exist_ok=True)
        
        # è®­ç»ƒå˜é‡
        best_reward = float('-inf')
        best_checkpoint = None
        total_episodes = 0
        training_history = []
        
        print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ ({start_datetime.strftime('%H:%M:%S')})")
        print("=" * 80)
        
        # è®­ç»ƒå¾ªç¯
        for iteration in range(1, num_iterations + 1):
            timer.start_iteration()
            
            # æ‰§è¡Œè®­ç»ƒ
            result = algo.train()
            
            # è®°å½•æ—¶é—´
            iter_duration = timer.end_iteration()
            stats = timer.get_stats()
            
            # æå–æŒ‡æ ‡
            reward_mean = result.get("episode_reward_mean", 0)
            episodes_this_iter = result.get("episodes_this_iter", 0)
            episode_len_mean = result.get("episode_len_mean", 0)
            
            # å…¼å®¹Ray 2.48.0
            if episodes_this_iter == 0 and 'env_runners' in result:
                env_stats = result['env_runners']
                reward_mean = env_stats.get("episode_reward_mean", reward_mean)
                episodes_this_iter = env_stats.get("episodes_this_iter", episodes_this_iter)
                episode_len_mean = env_stats.get("episode_len_mean", episode_len_mean)
            
            total_episodes += episodes_this_iter
            
            # è®°å½•è®­ç»ƒå†å²
            training_history.append({
                'iteration': iteration,
                'reward': reward_mean,
                'episodes': episodes_this_iter,
                'duration': iter_duration,
                'timestamp': time.time()
            })
            
            # æ˜¾ç¤ºè¿›åº¦
            progress = iteration / num_iterations * 100
            progress_bar = "â–ˆ" * int(progress // 5) + "â–‘" * (20 - int(progress // 5))
            
            print(f"\nè½®æ¬¡ {iteration:2d}/{num_iterations} [{progress_bar}] {progress:5.1f}%")
            print(f"å¥–åŠ±: {reward_mean:8.2f} | Episodes: {episodes_this_iter:2d} | é•¿åº¦: {episode_len_mean:5.1f}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›
            is_best = reward_mean > best_reward
            if is_best:
                best_reward = reward_mean
                best_checkpoint = algo.save(results_dir)
                print(f"ğŸ‰ æ–°æœ€ä½³! å¥–åŠ±: {best_reward:.2f} | å·²ä¿å­˜æ£€æŸ¥ç‚¹")
            else:
                improvement = reward_mean - best_reward
                print(f"ğŸ“Š å½“å‰å¥–åŠ±ä¸æœ€ä½³å·®è·: {improvement:+6.2f}")
            
            # æ˜¾ç¤ºå­¦ä¹ ç»Ÿè®¡
            if "info" in result and "learner" in result["info"]:
                learner_info = result["info"]["learner"]["shared_policy"]
                if "learner_stats" in learner_info:
                    stats_info = learner_info["learner_stats"]
                    policy_loss = stats_info.get("policy_loss", 0)
                    vf_loss = stats_info.get("vf_loss", 0)
                    entropy = stats_info.get("entropy", 0)
                    print(f"æŸå¤± - ç­–ç•¥: {policy_loss:8.4f} | ä»·å€¼: {vf_loss:6.4f} | ç†µ: {entropy:6.4f}")
            
            # æ—¶é—´ç»Ÿè®¡
            print(f"æ—¶é—´ - æœ¬è½®: {format_time(iter_duration)} | å¹³å‡: {format_time(stats['avg_iteration_time'])}")
            print(f"è¿›åº¦ - å·²ç”¨: {format_time(stats['elapsed_time'])} | å‰©ä½™: {format_time(stats['estimated_remaining'])}")
            
            if stats['estimated_finish']:
                finish_time = datetime.fromtimestamp(stats['estimated_finish'])
                print(f"é¢„è®¡å®Œæˆæ—¶é—´: {finish_time.strftime('%H:%M:%S')}")
            
            print("-" * 80)
        
        # è®­ç»ƒå®Œæˆç»Ÿè®¡
        end_datetime = datetime.now()
        total_duration = (end_datetime - start_datetime).total_seconds()
        final_stats = timer.get_stats()
        
        print(f"\nğŸ è®­ç»ƒå®Œæˆ! ({end_datetime.strftime('%H:%M:%S')})")
        print("=" * 80)
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   æœ€ä½³å¥–åŠ±: {best_reward:.2f}")
        print(f"   æ€»Episodes: {total_episodes}")
        print(f"   æ€»ç”¨æ—¶: {format_time(total_duration)}")
        print(f"   å¹³å‡æ¯è½®: {format_time(final_stats['avg_iteration_time'])}")
        print(f"   æœ€å¿«å•è½®: {format_time(final_stats['fastest_iteration'])}")
        print(f"   æœ€æ…¢å•è½®: {format_time(final_stats['slowest_iteration'])}")
        
        # ä¿å­˜è®­ç»ƒæ‘˜è¦
        summary = {
            "training_info": {
                "start_time": start_datetime.isoformat(),
                "end_time": end_datetime.isoformat(),
                "total_duration": total_duration,
                "iterations": num_iterations
            },
            "results": {
                "best_reward": best_reward,
                "total_episodes": total_episodes,
                "avg_iteration_time": final_stats['avg_iteration_time'],
                "fastest_iteration": final_stats['fastest_iteration'],
                "slowest_iteration": final_stats['slowest_iteration']
            },
            "training_history": training_history,
            "checkpoint": str(best_checkpoint) if best_checkpoint else None
        }
        
        summary_file = f"{results_dir}/training_summary.json"
        with open(summary_file, "w", encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ è®­ç»ƒæ‘˜è¦å·²ä¿å­˜: {summary_file}")
        
        if best_checkpoint:
            print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹: {results_dir}/")
        
        # æ¸…ç†
        algo.stop()
        
        return summary
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    # è®°å½•è„šæœ¬å¼€å§‹æ—¶é—´
    script_start_time = time.time()
    script_start_datetime = datetime.now()
    
    print("ğŸ§ WSLå¢å¼ºç‰ˆMARLè®­ç»ƒç³»ç»Ÿ")
    print("ğŸ”§ ç‰¹æ€§: æ—¶é—´ç»Ÿè®¡ | è¿›åº¦é¢„æµ‹ | ç®€æ´è¾“å‡º | æ€§èƒ½ç›‘æ§")
    print(f"ğŸ• è„šæœ¬å¯åŠ¨æ—¶é—´: {script_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # è¯¢é—®ç”¨æˆ·è®­ç»ƒè½®æ¬¡
    try:
        iterations = input("\nè¯·è¾“å…¥è®­ç»ƒè½®æ¬¡ (é»˜è®¤10): ").strip()
        iterations = int(iterations) if iterations else 10
    except ValueError:
        iterations = 10
    
    print(f"\nğŸš€ å¼€å§‹ {iterations} è½®è®­ç»ƒ...")
    
    # è¿è¡Œè®­ç»ƒ
    result = run_enhanced_training(num_iterations=iterations)
    
    # è®¡ç®—è„šæœ¬æ€»è¿è¡Œæ—¶é—´
    script_end_time = time.time()
    script_end_datetime = datetime.now()
    total_script_time = script_end_time - script_start_time
    
    if result:
        print("\nâœ… è®­ç»ƒæˆåŠŸå®Œæˆ!")
        
        # æ˜¾ç¤ºè¯¦ç»†æ—¶é—´ç»Ÿè®¡
        print(f"\nâ° å®Œæ•´æ—¶é—´ç»Ÿè®¡:")
        print(f"   è„šæœ¬å¼€å§‹: {script_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   è„šæœ¬ç»“æŸ: {script_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   è„šæœ¬æ€»è¿è¡Œæ—¶é—´: {total_script_time/60:.1f}åˆ†é’Ÿ ({total_script_time:.1f}ç§’)")
        
        # ä»ç»“æœä¸­è·å–çº¯è®­ç»ƒæ—¶é—´
        if 'training_info' in result and 'total_duration' in result['training_info']:
            training_time = result['training_info']['total_duration']
            setup_time = total_script_time - training_time
            print(f"   çº¯è®­ç»ƒæ—¶é—´: {training_time/60:.1f}åˆ†é’Ÿ ({training_time:.1f}ç§’)")
            print(f"   ç¯å¢ƒåˆå§‹åŒ–æ—¶é—´: {setup_time/60:.1f}åˆ†é’Ÿ ({setup_time:.1f}ç§’)")
            print(f"   è®­ç»ƒæ•ˆç‡: {training_time/total_script_time*100:.1f}%")
        
        print("\nğŸ“‹ åç»­æ“ä½œ:")
        print("1. æŸ¥çœ‹ç»“æœ: ls D:\\MPU\\æ¯•ä¸šè®ºæ–‡\\MARL_FOR_W_Factory\\wsl\\ray_result\\")
        print("2. åˆ†ææ‘˜è¦: cat D:\\MPU\\æ¯•ä¸šè®ºæ–‡\\MARL_FOR_W_Factory\\wsl\\ray_result\\training_summary.json")
        print("3. è¿è¡Œå¯è§†åŒ–: python wsl/analyze_results.py")
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥!")
        print(f"â° è„šæœ¬è¿è¡Œæ—¶é—´: {total_script_time/60:.1f}åˆ†é’Ÿ") 