"""
WSL Ubuntuä¸“ç”¨çš„Ray RLlibå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒè„šæœ¬
é’ˆå¯¹WSLç¯å¢ƒä¼˜åŒ–ï¼Œè§£å†³Windows Rayå…¼å®¹æ€§é—®é¢˜
"""

import os
import sys
import time
import json
import tempfile
import subprocess
from typing import Dict, Any
from pathlib import Path
from datetime import datetime

# ğŸ• è„šæœ¬çœŸå®å¯åŠ¨æ—¶é—´ï¼ˆç¬¬ä¸€è¡Œä»£ç æ‰§è¡Œï¼‰
SCRIPT_START_TIME = time.time()
SCRIPT_START_DATETIME = datetime.now()
print(f"ğŸ• è„šæœ¬å¯åŠ¨æ—¶é—´: {SCRIPT_START_DATETIME.strftime('%Y-%m-%d %H:%M:%S')}")

# WSLç¯å¢ƒä¼˜åŒ–è®¾ç½®
os.environ['RAY_DISABLE_IMPORT_WARNING'] = '1'
os.environ['RAY_DEDUP_LOGS'] = '0'
os.environ['RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE'] = '1'
os.environ['RAY_DISABLE_PYARROW_TENSOR_EXTENSION'] = '1'
# ç¦ç”¨ä¸€äº›å¯èƒ½åœ¨WSLä¸­æœ‰é—®é¢˜çš„åŠŸèƒ½
os.environ['RAY_DISABLE_IMPORT_WARNING'] = '1'
os.environ['RAY_USAGE_STATS_ENABLED'] = '0'

print("ğŸ§ WSL Ubuntuç¯å¢ƒæ£€æµ‹...")

# æ£€æŸ¥æ˜¯å¦åœ¨WSLç¯å¢ƒä¸­
def check_wsl_environment():
    """æ£€æŸ¥WSLç¯å¢ƒ"""
    try:
        # æ£€æŸ¥/proc/versionæ–‡ä»¶
        with open('/proc/version', 'r') as f:
            version_info = f.read().lower()
            if 'microsoft' in version_info or 'wsl' in version_info:
                print("âœ… æ£€æµ‹åˆ°WSLç¯å¢ƒ")
                return True
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        if 'WSL_DISTRO_NAME' in os.environ:
            print("âœ… æ£€æµ‹åˆ°WSLç¯å¢ƒ (é€šè¿‡ç¯å¢ƒå˜é‡)")
            return True
            
        print("âš ï¸  æœªæ£€æµ‹åˆ°WSLç¯å¢ƒï¼Œä½†ç»§ç»­æ‰§è¡Œ...")
        return False
        
    except Exception as e:
        print(f"âš ï¸  WSLæ£€æµ‹å¤±è´¥: {e}")
        return False

# æ£€æŸ¥WSLç¯å¢ƒ
is_wsl = check_wsl_environment()

try:
    import ray
    from ray import tune
    from ray.rllib.algorithms.ppo import PPOConfig
    from ray.rllib.policy.policy import PolicySpec
    from ray.rllib.env import PettingZooEnv, MultiAgentEnv  # ğŸ”§ æ·»åŠ MultiAgentEnvå¯¼å…¥
    from ray.tune.registry import register_env
    import numpy as np
    import gymnasium as gym
    print("âœ… Rayåº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ Rayåº“å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·åœ¨WSLä¸­å®‰è£…Ray:")
    print("pip install ray[rllib] gymnasium pettingzoo")
    sys.exit(1)

# æ·»åŠ ç¯å¢ƒè·¯å¾„ - WSLè·¯å¾„å¤„ç†
current_dir = Path(__file__).parent.absolute()
parent_dir = current_dir.parent  # ä¸Šä¸€çº§ç›®å½•ï¼ŒåŒ…å«environments
sys.path.append(str(current_dir))
sys.path.append(str(parent_dir))

print(f"ğŸ” è„šæœ¬ç›®å½•: {current_dir}")
print(f"ğŸ” é¡¹ç›®æ ¹ç›®å½•: {parent_dir}")
print(f"ğŸ” æŸ¥æ‰¾environmentsç›®å½•: {parent_dir / 'environments'}")

try:
    from environments.w_factory_env import WFactoryGymEnv
    from environments.w_factory_config import *
    print("âœ… å·¥å‚ç¯å¢ƒå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å·¥å‚ç¯å¢ƒå¯¼å…¥å¤±è´¥: {e}")
    print(f"è¯·ç¡®ä¿environmentsç›®å½•å­˜åœ¨äº: {parent_dir}")
    sys.exit(1)

# ğŸ”§ é›†æˆV3ç‰ˆæœ¬çš„æˆåŠŸåŒ…è£…å™¨ + ç»§æ‰¿ä¿®å¤
class OptimizedWFactoryWrapper(MultiAgentEnv):
    """ä¼˜åŒ–ç‰ˆå·¥å‚ç¯å¢ƒåŒ…è£…å™¨ - åŸºäºV3æˆåŠŸç‰ˆæœ¬ + æ­£ç¡®ç»§æ‰¿MultiAgentEnv"""
    
    def __init__(self, config=None):
        super().__init__()  # ğŸ”§ å…³é”®ä¿®å¤ï¼šè°ƒç”¨MultiAgentEnvçš„åˆå§‹åŒ–
        self.config = config or {}
        
        # ğŸ”§ ç¡®ä¿ä½¿ç”¨ä¿®å¤åçš„å¥–åŠ±é…ç½®
        env_config = self.config.copy()
        env_config.update({
            'debug_level': 'INFO',         # ğŸ”§ æ˜¾ç¤ºå…³é”®ä¿¡æ¯ç”¨äºè¯Šæ–­
            'training_mode': False,        # ğŸ”§ ä¿®å¤ï¼šæ”¹ä¸ºFalseä»¥æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
            'use_fixed_rewards': True,     # ğŸ”§ æ–°å¢ï¼šä½¿ç”¨ä¿®å¤åçš„å¥–åŠ±ç³»ç»Ÿ
            'show_completion_stats': True  # ğŸ”§ V5æ–°å¢ï¼šæ˜¾ç¤ºå®Œæˆç»Ÿè®¡
        })
        
        self.base_env = WFactoryGymEnv(env_config)
        
        # è·å–æ™ºèƒ½ä½“åˆ—è¡¨
        self.agents = list(self.base_env.possible_agents)
        self._agent_ids = set(self.agents)
        
        # è®¾ç½®è§‚æµ‹å’ŒåŠ¨ä½œç©ºé—´
        self.observation_space = self.base_env.observation_space
        self.action_space = self.base_env.action_space
        self.observation_spaces = self.base_env.observation_spaces
        self.action_spaces = self.base_env.action_spaces
        
        # ğŸ”§ ä¿®å¤ï¼šæ¢å¤ä¸ä»¿çœŸæ—¶é—´åŒ¹é…çš„episodeé•¿åº¦
        self.max_episode_steps = 480   # ğŸ”§ æ¢å¤åˆ°480ï¼ŒåŒ¹é…SIMULATION_TIME
        self.current_step = 0
        
        # ğŸ”§ V5æ–°å¢ï¼šè‡ªç„¶ç»ˆæ­¢ä¼˜å…ˆæ ‡å¿—
        self.prefer_natural_termination = True
        
        # ğŸ”§ å®Œå…¨ç§»é™¤äººä¸ºå¥–åŠ±é˜ˆå€¼ - è®©æ™ºèƒ½ä½“é¢å¯¹çœŸå®æŒ‘æˆ˜
        self.episode_reward_threshold = None  # ä¸è®¾ç½®ä»»ä½•äººä¸ºæˆåŠŸæ ‡å‡†
        self.cumulative_reward = 0.0
        
        # ğŸ”§ å¤§å¹…æ”¾å®½æ— è¿›å±•æ£€æµ‹ - çœŸå®å­¦ä¹ éœ€è¦æ›´å¤šæ¢ç´¢æ—¶é—´
        self.consecutive_no_progress_steps = 0
        self.max_no_progress_steps = 500  # å¤§å¹…å¢åŠ ï¼Œç»™æ™ºèƒ½ä½“å……åˆ†æ¢ç´¢æœºä¼š
        
        # è®­ç»ƒæ¨¡å¼æ§åˆ¶ï¼ˆå‡å°‘è¾“å‡ºï¼‰
        self.training_mode = self.config.get('training_mode', True)
        
        # ğŸ”§ æ˜¾ç¤ºä¿®å¤çŠ¶æ€ï¼ˆä»…åœ¨éè®­ç»ƒæ¨¡å¼ä¸‹ï¼‰
        if not self.training_mode:
            print(f"ğŸ”§ ä¼˜åŒ–ç‰ˆç¯å¢ƒåŒ…è£…å™¨åˆå§‹åŒ–:")
            print(f"   ç»§æ‰¿ç±»: {self.__class__.__bases__}")
            print(f"   æ™ºèƒ½ä½“æ•°é‡: {len(self.agents)}")
            print(f"   æœ€å¤§episodeæ­¥æ•°: {self.max_episode_steps}")
            print(f"   æ— è¿›å±•æ£€æµ‹: {self.max_no_progress_steps}æ­¥")
            print(f"   å¥–åŠ±é˜ˆå€¼: å·²å®Œå…¨ç§»é™¤ (é¢å¯¹çœŸå®æŒ‘æˆ˜)")
            print(f"   ğŸ¯ ä½¿ç”¨ä¿®å¤åå¥–åŠ±ç³»ç»Ÿ: completion_reward={REWARD_CONFIG['completion_reward']}")
            print(f"   ğŸ¯ å¥–åŠ±åˆ†é…: å®Œæˆå¥–åŠ±åªç»™åŒ…è£…å°æ™ºèƒ½ä½“")
        
    def reset(self, *, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        try:
            # é‡ç½®è®¡æ•°å™¨
            self.current_step = 0
            self.cumulative_reward = 0.0
            self.consecutive_no_progress_steps = 0
            
            obs, info = self.base_env.reset(seed=seed, options=options)
            
            # ç¡®ä¿è¿”å›æ­£ç¡®æ ¼å¼
            if isinstance(obs, dict):
                return obs, info
            else:
                multi_obs = {agent: obs for agent in self.agents}
                return multi_obs, info
                
        except Exception as e:
            if not self.training_mode:
                print(f"âŒ ç¯å¢ƒé‡ç½®å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤è§‚æµ‹
            default_obs = np.zeros(2, dtype=np.float32)
            multi_obs = {agent: default_obs for agent in self.agents}
            return multi_obs, {agent: {} for agent in self.agents}
    
    def step(self, actions):
        """æ‰§è¡ŒåŠ¨ä½œ - ä¿®å¤ç‰ˆ"""
        try:
            self.current_step += 1
            
            # æ£€æŸ¥åŠ¨ä½œæ ¼å¼
            if isinstance(actions, dict):
                processed_actions = actions
            else:
                processed_actions = {agent: actions for agent in self.agents}
            
            # è°ƒç”¨åŸºç¡€ç¯å¢ƒ
            obs, rewards, terminated, truncated, info = self.base_env.step(processed_actions)
            
            # ä½¿ç”¨ç¯å¢ƒåŸç”Ÿå¥–åŠ±ï¼Œä½†æ·»åŠ è¿›åº¦æ£€æµ‹
            if isinstance(rewards, dict):
                step_reward = sum(rewards.values())
                self.cumulative_reward += step_reward
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¿›åº¦
                if step_reward > 0.2:
                    self.consecutive_no_progress_steps = 0
                else:
                    self.consecutive_no_progress_steps += 1
            
            # ğŸ”§ V5ä¿®å¤ï¼šä¼˜å…ˆæ£€æŸ¥è‡ªç„¶ç»ˆæ­¢ï¼Œè€Œä¸æ˜¯æ­¥æ•°é™åˆ¶
            natural_done = False
            if hasattr(self.base_env, 'pz_env') and hasattr(self.base_env.pz_env, 'sim'):
                sim = self.base_env.pz_env.sim
                if sim:
                    natural_done = sim.is_done()
            elif hasattr(self.base_env, 'sim') and self.base_env.sim:
                natural_done = self.base_env.sim.is_done()
            
            step_limit_reached = self.current_step >= self.max_episode_steps
            
            # ç»ˆæ­¢æ¡ä»¶ï¼šä¼˜å…ˆè‡ªç„¶ç»ˆæ­¢
            if natural_done:
                terminated = {agent: True for agent in self.agents}
                terminated['__all__'] = True
                if not self.training_mode:
                    print(f"   ğŸ Episodeè‡ªç„¶ç»ˆæ­¢äºç¬¬{self.current_step}æ­¥ (ä»»åŠ¡å®Œæˆ)!")
            elif step_limit_reached:
                terminated = {agent: True for agent in self.agents}
                terminated['__all__'] = True
                if not self.training_mode:
                    print(f"   â° Episodeæ­¥æ•°é™åˆ¶ç»ˆæ­¢äºç¬¬{self.current_step}æ­¥!")
            else:
                terminated = {agent: False for agent in self.agents}
                terminated['__all__'] = False
            
            # ç¡®ä¿å…¶ä»–è¿”å›å€¼æ ¼å¼æ­£ç¡®
            if not isinstance(obs, dict):
                obs = {agent: obs for agent in self.agents}
            if not isinstance(rewards, dict):
                rewards = {agent: rewards for agent in self.agents}
            if not isinstance(info, dict):
                info = {agent: info for agent in self.agents}
            
            return obs, rewards, terminated, truncated, info
            
        except Exception as e:
            if not self.training_mode:
                print(f"âŒ ç¯å¢ƒæ­¥è¿›å¤±è´¥: {e}")
            
            # è¿”å›é»˜è®¤å€¼ï¼Œå¼ºåˆ¶ç»ˆæ­¢
            default_obs = np.zeros(2, dtype=np.float32)
            terminated_dict = {agent: True for agent in self.agents}
            terminated_dict['__all__'] = True
            truncated_dict = {agent: False for agent in self.agents}
            truncated_dict['__all__'] = False
            
            return (
                {agent: default_obs for agent in self.agents},
                {agent: 0.0 for agent in self.agents},
                terminated_dict,
                truncated_dict,
                {agent: {} for agent in self.agents}
            )
    
    # Ray RLlib å…¼å®¹æ€§æ–¹æ³•
    def get_agent_ids(self):
        return self._agent_ids
    
    def get_observation_space(self, agent_id=None):
        if agent_id is None:
            return self.observation_spaces
        return self.observation_spaces.get(agent_id)
    
    def get_action_space(self, agent_id=None):
        if agent_id is None:
            return self.action_spaces
        return self.action_spaces.get(agent_id)

def env_creator(config):
    """ç¯å¢ƒåˆ›å»ºå‡½æ•° - ä½¿ç”¨ä¼˜åŒ–ç‰ˆåŒ…è£…å™¨"""
    return OptimizedWFactoryWrapper(config)

# æ³¨å†Œç¯å¢ƒ
register_env("w_factory", env_creator)

def get_wsl_system_info():
    """è·å–WSLç³»ç»Ÿä¿¡æ¯"""
    try:
        import psutil
        cpu_count = psutil.cpu_count()
        memory_info = psutil.virtual_memory()
        memory_mb = memory_info.total // (1024 * 1024)
        
        return {
            "cpu_count": cpu_count,
            "memory_mb": memory_mb,
            "available_memory_mb": memory_info.available // (1024 * 1024)
        }
    except ImportError:
        # å¦‚æœæ²¡æœ‰psutilï¼Œä½¿ç”¨é»˜è®¤å€¼
        import os
        cpu_count = os.cpu_count() or 4
        return {
            "cpu_count": cpu_count,
            "memory_mb": 4096,  # é»˜è®¤4GB
            "available_memory_mb": 2048
        }

def get_wsl_ray_config():
    """è·å–WSLä¼˜åŒ–çš„Rayé…ç½®"""
    system_info = get_wsl_system_info()
    
    # æ ¹æ®ç³»ç»Ÿèµ„æºåŠ¨æ€è°ƒæ•´
    cpu_count = system_info.get("cpu_count", 4)
    memory_mb = system_info.get("memory_mb", 4096)
    
    # WSLç¯å¢ƒä¸‹çš„ä¿å®ˆé…ç½®
    num_cpus = min(cpu_count, 6)  # é™åˆ¶CPUä½¿ç”¨
    object_store_memory = min(memory_mb * 1024 * 1024 // 4, 500_000_000)  # 1/4å†…å­˜æˆ–500MB
    
    print(f"ğŸ”§ WSLç³»ç»Ÿé…ç½®:")
    print(f"   CPUæ ¸å¿ƒ: {cpu_count} (ä½¿ç”¨: {num_cpus})")
    print(f"   å†…å­˜: {memory_mb}MB (å¯¹è±¡å­˜å‚¨: {object_store_memory//1024//1024}MB)")
    
    # åˆ›å»ºWSLå‹å¥½çš„ä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp(prefix="ray_wsl_")
    
    return {
        "local_mode": False,
        "ignore_reinit_error": True,
        "include_dashboard": False,  # WSLä¸­ç¦ç”¨dashboard
        "_temp_dir": temp_dir,
        "object_store_memory": object_store_memory,
        "num_cpus": num_cpus,
        # WSLç‰¹å®šé…ç½®
        "log_to_driver": True,
        "configure_logging": True,
        "logging_level": "ERROR",
    }

def create_ray_config():
    """åˆ›å»ºRay RLlibé…ç½® - Ray 2.48.0å…¼å®¹ç‰ˆæœ¬ + å¥–åŠ±ä¿®å¤é›†æˆ"""
    config = (
        PPOConfig()
        .environment(
            env="w_factory",
            env_config={
                'debug_level': 'WARNING',      # å‡å°‘ç¯å¢ƒè¾“å‡º
                'training_mode': True,         # å¯ç”¨è®­ç»ƒæ¨¡å¼
                'use_fixed_rewards': True,     # ğŸ”§ ä½¿ç”¨ä¿®å¤åçš„å¥–åŠ±ç³»ç»Ÿ
                'show_completion_stats': True  # ğŸ”§ V5æ–°å¢ï¼šæ˜¾ç¤ºå®Œæˆç»Ÿè®¡
            }
        )
        .framework("torch")
        .api_stack(
            # ç¦ç”¨æ–°APIæ ˆï¼Œä½¿ç”¨æ—§ç‰ˆæœ¬å…¼å®¹æ¨¡å¼
            enable_rl_module_and_learner=False,
            enable_env_runner_and_connector_v2=False,
        )
        .training(
            train_batch_size=1000,  # ğŸ”§ ä¿®å¤ï¼šå‡å°‘æ‰¹æ¬¡å¤§å°ï¼Œé¿å…è¿‡åº¦è®­ç»ƒ
            minibatch_size=64,      # ğŸ”§ ä¿®å¤ï¼šå‡å°‘å°æ‰¹æ¬¡å¤§å°
            num_epochs=3,           # ğŸ”§ ä¿®å¤ï¼šå‡å°‘epochæ•°
            lr=3e-4,               # ğŸ”§ ä¿®å¤ï¼šé™ä½å­¦ä¹ ç‡
            gamma=0.99,
            lambda_=0.95,
            clip_param=0.2,
            vf_clip_param=10.0,
            entropy_coeff=0.01,
            vf_loss_coeff=0.5,
        )
        .env_runners(
            # ğŸ”§ V3ä¿®å¤ï¼šä½¿ç”¨æœ¬åœ°æ¨¡å¼é¿å…åºåˆ—åŒ–é—®é¢˜
            num_env_runners=0,  # æœ¬åœ°æ¨¡å¼ï¼Œé¿å…Ray workeråºåˆ—åŒ–é—®é¢˜
            rollout_fragment_length=TRAINING_CONFIG["rollout_fragment_length"],  # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„200
            batch_mode="complete_episodes",  # ä½¿ç”¨å®Œæ•´episode
        )
        .resources(
            num_gpus=0,
        )
        .multi_agent(
            policies={"shared_policy": PolicySpec()},
            policy_mapping_fn=lambda agent_id, *args, **kwargs: "shared_policy",
        )
        .evaluation(
            evaluation_interval=10,
            evaluation_duration=5,
        )
        .debugging(
            log_level="WARNING",  # å‡å°‘Rayæ—¥å¿—
        )
    )
    
    print(f"ğŸ”§ Rayé…ç½®å·²æ›´æ–°:")
    print(f"   ğŸ¯ å¥–åŠ±ç³»ç»Ÿ: ä¿®å¤ç‰ˆ (completion_reward={REWARD_CONFIG['completion_reward']})")
    print(f"   ğŸ¯ å¥–åŠ±åˆ†é…: æ™ºèƒ½åˆ†é…æœºåˆ¶ (å®Œæˆå¥–åŠ±åªç»™åŒ…è£…å°)")
    print(f"   ğŸ“Š è®­ç»ƒæ‰¹æ¬¡: {config.train_batch_size}")
    print(f"   ğŸ® Episodeæ¨¡å¼: complete_episodes")
    
    return config

def get_wsl_training_config():
    """è·å–WSLä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""
    system_info = get_wsl_system_info()
    cpu_count = system_info.get("cpu_count", 4)
    
    # æ ¹æ®CPUæ•°é‡è°ƒæ•´workeræ•°é‡
    num_workers = max(1, min(cpu_count - 1, 4))  # ä¿ç•™1ä¸ªCPUç»™ä¸»è¿›ç¨‹
    
    config = (
        PPOConfig()
        .environment(
            env="w_factory",  # ğŸ”§ ä½¿ç”¨ä¿®å¤ç‰ˆç¯å¢ƒ
            env_config={},
            disable_env_checking=True
        )
        .framework("torch") 
        .env_runners(
            # æœ¬åœ°æ¨¡å¼é…ç½® (é¿å…ç¯å¢ƒæ³¨å†Œé—®é¢˜)
            num_env_runners=0,  # æœ¬åœ°æ¨¡å¼ä¸ä½¿ç”¨è¿œç¨‹runner
            rollout_fragment_length=500,  # å¢åŠ rollouté•¿åº¦
            batch_mode="complete_episodes",  # ğŸ”§ ä¿®å¤ï¼šæ”¹ä¸ºå®Œæ•´episodeæ¨¡å¼ï¼Œé¿å…å¼ºåˆ¶æˆªæ–­
        )
        .training(
            # PPOè®­ç»ƒå‚æ•° (Ray 2.48 API)
            train_batch_size=4000,  # å¢åŠ è®­ç»ƒæ‰¹æ¬¡å¤§å°
            lr=3e-4,
            gamma=0.99,
        )
        .multi_agent(
            # å¤šæ™ºèƒ½ä½“é…ç½® (æŒ‰ç…§main.pyæ¨¡å¼)
            policies={"shared_policy": PolicySpec()},
            policy_mapping_fn=(lambda agent_id, *args, **kwargs: "shared_policy"),
        )
        .resources(
            # WSLèµ„æºé…ç½® (Ray 2.48 API)
            num_gpus=0  # WSLé€šå¸¸ä¸æ”¯æŒGPUï¼Œç§»é™¤è¿‡æ—¶å‚æ•°
        )
        .evaluation(
            # è¯„ä¼°é…ç½® (ç®€åŒ–ç‰ˆæœ¬)
            evaluation_interval=25,
            evaluation_duration=10,
            evaluation_config={
                "explore": False,
                "render_env": False,
            }
        )
        .debugging(
            # WSLè°ƒè¯•é…ç½®
            log_level="INFO",  # WSLä¸­å¯ä»¥æ˜¾ç¤ºæ›´å¤šæ—¥å¿—
        )
        .experimental(
            # å®éªŒæ€§é…ç½®
            _disable_preprocessor_api=True,
        )
    )
    
    # è®¾ç½®PPOç‰¹å®šçš„è¶…å‚æ•° (Ray 2.48æ–¹å¼)
    config.lambda_ = 0.95
    config.clip_param = 0.2
    config.vf_loss_coeff = 0.5
    config.entropy_coeff = 0.01
    config.minibatch_size = 128  # Ray 2.48.0ä¸­çš„æ­£ç¡®å‚æ•°å
    config.num_sgd_iter = 10
    config.horizon = 2000  # ğŸ”§ åŒ¹é…ç¯å¢ƒçš„æœ€å¤§æ­¥æ•°ï¼Œè®©å®Œæ•´ç”Ÿäº§å‘¨æœŸæœ‰æœºä¼šå®Œæˆ
    
    print(f"ğŸ”§ è®­ç»ƒé…ç½®:")
    print(f"   æ¨¡å¼: æœ¬åœ°æ¨¡å¼ (é¿å…ç¯å¢ƒæ³¨å†Œé—®é¢˜)")
    print(f"   Env Runners: 0 (æœ¬åœ°æ¨¡å¼)")
    print(f"   è®­ç»ƒæ‰¹æ¬¡å¤§å°: 4000")
    print(f"   Episodeé•¿åº¦: 2000æ­¥ (è®©å®Œæ•´ç”Ÿäº§å‘¨æœŸå®Œæˆ)")
    print(f"   Rollouté•¿åº¦: 500æ­¥")
    print(f"   SGDè¿­ä»£æ¬¡æ•°: 10")
    print(f"   SGDå°æ‰¹æ¬¡å¤§å°: 128")
    
    return config

def run_wsl_ray_training(num_iterations=20):
    """è¿è¡ŒWSL Ray RLlibè®­ç»ƒ"""
    print("ğŸš€ å¼€å§‹WSL Ray RLlibè®­ç»ƒ...")
    
    # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
    training_start_time = time.time()
    
    try:
        # åˆå§‹åŒ–Ray
        if not ray.is_initialized():
            ray.init(
                num_cpus=4,
                num_gpus=0,
                object_store_memory=1000000000,  # 1GB
                ignore_reinit_error=True,
                log_to_driver=False,  # å‡å°‘æ—¥å¿—è¾“å‡º
            )
        
        # æ³¨å†Œç¯å¢ƒ - ä½¿ç”¨ä¼˜åŒ–ç‰ˆåŒ…è£…å™¨
        register_env("w_factory", env_creator)
        
        # åˆ›å»ºé…ç½®
        config = create_ray_config()
        
        # åˆ›å»ºç®—æ³•
        algo = config.build()
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½• - WSLè·¯å¾„æ ¼å¼
        checkpoint_dir = "/mnt/d/MPU/æ¯•ä¸šè®ºæ–‡/MARL_FOR_W_Factory/wsl/ray_result"
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # è®­ç»ƒå¾ªç¯
        print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ {num_iterations} è½®...")
        print(f"â° è®­ç»ƒå¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(training_start_time))}")
        print("=" * 70)
        
        best_reward = float('-inf')
        best_checkpoint = None
        total_episodes_completed = 0
        iteration_times = []  # è®°å½•æ¯è½®è®­ç»ƒæ—¶é—´
        
        for i in range(num_iterations):
            iteration_start_time = time.time()
            
            print(f"\nğŸ“Š è®­ç»ƒè½®æ¬¡ {i+1}/{num_iterations}")
            print("-" * 50)
            
            result = algo.train()
            
            iteration_end_time = time.time()
            iteration_duration = iteration_end_time - iteration_start_time
            iteration_times.append(iteration_duration)
            
            # è·å–å…³é”®æŒ‡æ ‡ - Ray 2.48.0å…¼å®¹
            episode_reward_mean = result.get("episode_reward_mean", 0)
            episodes_this_iter = result.get("episodes_this_iter", 0)
            episode_len_mean = result.get("episode_len_mean", 0)
            
            # Ray 2.48.0ä¸­ç»Ÿè®¡æ•°æ®å¯èƒ½åœ¨env_runnersä¸­
            if episodes_this_iter == 0 and 'env_runners' in result:
                env_stats = result['env_runners']
                episode_reward_mean = env_stats.get("episode_reward_mean", episode_reward_mean)
                episodes_this_iter = env_stats.get("episodes_this_iter", episodes_this_iter)
                episode_len_mean = env_stats.get("episode_len_mean", episode_len_mean)
            
            total_episodes_completed += episodes_this_iter
            
            # æ˜¾ç¤ºè®­ç»ƒç»“æœ
            print(f"   å¹³å‡å¥–åŠ±: {episode_reward_mean:.2f}")
            print(f"   å®Œæˆepisodeæ•°: {episodes_this_iter}")
            print(f"   å¹³å‡episodeé•¿åº¦: {episode_len_mean:.1f}")
            print(f"   ç´¯è®¡å®Œæˆepisode: {total_episodes_completed}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›
            if episode_reward_mean > best_reward:
                best_reward = episode_reward_mean
                print(f"   ğŸ‰ æ–°çš„æœ€ä½³å¥–åŠ±: {best_reward:.2f}")
                
                # ä¿å­˜æ£€æŸ¥ç‚¹ - åªæ˜¾ç¤ºç®€æ´ä¿¡æ¯
                best_checkpoint = algo.save(checkpoint_dir)
                if hasattr(best_checkpoint, 'path'):
                    checkpoint_path = best_checkpoint.path
                else:
                    # ä»å­—ç¬¦ä¸²ä¸­æå–è·¯å¾„
                    checkpoint_str = str(best_checkpoint)
                    if 'path=' in checkpoint_str:
                        path_start = checkpoint_str.find('path=') + 5
                        path_end = checkpoint_str.find(')', path_start)
                        if path_end == -1:
                            path_end = checkpoint_str.find(',', path_start)
                        checkpoint_path = checkpoint_str[path_start:path_end]
                    else:
                        checkpoint_path = "wsl_ray_results/checkpoints"
                
                print(f"   ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
            else:
                print(f"   ğŸ“Š å½“å‰å¥–åŠ±: {episode_reward_mean:.2f} (æœ€ä½³: {best_reward:.2f})")
            
            # æ˜¾ç¤ºå­¦ä¹ è¿›åº¦
            if "info" in result and "learner" in result["info"]:
                learner_info = result["info"]["learner"]["shared_policy"]
                if "learner_stats" in learner_info:
                    stats = learner_info["learner_stats"]
                    policy_loss = stats.get("policy_loss", 0)
                    vf_loss = stats.get("vf_loss", 0)
                    print(f"   ç­–ç•¥æŸå¤±: {policy_loss:.4f}, ä»·å€¼æŸå¤±: {vf_loss:.4f}")
            
            # ğŸ”§ æ ¸å¿ƒKPIç›‘æ§ï¼šç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…Ray APIå…¼å®¹æ€§é—®é¢˜
            try:
                # æ˜¾ç¤ºåŸºæœ¬è®­ç»ƒç»Ÿè®¡
                print(f"   ğŸ“ˆ è®­ç»ƒè¿›åº¦: {i+1}/{num_iterations} ({(i+1)/num_iterations*100:.1f}%)")
                
                # å°è¯•ä»resultä¸­è·å–åŸºç¡€KPIä¿¡æ¯
                if hasattr(result, 'info') and result.info:
                    episode_info = result.info.get('episode', {})
                    if episode_info:
                        episode_len = episode_info.get('len', episode_len_mean)
                        episode_reward = episode_info.get('reward', episode_reward_mean)
                        print(f"   ğŸ“Š Episodeä¿¡æ¯: é•¿åº¦={episode_len:.1f}, å¥–åŠ±={episode_reward:.1f}")
                        
                        # è®¡ç®—è‡ªç„¶ç»ˆæ­¢ç‡
                        if episode_len < 480:
                            natural_rate = ((480 - episode_len) / 480) * 100
                            print(f"   ğŸ¯ è‡ªç„¶ç»ˆæ­¢ç‡: {natural_rate:.1f}% (æå‰{480-episode_len:.1f}æ­¥å®Œæˆ)")
                        else:
                            print(f"   â° è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶ (480æ­¥)")
                
                # æ˜¾ç¤ºå­¦ä¹ æ•ˆæœæŒ‡æ ‡
                if episode_reward_mean > 1800:
                    print(f"   âœ… å­¦ä¹ æ•ˆæœ: è‰¯å¥½ (å¥–åŠ±>{episode_reward_mean:.0f})")
                elif episode_reward_mean > 1500:
                    print(f"   âš ï¸  å­¦ä¹ æ•ˆæœ: ä¸€èˆ¬ (å¥–åŠ±={episode_reward_mean:.0f})")
                else:
                    print(f"   âŒ å­¦ä¹ æ•ˆæœ: éœ€æ”¹è¿› (å¥–åŠ±={episode_reward_mean:.0f})")
                
            except Exception as e:
                print(f"   âš ï¸  KPIæ˜¾ç¤ºå¤±è´¥: {e}")
                print(f"   ğŸ“ˆ è®­ç»ƒè¿›åº¦: {i+1}/{num_iterations} ({(i+1)/num_iterations*100:.1f}%)")
            
            # æ—¶é—´ç»Ÿè®¡å’Œé¢„æµ‹
            elapsed_time = time.time() - training_start_time
            avg_iteration_time = sum(iteration_times) / len(iteration_times)
            remaining_iterations = num_iterations - (i + 1)
            estimated_remaining_time = remaining_iterations * avg_iteration_time
            
            print(f"   â±ï¸  æœ¬è½®ç”¨æ—¶: {iteration_duration:.1f}ç§’")
            
            if remaining_iterations > 0:
                print(f"   ğŸ”® é¢„è®¡å‰©ä½™: {estimated_remaining_time/60:.1f}åˆ†é’Ÿ")
                estimated_finish_time = time.time() + estimated_remaining_time
                finish_time_str = time.strftime('%H:%M:%S', time.localtime(estimated_finish_time))
                print(f"   ğŸ é¢„è®¡å®Œæˆ: {finish_time_str}")
            
            # å¦‚æœæ²¡æœ‰å®Œæˆä»»ä½•episodeï¼Œç»™å‡ºæç¤º
            if episodes_this_iter == 0:
                print("   â³ æœ¬è½®æœªå®Œæˆepisodeï¼Œç»§ç»­è®­ç»ƒ...")
        
        # è®­ç»ƒå®Œæˆç»Ÿè®¡
        training_end_time = time.time()
        total_training_time = training_end_time - training_start_time
        
        print("\n" + "=" * 70)
        print(f"ğŸ è®­ç»ƒå®Œæˆï¼")
        print(f"   æœ€ä½³å¹³å‡å¥–åŠ±: {best_reward:.2f}")
        print(f"   æ€»å®Œæˆepisodeæ•°: {total_episodes_completed}")
        print(f"   æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f}åˆ†é’Ÿ ({total_training_time:.1f}ç§’)")
        print(f"   å¹³å‡æ¯è½®æ—¶é—´: {total_training_time/num_iterations:.1f}ç§’")
        print(f"   æœ€å¿«å•è½®: {min(iteration_times):.1f}ç§’")
        print(f"   æœ€æ…¢å•è½®: {max(iteration_times):.1f}ç§’")
        
        if total_episodes_completed == 0:
            print("âš ï¸  è­¦å‘Š: è®­ç»ƒæœŸé—´æ²¡æœ‰å®Œæˆä»»ä½•episode")
            print("ğŸ’¡ å»ºè®®: å¢åŠ è®­ç»ƒè½®æ¬¡æˆ–è°ƒæ•´ç¯å¢ƒå‚æ•°")
        
        # åˆ›å»ºæœ€ä½³ç»“æœå¯¹è±¡
        class BestResult:
            def __init__(self, reward, checkpoint, training_time, iteration_times):
                self.metrics = {
                    "episode_reward_mean": reward, 
                    "training_iteration": num_iterations,
                    "total_training_time": training_time,
                    "avg_iteration_time": sum(iteration_times) / len(iteration_times),
                    "total_episodes": total_episodes_completed
                }
                self.checkpoint = checkpoint
        
        # å¦‚æœæ²¡æœ‰ä¿å­˜è¿‡æ£€æŸ¥ç‚¹ï¼Œä¿å­˜æœ€åä¸€ä¸ª
        if best_checkpoint is None:
            best_checkpoint = algo.save(checkpoint_dir)
            print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹: {best_checkpoint}")
        
        best_result = BestResult(best_reward, best_checkpoint, total_training_time, iteration_times)
        
        # æ¸…ç†
        algo.stop()
        
        return best_result
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None

def create_wsl_setup_script():
    """åˆ›å»ºWSLç¯å¢ƒè®¾ç½®è„šæœ¬"""
    setup_script = """#!/bin/bash
# WSL Ubuntuç¯å¢ƒè®¾ç½®è„šæœ¬

echo "ğŸ§ è®¾ç½®WSL Ubuntuç¯å¢ƒç”¨äºRay MARLè®­ç»ƒ"

# æ›´æ–°ç³»ç»Ÿ
echo "ğŸ“¦ æ›´æ–°ç³»ç»ŸåŒ…..."
sudo apt update && sudo apt upgrade -y

# å®‰è£…Pythonå’Œpip
echo "ğŸ å®‰è£…Pythonç¯å¢ƒ..."
sudo apt install -y python3 python3-pip python3-venv

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
echo "ğŸ”§ åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ..."
python3 -m venv marl_env
source marl_env/bin/activate

# å®‰è£…ä¾èµ–
echo "ğŸ“š å®‰è£…Pythonä¾èµ–..."
pip install --upgrade pip
pip install ray[rllib]
pip install gymnasium
pip install pettingzoo
pip install simpy
pip install numpy
pip install tensorflow

echo "âœ… WSLç¯å¢ƒè®¾ç½®å®Œæˆï¼"
echo "ä½¿ç”¨æ–¹æ³•:"
echo "1. æ¿€æ´»ç¯å¢ƒ: source marl_env/bin/activate"
echo "2. è¿è¡Œè®­ç»ƒ: python3 wsl_ray_marl_train.py"
"""
    
    setup_file = Path("setup_wsl_env.sh")
    with open(setup_file, 'w') as f:
        f.write(setup_script)
    
    # è®¾ç½®æ‰§è¡Œæƒé™
    os.chmod(setup_file, 0o755)
    
    print(f"ğŸ“„ WSLè®¾ç½®è„šæœ¬å·²åˆ›å»º: {setup_file}")
    return setup_file

def main():
    """ä¸»å‡½æ•°"""
    # è®°å½•è„šæœ¬å¼€å§‹æ—¶é—´
    print("ğŸ§ Wå·¥å‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ - WSLç‰ˆæœ¬")
    print("=" * 70)
    
    # æ£€æŸ¥WSLç¯å¢ƒ
    if not is_wsl:
        print("âš ï¸  å»ºè®®åœ¨WSLç¯å¢ƒä¸­è¿è¡Œæ­¤è„šæœ¬ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
    
    # åˆ›å»ºè®¾ç½®è„šæœ¬
    setup_file = create_wsl_setup_script()
    
    try:
        # è¿è¡ŒRayè®­ç»ƒ - ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨åˆç†è½®æ¬¡é¿å…è¿‡åº¦è®­ç»ƒ
        ray_result = run_wsl_ray_training(num_iterations=20)  # ğŸ”§ ä¿®å¤ï¼šå‡å°‘åˆ°20è½®ï¼Œé¿å…è¿‡åº¦è®­ç»ƒ
        
        # è®¡ç®—è„šæœ¬æ€»è¿è¡Œæ—¶é—´ï¼ˆä½¿ç”¨å…¨å±€å¯åŠ¨æ—¶é—´ï¼‰
        script_end_time = time.time()
        script_end_datetime = datetime.now()
        total_script_time = script_end_time - SCRIPT_START_TIME
        
        if ray_result:
            print("\nğŸ‰ WSL Ray RLlibè®­ç»ƒæˆåŠŸå®Œæˆï¼")
            
            # æ˜¾ç¤ºæ—¶é—´ç»Ÿè®¡
            print(f"\nâ° æ—¶é—´ç»Ÿè®¡:")
            print(f"   è„šæœ¬å¼€å§‹: {SCRIPT_START_DATETIME.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"   è„šæœ¬ç»“æŸ: {script_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"   è„šæœ¬æ€»è¿è¡Œæ—¶é—´: {total_script_time/60:.1f}åˆ†é’Ÿ ({total_script_time:.1f}ç§’)")
            
            # ğŸ”§ ä¿®å¤ï¼šæ›´çœŸå®çš„æ—¶é—´åˆ†æ
            # ä»è¿è¡Œæ—¥å¿—å¯ä»¥çœ‹å‡ºï¼š
            # - Rayåˆå§‹åŒ–çº¦30ç§’ï¼ˆ14:45:10åˆ°14:45:41ï¼‰
            # - ç®—æ³•æ„å»ºçº¦1.5åˆ†é’Ÿï¼ˆåˆ°14:47:29ï¼‰
            # - çº¯è®­ç»ƒçº¦9.8åˆ†é’Ÿï¼ˆ14:47:29åˆ°14:54:58ï¼‰
            
            # ä¼°ç®—å„é˜¶æ®µæ—¶é—´ï¼ˆåŸºäºå®é™…è¿è¡Œè§‚å¯Ÿï¼‰
            estimated_import_time = 30  # å¯¼å…¥å’Œç¯å¢ƒæ£€æµ‹
            estimated_ray_init_time = 90  # Rayåˆå§‹åŒ–å’Œç®—æ³•æ„å»º
            estimated_training_time = total_script_time - estimated_import_time - estimated_ray_init_time
            
            print(f"   å¯¼å…¥å’Œç¯å¢ƒæ£€æµ‹: ~{estimated_import_time}ç§’")
            print(f"   Rayåˆå§‹åŒ–å’Œç®—æ³•æ„å»º: ~{estimated_ray_init_time}ç§’")
            print(f"   çº¯è®­ç»ƒæ—¶é—´: ~{estimated_training_time:.1f}ç§’ ({estimated_training_time/60:.1f}åˆ†é’Ÿ)")
            print(f"   è®­ç»ƒæ•ˆç‡: {estimated_training_time/total_script_time*100:.1f}%")
            
            # ğŸ”§ è‡ªåŠ¨éªŒè¯å·²ç¦ç”¨ï¼Œé¿å…è¶…æ—¶é—®é¢˜
            print(f"\nğŸ’¡ è®­ç»ƒå®Œæˆï¼Œå»ºè®®æ‰‹åŠ¨è¿è¡ŒéªŒè¯:")
            print(f"   ğŸ” æ¨ç†æµ‹è¯•: python wsl/test_trained_model_inference.py")
            print(f"   ğŸ“Š æ€§èƒ½åŸºå‡†: python wsl/test_performance_benchmark.py")
            
            # æ˜¾ç¤ºåç»­æ­¥éª¤
            print("\nğŸ“‹ åç»­æ­¥éª¤:")
            print("1. æŸ¥çœ‹è®­ç»ƒç»“æœ: ls /mnt/d/MPU/æ¯•ä¸šè®ºæ–‡/MARL_FOR_W_Factory/wsl/ray_result/")
            print("2. è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•: python wsl/test_performance_benchmark.py")
            print("3. æ‰‹åŠ¨è¿è¡Œæ¨ç†æµ‹è¯•: python wsl/test_trained_model_inference.py")
            print("4. å¯è§†åŒ–è®­ç»ƒæ›²çº¿")
            
        else:
            print("\nâŒ WSL Rayè®­ç»ƒå¤±è´¥")
            print(f"ğŸ’¡ è¯·å…ˆè¿è¡Œè®¾ç½®è„šæœ¬: bash {setup_file}")
            print(f"â° è„šæœ¬è¿è¡Œæ—¶é—´: {total_script_time/60:.1f}åˆ†é’Ÿ")
            
    except Exception as e:
        script_end_time = time.time()
        total_script_time = script_end_time - SCRIPT_START_TIME
        print(f"âŒ ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"â° è„šæœ¬è¿è¡Œæ—¶é—´: {total_script_time/60:.1f}åˆ†é’Ÿ")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 