"""
MARLç”Ÿäº§è°ƒåº¦ç³»ç»Ÿå…¨é¢è‡ªåŠ¨åŒ–ä¼˜åŒ–æ¡†æ¶
ä¸ä»…è°ƒä¼˜è¶…å‚æ•°ï¼Œè¿˜è‡ªåŠ¨ä¼˜åŒ–ç¯å¢ƒè®¾è®¡ã€å¥–åŠ±å‡½æ•°ã€ç½‘ç»œæ¶æ„ç­‰å…³é”®ç»„ä»¶
"""

import os
import sys
import copy
import json
import time
import numpy as np
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional
import optuna
from optuna.samplers import TPESampler
from optuna.pruners import MedianPruner
from dataclasses import dataclass
from enum import Enum

# æ·»åŠ ç¯å¢ƒè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

# å¯¼å…¥ç°æœ‰æ¨¡å—
from environments import w_factory_config

class OptimizationLevel(Enum):
    """ä¼˜åŒ–çº§åˆ«æšä¸¾"""
    HYPERPARAMS_ONLY = "hyperparams"          # ä»…ä¼˜åŒ–è¶…å‚æ•°
    ENVIRONMENT_DESIGN = "environment"        # ç¯å¢ƒè®¾è®¡ä¼˜åŒ–
    REWARD_ENGINEERING = "reward"            # å¥–åŠ±å·¥ç¨‹ä¼˜åŒ–
    ARCHITECTURE_SEARCH = "architecture"     # æ¶æ„æœç´¢
    FULL_SYSTEM = "full"                     # å…¨ç³»ç»Ÿä¼˜åŒ–

@dataclass
class OptimizationConfig:
    """ä¼˜åŒ–é…ç½®"""
    n_trials: int = 100
    n_eval_episodes: int = 15
    max_train_episodes: int = 300
    optimization_level: OptimizationLevel = OptimizationLevel.FULL_SYSTEM
    enable_generalization_test: bool = True
    parallel_trials: int = 1
    output_dir: str = "advanced_optimization_results"

class AdvancedMARL_Optimizer:
    """é«˜çº§MARLç³»ç»Ÿä¼˜åŒ–å™¨"""
    
    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.results_dir = f"{config.output_dir}/{self.timestamp}"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # ä¼˜åŒ–å†å²è®°å½•
        self.optimization_history = []
        self.best_configurations = {}
        
        print(f"ğŸš€ é«˜çº§MARLä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š ä¼˜åŒ–çº§åˆ«: {config.optimization_level.value}")
        print(f"ğŸ”¬ è¯•éªŒæ¬¡æ•°: {config.n_trials}")
        print(f"ğŸ¯ è®­ç»ƒè½®æ•°ä¸Šé™: {config.max_train_episodes}")
        print(f"ğŸ“ ç»“æœç›®å½•: {self.results_dir}")
        
        # æ ¹æ®ä¼˜åŒ–çº§åˆ«è®¾ç½®æœç´¢ç©ºé—´
        self.search_space_functions = {
            OptimizationLevel.HYPERPARAMS_ONLY: self._suggest_hyperparameters_only,
            OptimizationLevel.ENVIRONMENT_DESIGN: self._suggest_environment_design,
            OptimizationLevel.REWARD_ENGINEERING: self._suggest_reward_engineering,
            OptimizationLevel.ARCHITECTURE_SEARCH: self._suggest_architecture_search,
            OptimizationLevel.FULL_SYSTEM: self._suggest_full_system
        }

    def _suggest_hyperparameters_only(self, trial: optuna.Trial) -> Dict[str, Any]:
        """åŸºç¡€è¶…å‚æ•°ä¼˜åŒ–ï¼ˆä¸åŸè„šæœ¬ç±»ä¼¼ï¼‰"""
        return {
            'learning_rate_config': {
                'initial_lr': trial.suggest_float('initial_lr', 1e-5, 1e-3, log=True),
                'end_lr': trial.suggest_float('end_lr', 1e-6, 1e-4, log=True),
                'decay_power': trial.suggest_float('decay_power', 0.5, 1.0, step=0.1)
            },
            'network_config': {
                'hidden_sizes': [
                    trial.suggest_categorical('hidden_size_1', [256, 512, 768, 1024]),
                    trial.suggest_categorical('hidden_size_2', [128, 256, 384, 512])
                ],
                'dropout_rate': trial.suggest_float('dropout_rate', 0.0, 0.3, step=0.05),
                'clip_ratio': trial.suggest_float('clip_ratio', 0.1, 0.3, step=0.05),
                'entropy_coeff': trial.suggest_float('entropy_coeff', 0.01, 0.2, step=0.01),
                'num_policy_updates': trial.suggest_int('num_policy_updates', 5, 20)
            }
        }

    def _suggest_environment_design(self, trial: optuna.Trial) -> Dict[str, Any]:
        """ç¯å¢ƒè®¾è®¡ä¼˜åŒ–"""
        base_config = self._suggest_hyperparameters_only(trial)
        
        # è§‚å¯Ÿç©ºé—´è®¾è®¡ä¼˜åŒ–
        base_config['obs_config'] = {
            'top_n_parts': trial.suggest_int('top_n_parts', 2, 6),
            'include_downstream_info': trial.suggest_categorical('include_downstream_info', [True, False]),
            'time_feature_normalization': trial.suggest_categorical('time_norm', [50.0, 100.0, 200.0]),
            
            # ğŸŒŸ æ–°å¢ï¼šçŠ¶æ€ç‰¹å¾å·¥ç¨‹
            'include_urgency_features': trial.suggest_categorical('include_urgency', [True, False]),
            'include_workload_balance': trial.suggest_categorical('include_workload', [True, False]),
            'state_history_length': trial.suggest_int('history_length', 1, 5),
            
            # ğŸŒŸ æ–°å¢ï¼šåŠ¨ä½œç©ºé—´ä¼˜åŒ–
            'action_masking': trial.suggest_categorical('action_masking', [True, False]),
            'priority_based_action_space': trial.suggest_categorical('priority_actions', [True, False])
        }
        
        return base_config

    def _suggest_reward_engineering(self, trial: optuna.Trial) -> Dict[str, Any]:
        """å¥–åŠ±å‡½æ•°å·¥ç¨‹ä¼˜åŒ–"""
        base_config = self._suggest_environment_design(trial)
        
        # ğŸŒŸ é«˜çº§å¥–åŠ±è®¾è®¡
        base_config['reward_config'] = {
            # åŸºç¡€å¥–åŠ±ç»„ä»¶
            'part_completion_reward': trial.suggest_float('part_reward', 5.0, 50.0),
            'order_completion_reward': trial.suggest_float('order_reward', 25.0, 200.0),
            
            # æ—¶é—´ç›¸å…³å¥–åŠ±
            'early_completion_bonus': trial.suggest_float('early_bonus', 0.0, 10.0),
            'lateness_penalty_type': trial.suggest_categorical('penalty_type', ['linear', 'quadratic', 'exponential']),
            'continuous_lateness_penalty': trial.suggest_float('lateness_penalty', -2.0, -0.1),
            
            # æ•ˆç‡å¥–åŠ±
            'idle_penalty': trial.suggest_float('idle_penalty', -5.0, -0.5),
            'work_bonus': trial.suggest_float('work_bonus', 0.5, 5.0),
            'utilization_bonus_weight': trial.suggest_float('util_bonus', 0.0, 5.0),
            
            # ğŸŒŸ åè°ƒå¥–åŠ±ï¼ˆå¤šæ™ºèƒ½ä½“ç‰¹æœ‰ï¼‰
            'coordination_bonus': trial.suggest_float('coordination_bonus', 0.0, 3.0),
            'bottleneck_awareness_bonus': trial.suggest_float('bottleneck_bonus', 0.0, 5.0),
            
            # ğŸŒŸ ç¨€ç– vs å¯†é›†å¥–åŠ±
            'reward_frequency': trial.suggest_categorical('reward_freq', ['step', 'completion', 'mixed']),
            'shaped_reward_weight': trial.suggest_float('shaped_weight', 0.0, 1.0),
            
            # ğŸŒŸ å¤šç›®æ ‡æƒé‡è‡ªåŠ¨è°ƒä¼˜
            'completion_weight': trial.suggest_float('completion_w', 0.3, 0.7),
            'tardiness_weight': trial.suggest_float('tardiness_w', 0.1, 0.4),
            'makespan_weight': trial.suggest_float('makespan_w', 0.05, 0.3),
            'utilization_weight': trial.suggest_float('utilization_w', 0.05, 0.2)
        }
        
        return base_config

    def _suggest_architecture_search(self, trial: optuna.Trial) -> Dict[str, Any]:
        """ç¥ç»ç½‘ç»œæ¶æ„æœç´¢"""
        base_config = self._suggest_reward_engineering(trial)
        
        # ğŸŒŸ é«˜çº§ç½‘ç»œæ¶æ„è®¾è®¡
        architecture_type = trial.suggest_categorical('architecture_type', 
                                                    ['standard', 'attention', 'residual', 'hierarchical'])
        
        if architecture_type == 'standard':
            # æ ‡å‡†å…¨è¿æ¥ç½‘ç»œ
            base_config['network_config'].update({
                'architecture_type': 'standard',
                'hidden_sizes': [
                    trial.suggest_categorical('hidden_1', [128, 256, 512, 768, 1024]),
                    trial.suggest_categorical('hidden_2', [64, 128, 256, 384, 512]),
                    trial.suggest_categorical('hidden_3', [0, 64, 128, 256])  # 0è¡¨ç¤ºä¸ä½¿ç”¨ç¬¬ä¸‰å±‚
                ]
            })
        
        elif architecture_type == 'attention':
            # æ³¨æ„åŠ›æœºåˆ¶ç½‘ç»œ
            base_config['network_config'].update({
                'architecture_type': 'attention',
                'attention_heads': trial.suggest_int('attention_heads', 2, 8),
                'attention_dim': trial.suggest_categorical('attention_dim', [64, 128, 256]),
                'use_self_attention': trial.suggest_categorical('self_attention', [True, False]),
                'use_cross_attention': trial.suggest_categorical('cross_attention', [True, False])
            })
        
        elif architecture_type == 'residual':
            # æ®‹å·®ç½‘ç»œ
            base_config['network_config'].update({
                'architecture_type': 'residual',
                'residual_blocks': trial.suggest_int('residual_blocks', 2, 6),
                'block_size': trial.suggest_categorical('block_size', [128, 256, 512])
            })
        
        elif architecture_type == 'hierarchical':
            # åˆ†å±‚ç½‘ç»œç»“æ„
            base_config['network_config'].update({
                'architecture_type': 'hierarchical',
                'low_level_dim': trial.suggest_categorical('low_level_dim', [64, 128, 256]),
                'high_level_dim': trial.suggest_categorical('high_level_dim', [128, 256, 512]),
                'hierarchy_levels': trial.suggest_int('hierarchy_levels', 2, 4)
            })
        
        # ğŸŒŸ æ¿€æ´»å‡½æ•°å’Œæ­£åˆ™åŒ–ä¼˜åŒ–
        base_config['network_config'].update({
            'activation_function': trial.suggest_categorical('activation', ['relu', 'gelu', 'swish', 'leaky_relu']),
            'batch_normalization': trial.suggest_categorical('batch_norm', [True, False]),
            'layer_normalization': trial.suggest_categorical('layer_norm', [True, False]),
            'gradient_clipping': trial.suggest_float('grad_clip', 0.5, 2.0),
            'weight_decay': trial.suggest_float('weight_decay', 1e-8, 0.01, log=True)
        })
        
        return base_config

    def _suggest_full_system(self, trial: optuna.Trial) -> Dict[str, Any]:
        """å…¨ç³»ç»Ÿä¼˜åŒ–ï¼ˆåŒ…å«æ‰€æœ‰ç»´åº¦ï¼‰"""
        base_config = self._suggest_architecture_search(trial)
        
        # ğŸ”§ æœ€ç»ˆä¿®å¤: å°†é«˜çº§é…ç½®è§£åŒ…åˆ°é¡¶å±‚ï¼Œç¡®ä¿æ•°æ®ç»“æ„ä¸€è‡´æ€§
        # ç®—æ³•çº§åˆ«çš„ä¼˜åŒ–
        base_config.update({
            # å¤šæ™ºèƒ½ä½“åè°ƒæœºåˆ¶
            'coordination_mechanism': trial.suggest_categorical('coordination', 
                                                               ['independent', 'parameter_sharing', 'attention_based', 'communication']),
            
            # ç»éªŒå›æ”¾å’Œå­¦ä¹ ç­–ç•¥
            'experience_replay': trial.suggest_categorical('exp_replay', [True, False]),
            'prioritized_replay': trial.suggest_categorical('prioritized', [True, False]),
            
            # æ¢ç´¢ç­–ç•¥
            'exploration_strategy': trial.suggest_categorical('exploration', 
                                                            ['epsilon_greedy', 'boltzmann', 'noisy_networks', 'parameter_noise']),
            'exploration_decay': trial.suggest_float('exploration_decay', 0.99, 0.999),
            
            # æ··åˆç­–ç•¥ï¼ˆRL + å¯å‘å¼ï¼‰
            'hybrid_strategy': trial.suggest_categorical('hybrid', [True, False]),
            'heuristic_weight': trial.suggest_float('heuristic_weight', 0.0, 0.5) if base_config.get('hybrid_strategy') else 0.0,
            'heuristic_type': trial.suggest_categorical('heuristic_type', ['SPT', 'EDD', 'FIFO']) if base_config.get('hybrid_strategy') else 'SPT',
            
            # è¯¾ç¨‹å­¦ä¹ ä¼˜åŒ–
            'adaptive_curriculum': trial.suggest_categorical('adaptive_curriculum', [True, False]),
            'curriculum_stages': trial.suggest_int('curriculum_stages', 2, 5),
            'stage_transition_threshold': trial.suggest_float('transition_threshold', 0.7, 0.95),
            'curriculum_strategy': trial.suggest_categorical('curriculum_strategy', 
                                                           ['difficulty_based', 'diversity_based', 'performance_based'])
        })
        
        return base_config

    def comprehensive_evaluation(self, model_path: str, trial_config: Dict[str, Any]) -> Tuple[float, Dict[str, float]]:
        """ç»¼åˆè¯„ä¼°ï¼šåœ¨å¤šä¸ªæµ‹è¯•é…ç½®ä¸Šè¯„ä¼°æ¨¡å‹"""
        from evaluation import evaluate_marl_model, STATIC_EVAL_CONFIG, GENERALIZATION_CONFIG_1, GENERALIZATION_CONFIG_2, GENERALIZATION_CONFIG_3
        if not self.config.enable_generalization_test:
            # ä»…åŸºå‡†æµ‹è¯•
            all_kpis, all_scores = evaluate_marl_model(
                model_path, 
                config=STATIC_EVAL_CONFIG,
                env_config_overrides=trial_config, # ğŸ”§ ä¿®å¤: ä¼ é€’å½“å‰è¯•éªŒçš„ç¯å¢ƒé…ç½®
                generate_gantt=False
            )
            if all_scores is None:
                return -1.0, {}
            
            return np.mean(all_scores), {
                'static_score': np.mean(all_scores),
                'static_std': np.std(all_scores)
            }
        
        # å¤šé…ç½®æ³›åŒ–æµ‹è¯•
        test_configs = [
            ("static", STATIC_EVAL_CONFIG),
            ("gen1", GENERALIZATION_CONFIG_1),
            ("gen2", GENERALIZATION_CONFIG_2),
            ("gen3", GENERALIZATION_CONFIG_3)
        ]
        
        all_results = {}
        total_score = 0.0
        valid_tests = 0
        
        for test_name, test_config in test_configs:
            try:
                all_kpis, all_scores = evaluate_marl_model(
                    model_path, 
                    config=test_config,
                    env_config_overrides=trial_config, # ğŸ”§ ä¿®å¤: ä¼ é€’å½“å‰è¯•éªŒçš„ç¯å¢ƒé…ç½®
                    generate_gantt=False
                )
                
                if all_scores is not None and len(all_scores) > 0:
                    mean_score = np.mean(all_scores)
                    all_results[f'{test_name}_score'] = mean_score
                    all_results[f'{test_name}_std'] = np.std(all_scores)
                    total_score += mean_score
                    valid_tests += 1
                else:
                    all_results[f'{test_name}_score'] = -1.0
                    all_results[f'{test_name}_std'] = 0.0
                    
            except Exception as e:
                print(f"âš ï¸ æµ‹è¯•é…ç½® {test_name} å¤±è´¥: {e}")
                all_results[f'{test_name}_score'] = -1.0
                all_results[f'{test_name}_std'] = 0.0
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°ï¼ˆæ‰€æœ‰æœ‰æ•ˆæµ‹è¯•çš„å¹³å‡åˆ†ï¼‰
        final_score = total_score / valid_tests if valid_tests > 0 else -1.0
        
        # ğŸŒŸ æ³›åŒ–èƒ½åŠ›è¯„ä¼°ï¼šå¥–åŠ±åœ¨ä¸åŒé…ç½®é—´è¡¨ç°ç¨³å®šçš„æ¨¡å‹
        if valid_tests > 1:
            scores = [all_results[f'{name}_score'] for name, _ in test_configs if all_results.get(f'{name}_score', -1) > 0]
            if len(scores) > 1:
                score_std = np.std(scores)
                # ç¨³å®šæ€§å¥–åŠ±ï¼šåˆ†æ•°è¶Šç¨³å®šï¼ˆæ ‡å‡†å·®è¶Šå°ï¼‰ï¼Œè·å¾—é¢å¤–å¥–åŠ±
                stability_bonus = max(0, 0.1 - score_std)  # æœ€å¤š10%çš„ç¨³å®šæ€§å¥–åŠ±
                final_score += stability_bonus
                all_results['stability_bonus'] = stability_bonus
                all_results['score_stability'] = score_std
        
        return final_score, all_results

    def objective_function(self, trial: optuna.Trial) -> float:
        """Optunaç›®æ ‡å‡½æ•°"""
        from mappo.ppo_marl_train import SimplePPOTrainer
        trial_start_time = time.time()
        
        # 1. æ ¹æ®ä¼˜åŒ–çº§åˆ«ç”Ÿæˆé…ç½®
        search_function = self.search_space_functions[self.config.optimization_level]
        optimization_delta = search_function(trial)
        
        print(f"\nğŸ”¬ Trial {trial.number}: å¼€å§‹ä¼˜åŒ– ({self.config.optimization_level.value})")
        print(f"ğŸ“‹ å½“å‰é…ç½®æ‘˜è¦: {len(optimization_delta)} ä¸ªé…ç½®ç±»åˆ«")
        
        try:
            # ğŸ”§ å…³é”®ä¿®å¤ V2: æ„å»ºå®Œæ•´çš„ã€è‡ªåŒ…å«çš„é…ç½®åŒ…ï¼Œä¸å†ä¿®æ”¹å…¨å±€çŠ¶æ€
            trial_config = {
                'PPO_NETWORK_CONFIG': copy.deepcopy(w_factory_config.PPO_NETWORK_CONFIG),
                'LEARNING_RATE_CONFIG': copy.deepcopy(w_factory_config.LEARNING_RATE_CONFIG),
                'ADAPTIVE_TRAINING_CONFIG': copy.deepcopy(w_factory_config.ADAPTIVE_TRAINING_CONFIG),
                'REWARD_CONFIG': copy.deepcopy(w_factory_config.REWARD_CONFIG),
                'ENHANCED_OBS_CONFIG': copy.deepcopy(w_factory_config.ENHANCED_OBS_CONFIG),
                'ACTION_CONFIG_ENHANCED': copy.deepcopy(w_factory_config.ACTION_CONFIG_ENHANCED)
            }
            
            # å°†Optunaå»ºè®®çš„å¢é‡é…ç½®åˆå¹¶åˆ°å®Œæ•´é…ç½®ä¸­
            if 'learning_rate_config' in optimization_delta:
                trial_config['LEARNING_RATE_CONFIG'].update(optimization_delta['learning_rate_config'])
            if 'network_config' in optimization_delta:
                trial_config['PPO_NETWORK_CONFIG'].update(optimization_delta['network_config'])
            if 'obs_config' in optimization_delta:
                trial_config['ENHANCED_OBS_CONFIG'].update(optimization_delta['obs_config'])
                # åŒæ­¥åŠ¨ä½œç©ºé—´
                if 'top_n_parts' in optimization_delta['obs_config']:
                    trial_config['ACTION_CONFIG_ENHANCED']['action_space_size'] = optimization_delta['obs_config']['top_n_parts'] + 1
            if 'reward_config' in optimization_delta:
                trial_config['REWARD_CONFIG'].update(optimization_delta['reward_config'])

            # ğŸ”§ æœ€ç»ˆä¿®å¤: å°†ç®—æ³•å’Œè¯¾ç¨‹å­¦ä¹ çš„é¡¶å±‚é…ç½®ä¹Ÿåˆå¹¶è¿›æ¥
            # è¿™äº›é”®ç›´æ¥å­˜åœ¨äº optimization_delta çš„é¡¶å±‚
            top_level_keys_to_copy = [
                'coordination_mechanism', 'experience_replay', 'prioritized_replay',
                'exploration_strategy', 'exploration_decay', 'hybrid_strategy',
                'heuristic_weight', 'heuristic_type', 'adaptive_curriculum',
                'curriculum_stages', 'stage_transition_threshold', 'curriculum_strategy'
            ]
            for key in top_level_keys_to_copy:
                if key in optimization_delta:
                    trial_config[key] = optimization_delta[key]
                    
            # 3. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
            trainer = SimplePPOTrainer(
                # ä»æ„å»ºå¥½çš„é…ç½®åŒ…ä¸­è·å–å­¦ä¹ ç‡
                initial_lr=trial_config['LEARNING_RATE_CONFIG'].get('initial_lr', 3e-4),
                total_train_episodes=self.config.max_train_episodes,
                steps_per_episode=800,
                training_targets=trial_config['ADAPTIVE_TRAINING_CONFIG'],
                env_config=trial_config  # ğŸ”§ ä¿®å¤ï¼šä¼ å…¥å®Œæ•´çš„ã€è‡ªåŒ…å«çš„é…ç½®åŒ…
            )
            
            # è®­ç»ƒ
            results = trainer.train(
                max_episodes=self.config.max_train_episodes,
                steps_per_episode=800,
                eval_frequency=20,
                adaptive_mode=True
            )
            
            if results is None:
                print(f"âŒ Trial {trial.number}: è®­ç»ƒå¤±è´¥")
                return -1.0
            
            # 4. ä¿å­˜æ¨¡å‹
            trial_model_dir = f"{self.results_dir}/trial_{trial.number}"
            os.makedirs(trial_model_dir, exist_ok=True)
            model_path = trainer.save_model(f"{trial_model_dir}/best_model")
            
            if not model_path:
                print(f"âŒ Trial {trial.number}: æ¨¡å‹ä¿å­˜å¤±è´¥")
                return -1.0
            
            # 5. ç»¼åˆè¯„ä¼°
            final_score, detailed_results = self.comprehensive_evaluation(model_path, trial_config)
            
            # 6. è®°å½•è¯•éªŒä¿¡æ¯
            trial_duration = time.time() - trial_start_time
            trial_info = {
                'trial_number': trial.number,
                'optimization_level': self.config.optimization_level.value,
                'configuration': optimization_delta, # åªè®°å½•å¢é‡éƒ¨åˆ†
                'final_score': float(final_score),
                'detailed_results': detailed_results,
                'training_episodes': len(trainer.iteration_times) if hasattr(trainer, 'iteration_times') else 0,
                'trial_duration': trial_duration,
                'model_path': model_path
            }
            
            # ä¿å­˜è¯¦ç»†ä¿¡æ¯
            with open(f"{trial_model_dir}/trial_info.json", 'w', encoding='utf-8') as f:
                json.dump(trial_info, f, indent=2, ensure_ascii=False)
            
            self.optimization_history.append(trial_info)
            
            print(f"âœ… Trial {trial.number}: å®Œæˆ")
            print(f"ğŸ“Š ç»¼åˆåˆ†æ•°: {final_score:.4f}")
            print(f"â±ï¸ ç”¨æ—¶: {trial_duration/60:.1f}åˆ†é’Ÿ")
            
            return final_score
            
        except Exception as e:
            print(f"âŒ Trial {trial.number}: å‘ç”Ÿé”™è¯¯ - {str(e)}")
            import traceback
            traceback.print_exc()
            return -1.0
            
        finally:
            # æ¢å¤é…ç½®
            pass # ä¸å†éœ€è¦æ¢å¤é…ç½®ï¼Œå› ä¸ºæ¯æ¬¡è¯•éªŒéƒ½æ˜¯è‡ªåŒ…å«çš„

    def run_optimization(self):
        """å¯åŠ¨ä¼˜åŒ–è¿‡ç¨‹"""
        print("="*80)
        print("ğŸš€ å¯åŠ¨MARLç³»ç»Ÿå…¨é¢è‡ªåŠ¨åŒ–ä¼˜åŒ–")
        print(f"ğŸ¯ ä¼˜åŒ–çº§åˆ«: {self.config.optimization_level.value}")
        print(f"ğŸ”¬ è®¡åˆ’è¯•éªŒ: {self.config.n_trials} æ¬¡")
        print(f"ğŸŒ æ³›åŒ–æµ‹è¯•: {'å¯ç”¨' if self.config.enable_generalization_test else 'ç¦ç”¨'}")
        print("="*80)
        
        # åˆ›å»ºOptuna study
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=42, n_startup_trials=10),
            pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=15)
        )
        
        start_time = time.time()
        
        try:
            # å¼€å§‹ä¼˜åŒ–
            study.optimize(
                self.objective_function,
                n_trials=self.config.n_trials,
                callbacks=[self._trial_callback]
            )
            
            # åˆ†æå’Œä¿å­˜ç»“æœ
            total_time = time.time() - start_time
            self._generate_comprehensive_report(study, total_time)
            
            return study
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­äº†ä¼˜åŒ–è¿‡ç¨‹")
            if len(study.trials) > 0:
                self._generate_comprehensive_report(study, time.time() - start_time)
            return study
        
        except Exception as e:
            print(f"\nâŒ ä¼˜åŒ–è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _trial_callback(self, study, trial):
        """è¯•éªŒå›è°ƒ"""
        if trial.value is not None:
            current_best = study.best_value if study.best_trial else -float('inf')
            print(f"ğŸ”„ è¯•éªŒè¿›åº¦: {len(study.trials)}/{self.config.n_trials} " +
                  f"| å½“å‰æœ€ä½³: {current_best:.4f} " +
                  f"| æœ¬æ¬¡: {trial.value:.4f}")

    def _generate_comprehensive_report(self, study, total_time):
        """ç”Ÿæˆç»¼åˆä¼˜åŒ–æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ‰ MARLç³»ç»Ÿå…¨é¢ä¼˜åŒ–å®Œæˆ!")
        print(f"â±ï¸ æ€»ç”¨æ—¶: {total_time/3600:.2f}å°æ—¶")
        print(f"ğŸ”¬ å®Œæˆè¯•éªŒ: {len(study.trials)}")
        print("="*80)
        
        if len(study.trials) == 0:
            print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„è¯•éªŒç»“æœ")
            return
        
        best_trial = study.best_trial
        
        print(f"\nğŸ† æœ€ä½³ä¼˜åŒ–ç»“æœ:")
        print(f"ğŸ“Š æœ€ä½³ç»¼åˆåˆ†æ•°: {best_trial.value:.4f}")
        print(f"ğŸ”¢ æœ€ä½³è¯•éªŒç¼–å·: {best_trial.number}")
        print(f"ğŸ›ï¸ ä¼˜åŒ–çº§åˆ«: {self.config.optimization_level.value}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        comprehensive_results = {
            'optimization_config': {
                'level': self.config.optimization_level.value,
                'n_trials': self.config.n_trials,
                'enable_generalization': self.config.enable_generalization_test,
                'max_train_episodes': self.config.max_train_episodes
            },
            'best_trial': {
                'number': best_trial.number,
                'score': best_trial.value,
                'params': best_trial.params
            },
            'optimization_history': self.optimization_history,
            'study_statistics': {
                'total_time_hours': total_time / 3600,
                'completed_trials': len(study.trials),
                'successful_trials': len([t for t in study.trials if t.value is not None and t.value > 0])
            },
            'timestamp': self.timestamp
        }
        
        results_path = f"{self.results_dir}/comprehensive_optimization_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {results_path}")
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        self._generate_optimization_insights(study)

    def _generate_optimization_insights(self, study):
        """ç”Ÿæˆä¼˜åŒ–è§è§£å’Œå»ºè®®"""
        print(f"\nğŸ’¡ ä¼˜åŒ–è§è§£å’Œå»ºè®®:")
        print("="*60)
        
        # å‚æ•°é‡è¦æ€§åˆ†æ
        try:
            importance = optuna.importance.get_param_importances(study)
            print(f"ğŸ“ˆ æœ€é‡è¦çš„å‚æ•° (Top 5):")
            for i, (param_name, importance_value) in enumerate(sorted(importance.items(), key=lambda x: x[1], reverse=True)[:5]):
                print(f"  {i+1}. {param_name}: {importance_value:.4f}")
        except:
            print("âš ï¸ å‚æ•°é‡è¦æ€§åˆ†æä¸å¯ç”¨")
        
        # æ€§èƒ½è¶‹åŠ¿åˆ†æ
        successful_trials = [t for t in study.trials if t.value is not None and t.value > 0]
        if len(successful_trials) >= 5:
            scores = [t.value for t in successful_trials]
            recent_scores = scores[-5:]  # æœ€è¿‘5æ¬¡è¯•éªŒ
            early_scores = scores[:5]    # å‰5æ¬¡è¯•éªŒ
            
            recent_avg = np.mean(recent_scores)
            early_avg = np.mean(early_scores)
            
            print(f"\nğŸ“Š æ€§èƒ½è¶‹åŠ¿åˆ†æ:")
            print(f"  æ—©æœŸå¹³å‡åˆ†æ•°: {early_avg:.4f}")
            print(f"  è¿‘æœŸå¹³å‡åˆ†æ•°: {recent_avg:.4f}")
            print(f"  æ”¹è¿›å¹…åº¦: {((recent_avg - early_avg) / early_avg * 100):+.1f}%")
        
        # é…ç½®å»ºè®®
        print(f"\nğŸ”§ ä¸‹ä¸€æ­¥ä¼˜åŒ–å»ºè®®:")
        if self.config.optimization_level != OptimizationLevel.FULL_SYSTEM:
            print(f"  1. å°è¯•æ›´é«˜çº§åˆ«çš„ä¼˜åŒ–: {OptimizationLevel.FULL_SYSTEM.value}")
        
        if self.config.n_trials < 100:
            print(f"  2. å¢åŠ è¯•éªŒæ¬¡æ•°ä»¥è·å¾—æ›´ç¨³å®šçš„ç»“æœ")
        
        if not self.config.enable_generalization_test:
            print(f"  3. å¯ç”¨æ³›åŒ–æµ‹è¯•ä»¥è¯„ä¼°æ¨¡å‹é²æ£’æ€§")
        
        print(f"  4. è€ƒè™‘é’ˆå¯¹ç‰¹å®šé¢†åŸŸçŸ¥è¯†è¿›è¡Œå®šåˆ¶åŒ–ä¼˜åŒ–")
        print(f"  5. å°è¯•é›†æˆå…¶ä»–MARLç®—æ³•ï¼ˆå¦‚MADDPGã€QMIXç­‰ï¼‰")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="MARLç³»ç»Ÿå…¨é¢è‡ªåŠ¨åŒ–ä¼˜åŒ–")
    parser.add_argument("--level", type=str, 
                       choices=['hyperparams', 'environment', 'reward', 'architecture', 'full'],
                       default='full',
                       help="ä¼˜åŒ–çº§åˆ«")
    parser.add_argument("--trials", type=int, default=100, help="ä¼˜åŒ–è¯•éªŒæ¬¡æ•°")
    parser.add_argument("--train_episodes", type=int, default=300, help="æ¯æ¬¡è¯•éªŒçš„æœ€å¤§è®­ç»ƒè½®æ•°")
    parser.add_argument("--eval_episodes", type=int, default=15, help="è¯„ä¼°å›åˆæ•°")
    parser.add_argument("--no_generalization", action="store_true", help="ç¦ç”¨æ³›åŒ–æµ‹è¯•")
    parser.add_argument("--output_dir", type=str, default="advanced_optimization_results", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¼˜åŒ–é…ç½®
    config = OptimizationConfig(
        n_trials=args.trials,
        n_eval_episodes=args.eval_episodes,
        max_train_episodes=args.train_episodes,
        optimization_level=OptimizationLevel(args.level),
        enable_generalization_test=not args.no_generalization,
        output_dir=args.output_dir
    )
    
    # åˆ›å»ºå¹¶å¯åŠ¨ä¼˜åŒ–å™¨
    optimizer = AdvancedMARL_Optimizer(config)
    study = optimizer.run_optimization()
    
    if study:
        print("\nâœ… å…¨é¢ä¼˜åŒ–ä»»åŠ¡å®Œæˆ!")
        print(f"ğŸ† æœ€ä½³åˆ†æ•°: {study.best_value:.4f}")
    else:
        print("\nâŒ ä¼˜åŒ–ä»»åŠ¡å¤±è´¥!")


if __name__ == "__main__":
    import multiprocessing
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    main()
