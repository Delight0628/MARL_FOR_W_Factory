"""
MAPPOå¹¶è¡ŒWorkeræ¨¡å—
====================
å®ç°å¤šè¿›ç¨‹å¹¶è¡Œç»éªŒé‡‡é›†

åŠŸèƒ½è¯´æ˜ï¼š
- æ¯ä¸ªWorkeråœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œç‹¬ç«‹çš„ç¯å¢ƒå®ä¾‹
- æ™ºèƒ½ä½“æ¡ä»¶åŒ–å…¨å±€çŠ¶æ€ï¼ˆæ‹¼æ¥one-hotç¼–ç ï¼‰
- æ”¯æŒGPU/CPUè®¾å¤‡ç®¡ç†å’Œå†…å­˜ä¼˜åŒ–
- å¤„ç†episodeæˆªæ–­å’Œbootstrapä»·å€¼ä¼°è®¡
"""

import os
# åœ¨å¯¼å…¥TensorFlowä¹‹å‰æ ¹æ®ç¯å¢ƒå˜é‡å½»åº•å±è”½GPUï¼Œé¿å…å­è¿›ç¨‹è®¾å¤‡æšä¸¾è§¦å‘å´©æºƒ
if os.environ.get('FORCE_WORKER_CPU', '0') == '1':
    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
import random
import numpy as np
import tensorflow as tf
import gymnasium as gym
from typing import Dict, List, Tuple, Any, Optional

# å¯¼å…¥é…ç½®å’Œä¾èµ–
import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from environments.w_factory_env import make_parallel_env
from mappo.ppo_buffer import ExperienceBuffer
from mappo.ppo_network import PPONetwork


def run_simulation_worker(network_weights: Dict[str, List[np.ndarray]],
                          state_dim: int, action_space: gym.spaces.Space, num_steps: int, seed: int, 
                          global_state_dim: int, network_config: Dict[str, Any], curriculum_config: Dict[str, Any] = None) -> Tuple[Dict[str, ExperienceBuffer], float, Optional[np.ndarray], bool, bool]:
    """
    å¹¶è¡ŒWorkerè¿›ç¨‹ï¼šè¿è¡Œç‹¬ç«‹ç¯å¢ƒå®ä¾‹å¹¶é‡‡é›†ç»éªŒ
    
    æ‰§è¡Œæµç¨‹ï¼š
    1. é…ç½®GPU/CPUè®¾å¤‡
    2. åˆ›å»ºç¯å¢ƒå’Œç½‘ç»œå®ä¾‹
    3. åŠ è½½ç½‘ç»œæƒé‡
    4. è¿è¡Œnum_stepsæ­¥ä»¿çœŸ
    5. è¿”å›ç»éªŒç¼“å†²åŒºå’Œç»Ÿè®¡æ•°æ®
    
    Args:
        network_weights: ç½‘ç»œæƒé‡ {'actor': [...], 'critic': [...]}
        state_dim: å±€éƒ¨è§‚æµ‹ç»´åº¦
        action_space: åŠ¨ä½œç©ºé—´ï¼ˆæ”¯æŒDiscrete/MultiDiscreteï¼‰
        num_steps: å•æ¬¡ä»¿çœŸæœ€å¤§æ­¥æ•°
        seed: éšæœºç§å­
        global_state_dim: å…¨å±€çŠ¶æ€ç»´åº¦ï¼ˆå«æ™ºèƒ½ä½“one-hotï¼‰
        network_config: ç½‘ç»œæ¶æ„é…ç½®å­—å…¸
        curriculum_config: è¯¾ç¨‹å­¦ä¹ /ä»»åŠ¡é…ç½®ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        tuple: (buffers, total_reward, last_values, terminated, graduated)
            - buffers: å„æ™ºèƒ½ä½“çš„ç»éªŒç¼“å†²åŒº
            - total_reward: æ€»å¥–åŠ±
            - last_values: æœ€ç»ˆçŠ¶æ€ä»·å€¼ï¼ˆç”¨äºbootstrapï¼‰
            - terminated: æ˜¯å¦æ­£å¸¸ç»ˆæ­¢
            - graduated: æ˜¯å¦è¾¾åˆ°è¯¾ç¨‹æ¯•ä¸šæ ‡å‡†
    """
    try:
        # ========== GPU/CPUè®¾å¤‡é…ç½® ==========
        try:
            import os as _os
            if _os.environ.get('FORCE_WORKER_CPU', '0') == '1':
                # å¼ºåˆ¶CPUæ¨¡å¼
                _os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
                try:
                    _gpus = tf.config.list_physical_devices('GPU')
                    if _gpus:
                        tf.config.set_visible_devices([], 'GPU')
                except Exception:
                    pass
            else:
                # å¯ç”¨GPUå†…å­˜å¢é•¿
                try:
                    _gpus = tf.config.list_physical_devices('GPU')
                    for _g in _gpus:
                        tf.config.experimental.set_memory_growth(_g, True)
                except Exception:
                    pass
            
            # é™åˆ¶å­è¿›ç¨‹çº¿ç¨‹æ•°ï¼Œæå‡ç¨³å®šæ€§
            tf.config.threading.set_intra_op_parallelism_threads(1)
            tf.config.threading.set_inter_op_parallelism_threads(1)
        except Exception:
            pass
            
        # ========== ç¯å¢ƒå’Œéšæœºç§å­åˆå§‹åŒ– ==========
        if curriculum_config:
            worker_seed = seed + curriculum_config.get('worker_id', 0)
            env_config = curriculum_config.copy()
        else:
            worker_seed = seed
            env_config = {}
        
        # è®¾ç½®workerç‹¬ç«‹çš„éšæœºç§å­
        np.random.seed(worker_seed)
        random.seed(worker_seed)
        tf.random.set_seed(worker_seed)

        # åˆ›å»ºä»¿çœŸç¯å¢ƒ
        env_config['training_mode'] = True
        env = make_parallel_env(env_config)

        # ========== æ™ºèƒ½ä½“æ¡ä»¶åŒ–å…¨å±€çŠ¶æ€ ==========
        # ä¸ºé›†ä¸­å¼Criticæ„é€ åŒ…å«æ™ºèƒ½ä½“èº«ä»½çš„å…¨å±€çŠ¶æ€
        agent_list = list(env.possible_agents)
        agent_to_index = {agent_id: idx for idx, agent_id in enumerate(agent_list)}
        num_agents = len(agent_list)

        def _condition_global_state(raw_global_state: np.ndarray, agent_id: str) -> np.ndarray:
            """å°†æ™ºèƒ½ä½“one-hotç¼–ç æ‹¼æ¥åˆ°å…¨å±€çŠ¶æ€"""
            one_hot = np.zeros((num_agents,), dtype=np.float32)
            idx = agent_to_index.get(agent_id, 0)
            one_hot[idx] = 1.0
            return np.concatenate([raw_global_state.astype(np.float32), one_hot], axis=0)

        # ========== ç½‘ç»œå®ä¾‹åŒ–å’Œæƒé‡åŠ è½½ ==========
        tf.keras.backend.clear_session()
        
        # æ„å»ºç½‘ç»œï¼ˆä»…ç”¨äºæ¨ç†ï¼Œä¸éœ€è¦ä¼˜åŒ–å™¨ï¼‰
        try:
            network = PPONetwork(
                state_dim=state_dim,
                action_space=action_space,
                lr=None,  # æ¨ç†æ¨¡å¼ï¼šä¸æ„å»ºä¼˜åŒ–å™¨ï¼ŒèŠ‚çœèµ„æº
                global_state_dim=global_state_dim,
                network_config=network_config
            )
        except Exception as _e_build:
            # é¿å…åœ¨å¼ºåˆ¶CPUæ—¶è¿›è¡Œä»»ä½•GPUè®¾å¤‡æšä¸¾ï¼Œç›´æ¥å›é€€åˆ°å¤–å±‚å¼‚å¸¸å¤„ç†
            if 'vector::_M_range_check' in str(_e_build):
                if os.environ.get('FORCE_WORKER_CPU', '0') == '1':
                    raise
                # éå¼ºåˆ¶CPUæƒ…å†µä¸‹ï¼Œå°è¯•GPUå›é€€
                try:
                    _gpus = tf.config.list_physical_devices('GPU')
                    if _gpus:
                        with tf.device('/GPU:0'):
                            network = PPONetwork(
                                state_dim=state_dim,
                                action_space=action_space,
                                lr=None,
                                global_state_dim=global_state_dim,
                                network_config=network_config
                            )
                    else:
                        raise
                except Exception:
                    raise
            else:
                raise
        
        # åŠ è½½ç½‘ç»œæƒé‡
        if network_weights:
            try:
                network.actor.set_weights(network_weights['actor'])
                network.critic.set_weights(network_weights['critic'])
            except (ValueError, RuntimeError) as e:
                print(f"âš ï¸ Worker {curriculum_config.get('worker_id', 'N/A')} æƒé‡åŠ è½½è­¦å‘Š: {e}")
                print(f"   å°è¯•é‡å»ºç½‘ç»œ...")
                # é‡å»ºç½‘ç»œä½œä¸ºfallback
                tf.keras.backend.clear_session()
                network = PPONetwork(
                    state_dim=state_dim,
                    action_space=action_space,
                    lr=None,
                    global_state_dim=global_state_dim,
                    network_config=network_config
                )
                # å†æ¬¡å°è¯•åŠ è½½
                network.actor.set_weights(network_weights['actor'])
                network.critic.set_weights(network_weights['critic'])

            # åŠ è½½ååšå¥å£®æ€§æ ¡éªŒï¼šè‹¥ä»ä¸ºè¿‘é›¶æƒé‡ï¼Œæ”¾å¼ƒè¯¥workeré‡‡æ ·ä»¥é¿å…å™ªå£°
            try:
                actor_sum = float(np.sum([np.sum(np.abs(w)) for w in network.actor.get_weights()]))
                critic_sum = float(np.sum([np.sum(np.abs(w)) for w in network.critic.get_weights()]))
                if not np.isfinite(actor_sum) or not np.isfinite(critic_sum) or (actor_sum + critic_sum) < 1e-8:
                    print(f"âš ï¸ Worker {curriculum_config.get('worker_id', 'N/A')} æƒé‡æ ¡éªŒå¤±è´¥ï¼ˆè¿‘é›¶æˆ–éæ•°ï¼‰ï¼Œè·³è¿‡æœ¬workeré‡‡æ ·ã€‚")
                    env.close()
                    return {}, 0.0, None, True, False
            except Exception:
                # æ ¡éªŒå¼‚å¸¸æ—¶å®‰å…¨é€€å‡º
                env.close()
                return {}, 0.0, None, True, False

        # ========== ä»¿çœŸå¾ªç¯ ==========
        buffers = {agent: ExperienceBuffer() for agent in env.possible_agents}
        observations, infos = env.reset(seed=worker_seed)
        episode_rewards = {agent: 0 for agent in env.possible_agents}
        total_episode_reward = 0
        terminated_by_graduation = False

        for step in range(num_steps):
            actions = {}
            values = {}
            action_probs = {}
            
            # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“è·å–åŠ¨ä½œ
            active_agents_in_step = list(env.agents)
            for agent in active_agents_in_step:
                if agent in observations:
                    state = tf.expand_dims(observations[agent], 0)
                    # ä½¿ç”¨æ™ºèƒ½ä½“æ¡ä»¶åŒ–çš„å…¨å±€çŠ¶æ€
                    conditioned_global = _condition_global_state(infos[agent]['global_state'], agent)
                    global_state = tf.expand_dims(conditioned_global, 0)
                    
                    # ğŸ”§ æ–°å¢ï¼šè·å–åŠ¨ä½œæ©ç ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    action_mask = infos[agent].get('action_mask', None)
                    
                    action, value, action_prob = network.get_action_and_value(state, global_state, action_mask)
                    
                    actions[agent] = action
                    values[agent] = value
                    action_probs[agent] = action_prob
            
            # ä¸ºæœªè§‚æµ‹åˆ°çš„æ™ºèƒ½ä½“æä¾›é»˜è®¤åŠ¨ä½œï¼ˆå…¼å®¹Discrete/MultiDiscreteï¼‰
            for agent in env.possible_agents:
                if agent not in actions:
                    sp = network.action_space
                    if isinstance(sp, gym.spaces.MultiDiscrete):
                        actions[agent] = np.zeros(len(sp.nvec), dtype=sp.dtype)
                    else:
                        actions[agent] = 0
            
            # æ‰§è¡Œç¯å¢ƒæ­¥è¿›
            next_observations, rewards, terminations, truncations, next_infos = env.step(actions)
            
            # å­˜å‚¨ç»éªŒæ•°æ®
            for agent in active_agents_in_step:
                if agent in observations:
                    buffers[agent].store(
                        observations[agent], 
                        _condition_global_state(infos[agent]['global_state'], agent), 
                        actions[agent], 
                        rewards[agent], 
                        values[agent],
                        action_probs[agent],
                        terminations[agent],
                        truncations[agent]
                    )
                    episode_rewards[agent] += rewards.get(agent, 0)

            observations = next_observations
            infos = next_infos

            # æ£€æŸ¥episodeç»ˆæ­¢æ¡ä»¶
            if any(terminations.values()) or any(truncations.values()):
                # æ£€æŸ¥è¯¾ç¨‹å­¦ä¹ æ¯•ä¸šæ ‡å¿—
                if 'final_stats' in infos[active_agents_in_step[0]] and \
                   infos[active_agents_in_step[0]].get('final_stats', {}).get('graduated', False):
                    terminated_by_graduation = True

                total_episode_reward = sum(episode_rewards.values())
                
                # æœªæ»¡ç¼“å†²åŒºæ—¶é‡ç½®ç¯å¢ƒç»§ç»­æ”¶é›†
                if step < num_steps - 1:
                    observations, infos = env.reset(seed=worker_seed + step + 1)
                    episode_rewards = {agent: 0 for agent in env.possible_agents}
                else:
                    # ç¼“å†²åŒºå·²æ»¡ï¼Œè®¡ç®—bootstrapä»·å€¼
                    last_values = {}
                    for agent in active_agents_in_step:
                        if agent in observations:
                            conditioned_global = _condition_global_state(infos[agent]['global_state'], agent)
                            global_state = tf.expand_dims(conditioned_global, 0)
                            # æˆªæ–­æ—¶ä½¿ç”¨criticä¼°è®¡ï¼Œå¦åˆ™ä¸º0
                            if truncations[agent]:
                                last_values[agent] = network.get_value(global_state)
                            else:
                                last_values[agent] = 0.0
                    
                    env.close()
                    return buffers, total_episode_reward, last_values, any(terminations.values()), terminated_by_graduation

        # æ­£å¸¸ç»“æŸï¼šè®¡ç®—æœ€ç»ˆä»·å€¼ä¼°è®¡
        last_values = {}
        active_agents_in_step = list(env.agents)
        for agent in active_agents_in_step:
            if agent in observations:
                conditioned_global = _condition_global_state(infos[agent]['global_state'], agent)
                global_state = tf.expand_dims(conditioned_global, 0)
                last_values[agent] = network.get_value(global_state)
        
        env.close()
        return buffers, total_episode_reward, last_values, False, False

    except Exception as e:
        import traceback
        print(f"Worker {curriculum_config.get('worker_id', 'N/A')} failed with error: {e}")
        traceback.print_exc()
        # è¿”å›ç©ºæ•°æ®ä»¥é˜²ä¸»è¿›ç¨‹å´©æºƒ
        return {}, 0.0, None, True, False


def _collect_experience_wrapper(args):
    """
    è¿›ç¨‹æ± å‚æ•°è§£åŒ…åŒ…è£…å‡½æ•°
    
    ç”¨äºå…¼å®¹ProcessPoolExecutorçš„submitæ–¹æ³•ï¼Œå°†å…ƒç»„å‚æ•°è§£åŒ…å
    è°ƒç”¨run_simulation_workerå‡½æ•°
    """
    # è§£åŒ…å‚æ•°å…ƒç»„
    actor_weights, critic_weights, state_dim, action_space, num_steps, seed, global_state_dim, network_config, curriculum_config = args
    
    # æ„å»ºç½‘ç»œæƒé‡å­—å…¸
    network_weights = {
        'actor': actor_weights,
        'critic': critic_weights
    }
    
    # è°ƒç”¨å®é™…çš„workerå‡½æ•°
    return run_simulation_worker(
        network_weights=network_weights,
        state_dim=state_dim,
        action_space=action_space,
        num_steps=num_steps,
        seed=seed,
        global_state_dim=global_state_dim,
        network_config=network_config,
        curriculum_config=curriculum_config
    )

