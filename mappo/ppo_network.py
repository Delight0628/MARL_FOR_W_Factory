"""
MAPPOç¥ç»ç½‘ç»œæ¨¡å—
==================
å®ç°Actorå’ŒCriticç½‘ç»œæ¶æ„ï¼Œæ”¯æŒé›†ä¸­å¼è®­ç»ƒåˆ†å¸ƒå¼æ‰§è¡Œ(CTDE)èŒƒå¼
"""

import os
import numpy as np
import tensorflow as tf
import gymnasium as gym
from typing import Dict, Tuple, Any, Optional

# å¯¼å…¥é…ç½®
import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)
from environments.w_factory_config import PPO_NETWORK_CONFIG, LEARNING_RATE_CONFIG, WORKSTATIONS


class PPONetwork:
    """
    MAPPOç¥ç»ç½‘ç»œæ¶æ„
    -------------------
    å®ç°é›†ä¸­å¼è®­ç»ƒåˆ†å¸ƒå¼æ‰§è¡Œ (CTDE) èŒƒå¼ï¼š
    - Actor: åŸºäºå±€éƒ¨è§‚æµ‹ç”ŸæˆåŠ¨ä½œåˆ†å¸ƒï¼ˆåˆ†å¸ƒå¼æ‰§è¡Œï¼‰
    - Critic: åŸºäºå…¨å±€çŠ¶æ€è¯„ä¼°ä»·å€¼ï¼ˆé›†ä¸­å¼è®­ç»ƒï¼‰
    
    ç‰¹æ€§ï¼š
    - æ”¯æŒMultiDiscreteåŠ¨ä½œç©ºé—´ï¼ˆå¤šå¤´è¾“å‡ºï¼‰
    - æ”¯æŒå­¦ä¹ ç‡è°ƒåº¦
    - æ¢¯åº¦è£å‰ªå’Œä¼˜åŠ¿è£å‰ª
    """
    
    def __init__(self, state_dim: int, action_space: gym.spaces.Space, lr: Any, global_state_dim: int, network_config: Optional[Dict[str, Any]] = None):
        """
        Args:
            state_dim: å±€éƒ¨è§‚æµ‹ç»´åº¦
            action_space: åŠ¨ä½œç©ºé—´ï¼ˆæ”¯æŒDiscreteå’ŒMultiDiscreteï¼‰
            lr: å­¦ä¹ ç‡æˆ–å­¦ä¹ ç‡è°ƒåº¦å™¨
            global_state_dim: å…¨å±€çŠ¶æ€ç»´åº¦ï¼ˆåŒ…å«æ™ºèƒ½ä½“one-hotï¼‰
            network_config: ç½‘ç»œæ¶æ„é…ç½®
        """
        self.state_dim = int(state_dim)
        self.action_space = action_space
        self.global_state_dim = int(global_state_dim)
        self.config = network_config or PPO_NETWORK_CONFIG
        
        # ====== ä»£ç†ä¸è®¾å¤‡æ•°æ˜ å°„ï¼ˆç”¨äºæ— æ•ˆå¤´æ©ç ï¼‰======
        # ä¸ç¯å¢ƒä¸­ agents çš„æ„é€ é¡ºåºä¿æŒä¸€è‡´ï¼šlist(WORKSTATIONS.keys())
        self._station_names_order = list(WORKSTATIONS.keys())
        self._agent_machine_counts = tf.constant(
            [int(WORKSTATIONS[name]['count']) for name in self._station_names_order], dtype=tf.int32
        )
        self._num_agents = int(len(self._station_names_order))

        # è§£æåŠ¨ä½œç©ºé—´ç±»å‹
        self.is_multidiscrete = isinstance(self.action_space, gym.spaces.MultiDiscrete)
        if self.is_multidiscrete:
            self.action_dims = self.action_space.nvec
        else:
            self.action_dims = [self.action_space.n]
            
        # ç¡®å®šæ€§åˆå§‹åŒ–æ ‡å¿—ï¼ˆè¯„ä¼°æ¨¡å¼ä½¿ç”¨ï¼Œé¿å…éšæœºæ•°é—®é¢˜ï¼‰
        self._deterministic_init = (lr is None) or bool(int(os.environ.get("DETERMINISTIC_INIT", "1")))

        self.actor, self.critic = self._build_networks()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼ˆæ”¯æŒå­¦ä¹ ç‡è°ƒåº¦ï¼‰
        if lr is not None:
            self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
            
            # Criticä½¿ç”¨è¾ƒä½å­¦ä¹ ç‡ä»¥ç¨³å®šä»·å€¼å­¦ä¹ 
            if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):
                critic_lr_multiplier = LEARNING_RATE_CONFIG.get("critic_lr_multiplier", 0.5)
                critic_lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
                    initial_learning_rate=LEARNING_RATE_CONFIG["initial_lr"] * critic_lr_multiplier,
                    decay_steps=lr.decay_steps,
                    end_learning_rate=LEARNING_RATE_CONFIG["end_lr"] * critic_lr_multiplier,
                    power=LEARNING_RATE_CONFIG["decay_power"]
                )
                self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr_schedule)
            else:
                critic_lr_value = lr * LEARNING_RATE_CONFIG.get("critic_lr_multiplier", 0.5)
                self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr_value)
        else:
            self.actor_optimizer = None
            self.critic_optimizer = None
        
    def _build_networks(self):
        """
        æ„å»ºActorå’ŒCriticç¥ç»ç½‘ç»œ
        
        Actoræ¶æ„ï¼š
        - è¾“å…¥ï¼šå±€éƒ¨è§‚æµ‹
        - ä¸»å¹²ï¼šå¤šå±‚å…¨è¿æ¥ç½‘ç»œ + ReLU + Dropout
        - è¾“å‡ºï¼šåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒï¼ˆMultiDiscreteæ—¶ä¸ºå¤šå¤´è¾“å‡ºï¼‰
        
        Criticæ¶æ„ï¼š
        - è¾“å…¥ï¼šå…¨å±€çŠ¶æ€ï¼ˆåŒ…å«æ™ºèƒ½ä½“one-hotï¼‰
        - ä¸»å¹²ï¼šå¤šå±‚å…¨è¿æ¥ç½‘ç»œ + ReLU + Dropout
        - è¾“å‡ºï¼šçŠ¶æ€ä»·å€¼ä¼°è®¡
        """
        config = self.config or PPO_NETWORK_CONFIG
        hidden_sizes = [int(max(1, hs)) for hs in config["hidden_sizes"]]
        dropout_rate = config.get("dropout_rate", 0.1)
        
        # ==================== Actorç½‘ç»œ ====================
        state_input = tf.keras.layers.Input(
            shape=(int(self.state_dim),), 
            dtype=tf.float32, 
            name="actor_input"
        )
        
        # Actorä¸»å¹²ç½‘ç»œ
        actor_x = state_input
        for i, hidden_size in enumerate(hidden_sizes):
            kernel_init = (tf.keras.initializers.Zeros() if self._deterministic_init 
                          else tf.keras.initializers.GlorotUniform(seed=1234 + i))
            actor_x = tf.keras.layers.Dense(
                hidden_size,
                activation="relu",
                kernel_initializer=kernel_init,
                bias_initializer=tf.keras.initializers.Zeros(),
                name=f"actor_dense_{i}"
            )(actor_x)
            if dropout_rate > 0:
                actor_x = tf.keras.layers.Dropout(dropout_rate, name=f"actor_dropout_{i}")(actor_x)

        # Actorè¾“å‡ºå±‚ï¼ˆæ”¯æŒMultiDiscreteå¤šå¤´ï¼‰
        if self.is_multidiscrete:
            num_heads = len(self.action_dims)
            action_dim_per_head = self.action_dims[0]
            actor_outputs = []
            for i in range(num_heads):
                kernel_init = (tf.keras.initializers.Zeros() if self._deterministic_init 
                              else tf.keras.initializers.GlorotUniform(seed=2000 + i))
                head_output = tf.keras.layers.Dense(
                    action_dim_per_head,
                    activation="softmax",
                    kernel_initializer=kernel_init,
                    bias_initializer=tf.keras.initializers.Zeros(),
                    name=f"actor_output_{i}"
                )(actor_x)
                actor_outputs.append(head_output)
            self.actor = tf.keras.Model(inputs=state_input, outputs=actor_outputs)
        else:
            kernel_init = (tf.keras.initializers.Zeros() if self._deterministic_init 
                          else tf.keras.initializers.GlorotUniform(seed=3000))
            actor_output = tf.keras.layers.Dense(
                self.action_dims[0],
                activation="softmax",
                kernel_initializer=kernel_init,
                bias_initializer=tf.keras.initializers.Zeros(),
                name="actor_output"
            )(actor_x)
            self.actor = tf.keras.Model(inputs=state_input, outputs=actor_output)

        # ==================== Criticç½‘ç»œï¼ˆé›†ä¸­å¼ï¼‰====================
        global_state_input = tf.keras.layers.Input(
            shape=(int(self.global_state_dim),), 
            dtype=tf.float32, 
            name="critic_input"
        )
        
        # Criticä¸»å¹²ç½‘ç»œ
        critic_x = global_state_input
        for i, hidden_size in enumerate(hidden_sizes):
            kernel_init = (tf.keras.initializers.Zeros() if self._deterministic_init 
                          else tf.keras.initializers.GlorotUniform(seed=4000 + i))
            critic_x = tf.keras.layers.Dense(
                hidden_size,
                activation="relu",
                kernel_initializer=kernel_init,
                bias_initializer=tf.keras.initializers.Zeros(),
                name=f"critic_dense_{i}"
            )(critic_x)
            if dropout_rate > 0:
                critic_x = tf.keras.layers.Dropout(dropout_rate, name=f"critic_dropout_{i}")(critic_x)

        # Criticä»·å€¼è¾“å‡ºå±‚
        value_kernel_init = (tf.keras.initializers.Zeros() if self._deterministic_init 
                            else tf.keras.initializers.Orthogonal(gain=1.0, seed=5000))
        value_output = tf.keras.layers.Dense(
            1,
            activation=None,
            kernel_initializer=value_kernel_init,
            bias_initializer=tf.keras.initializers.Constant(0.0),
            name="critic_value"
        )(critic_x)
        critic = tf.keras.Model(inputs=global_state_input, outputs=value_output)
        
        return self.actor, critic
    
    def get_action_and_value(self, state: np.ndarray, global_state: np.ndarray, action_mask: Optional[np.ndarray] = None) -> Tuple[int, np.float32, np.float32]:
        """
        é‡‡æ ·åŠ¨ä½œå¹¶è®¡ç®—çŠ¶æ€ä»·å€¼
        
        å®ç°è¦ç‚¹ï¼š
        - MultiDiscreteä½¿ç”¨æ— æ”¾å›é‡‡æ ·ï¼Œé¿å…å¤šæ™ºèƒ½ä½“é€‰æ‹©é‡å¤å€™é€‰
        - æ¨ç†æ¨¡å¼ï¼ˆtraining=Falseï¼‰ï¼Œç¦ç”¨Dropoutç­‰æ­£åˆ™åŒ–
        
        Args:
            state: å±€éƒ¨è§‚æµ‹çŠ¶æ€
            global_state: å…¨å±€çŠ¶æ€
            
        Returns:
            tuple: (action, value, log_prob)
                - action: é‡‡æ ·çš„åŠ¨ä½œï¼ˆMultiDiscreteæ—¶ä¸ºå‘é‡ï¼‰
                - value: çŠ¶æ€ä»·å€¼ä¼°è®¡
                - log_prob: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
        """
        action_probs = self.actor(state, training=False)
        value = self.critic(global_state, training=False)

        # MultiDiscreteï¼šå¤šå¤´æ— æ”¾å›é‡‡æ ·ï¼ˆå¸¦æ— æ•ˆå¤´æ©ç ï¼‰
        if self.is_multidiscrete:
            action_prob_list = action_probs if isinstance(action_probs, list) else [action_probs]
            
            # æ ‡å‡†åŒ–å„å¤´å¼ é‡å½¢çŠ¶ä¸º (B, action_dim)
            normalized_heads = []
            for head_probs in action_prob_list:
                hp = tf.convert_to_tensor(head_probs, dtype=tf.float32)
                if tf.rank(hp) == 1:
                    hp = tf.expand_dims(hp, 0)
                hp = tf.reshape(hp, (-1, int(self.action_dims[0])))
                normalized_heads.append(hp)
            action_prob_list = normalized_heads
            
            num_heads = len(self.action_dims)
            action_dim = int(self.action_dims[0])
            chosen_actions = []
            log_prob_list = []
            mask = tf.zeros_like(action_prob_list[0], dtype=tf.bool)
            
            # ä»å…¨å±€çŠ¶æ€ä¸­è§£æ agent ç´¢å¼•ä¸æœ‰æ•ˆå¤´æ•°é‡ kï¼ˆè¯¥ç«™ç‚¹è®¾å¤‡æ•°ï¼‰
            gs = tf.convert_to_tensor(global_state, dtype=tf.float32)
            if len(gs.shape) == 1:
                gs = tf.expand_dims(gs, 0)
            agent_one_hot = gs[:, -self._num_agents:]
            agent_idx = tf.argmax(agent_one_hot, axis=1, output_type=tf.int32)  # (B,)
            k_valid_heads = tf.gather(self._agent_machine_counts, agent_idx)      # (B,)
            
            # é€å¤´é‡‡æ ·ï¼Œå±è”½å·²é€‰åŠ¨ä½œ
            for i in range(num_heads):
                probs_i = tf.convert_to_tensor(action_prob_list[i], dtype=tf.float32)
                probs_i = tf.reshape(probs_i, (-1, action_dim))
                
                # ğŸ”§ æ–°å¢ï¼šåº”ç”¨åŠ¨ä½œæ©ç ï¼ˆå¦‚æœæä¾›ï¼‰
                if action_mask is not None:
                    action_mask_tf = tf.convert_to_tensor(action_mask, dtype=tf.float32)
                    if len(action_mask_tf.shape) == 1:
                        action_mask_tf = tf.expand_dims(action_mask_tf, 0)  # (1, action_dim)
                    # å°†æ©ç åº”ç”¨åˆ°æ¦‚ç‡åˆ†å¸ƒ
                    probs_i = probs_i * action_mask_tf
                
                # å½“å‰å¤´åœ¨æ¯ä¸ªæ ·æœ¬ä¸­æ˜¯å¦æœ‰æ•ˆï¼ši < k_valid_heads
                valid = tf.less(tf.cast(i, tf.int32), k_valid_heads)  # (B,)
                valid_f = tf.cast(valid, tf.float32)                  # (B,)
                valid_f_ex = tf.expand_dims(valid_f, axis=1)          # (B,1)

                # å±è”½å·²é€‰åŠ¨ä½œå¹¶é‡å½’ä¸€åŒ–ï¼ˆä»…å¯¹æœ‰æ•ˆæ ·æœ¬ç”Ÿæ•ˆï¼‰
                masked_probs_full = tf.where(mask, tf.zeros_like(probs_i), probs_i)
                norm_full = tf.reduce_sum(masked_probs_full, axis=1, keepdims=True) + 1e-8
                masked_probs_full = masked_probs_full / norm_full
                logits_full = tf.math.log(masked_probs_full + 1e-8)

                # é‡‡æ ·ï¼ˆæœ‰æ•ˆæ ·æœ¬ç”¨çœŸå®åˆ†å¸ƒï¼›æ— æ•ˆæ ·æœ¬å›ºå®šä¸º0=IDLEï¼‰
                sampled_valid = tf.squeeze(tf.random.categorical(logits_full, 1), axis=1)  # (B,)
                sampled = tf.where(valid, sampled_valid, tf.zeros_like(sampled_valid))
                chosen_actions.append(sampled)

                # å¯¹æ•°æ¦‚ç‡ï¼ˆæ— æ•ˆæ ·æœ¬è®°ä¸º0è´¡çŒ®ï¼‰
                action_one_hot = tf.one_hot(sampled, action_dim)
                log_prob_for_head_full = tf.reduce_sum(logits_full * action_one_hot, axis=1)  # (B,)
                log_prob_for_head = log_prob_for_head_full * valid_f
                log_prob_list.append(log_prob_for_head)

                # æ›´æ–°æ©ç ï¼ˆä»…å¯¹æœ‰æ•ˆæ ·æœ¬æ›´æ–°ï¼‰ï¼Œä¸”å…è®¸å¤šä¸ªå¤´é€‰æ‹© IDLE(0)
                is_idle = tf.equal(sampled, tf.zeros_like(sampled))  # (B,)
                not_idle_ex = tf.expand_dims(tf.cast(tf.logical_not(is_idle), tf.bool), axis=1)  # (B,1)
                mask_update = tf.logical_and(tf.cast(action_one_hot > 0, tf.bool), tf.cast(valid_f_ex > 0, tf.bool))
                mask_update = tf.logical_and(mask_update, not_idle_ex)
                mask = tf.logical_or(mask, mask_update)

            action = tf.stack(chosen_actions, axis=1)
            joint_log_prob = tf.add_n(log_prob_list)
            
            # è½¬æ¢ä¸ºnumpyå¹¶å¤„ç†æ‰¹æ¬¡ç»´åº¦
            action_np = action.numpy()
            if action_np.ndim == 2 and action_np.shape[0] >= 1:
                action_out = action_np[0]
            elif action_np.ndim == 1:
                action_out = action_np
            else:
                action_out = np.zeros((num_heads,), dtype=self.action_space.dtype)

            joint_log_prob_np = joint_log_prob.numpy()
            if np.ndim(joint_log_prob_np) == 0:
                jlp_out = float(joint_log_prob_np)
            elif len(joint_log_prob_np) >= 1:
                jlp_out = float(joint_log_prob_np[0])
            else:
                jlp_out = 0.0

            return action_out, float(tf.squeeze(value).numpy()), jlp_out
        
        # Discreteï¼šå•å¤´é‡‡æ ·
        else:
            # ğŸ”§ æ–°å¢ï¼šåº”ç”¨åŠ¨ä½œæ©ç ï¼ˆå¦‚æœæä¾›ï¼‰
            probs_to_use = action_probs
            if action_mask is not None:
                action_mask_tf = tf.convert_to_tensor(action_mask, dtype=tf.float32)
                if len(action_mask_tf.shape) == 1:
                    action_mask_tf = tf.expand_dims(action_mask_tf, 0)  # (1, action_dim)
                probs_to_use = action_probs * action_mask_tf
                # é‡å½’ä¸€åŒ–
                norm = tf.reduce_sum(probs_to_use, axis=1, keepdims=True) + 1e-8
                probs_to_use = probs_to_use / norm
            
            sampled = tf.random.categorical(tf.math.log(probs_to_use + 1e-8), 1)
            action_one_hot = tf.one_hot(tf.squeeze(sampled, axis=-1), self.action_dims[0])
            action_prob = tf.math.log(tf.reduce_sum(probs_to_use * action_one_hot, axis=1) + 1e-8)
            action = tf.squeeze(sampled, axis=-1)
            action_np = action.numpy()
            return action_np, float(tf.squeeze(value).numpy()), float(tf.squeeze(action_prob).numpy())
    
    def get_value(self, global_state: np.ndarray) -> float:
        """
        è·å–å…¨å±€çŠ¶æ€çš„ä»·å€¼ä¼°è®¡
        
        Args:
            global_state: å…¨å±€çŠ¶æ€ï¼ˆæ”¯æŒæ‰¹æ¬¡æˆ–å•æ ·æœ¬ï¼‰
            
        Returns:
            çŠ¶æ€ä»·å€¼ä¼°è®¡ï¼ˆæ ‡é‡ï¼‰
        """
        gs = tf.convert_to_tensor(global_state)
        if len(gs.shape) == 1:
            gs = tf.expand_dims(gs, 0)
        return float(self.critic(gs)[0, 0])
    
    def update(self, states: np.ndarray, global_states: np.ndarray, actions: np.ndarray, 
               old_probs: np.ndarray, advantages: np.ndarray, 
               returns: np.ndarray, clip_ratio: float = None, entropy_coeff: float = None) -> Dict[str, float]:
        """
        æ‰§è¡Œä¸€æ¬¡PPOç­–ç•¥æ›´æ–°
        
        å®ç°æ ‡å‡†PPO-Clipç®—æ³•ï¼ŒåŒ…å«ï¼š
        1. Actoræ›´æ–°ï¼šClipç›®æ ‡ + ç†µå¥–åŠ±
        2. Criticæ›´æ–°ï¼šå‡æ–¹è¯¯å·®æŸå¤±
        3. æ¢¯åº¦è£å‰ªï¼šæå‡è®­ç»ƒç¨³å®šæ€§
        
        Args:
            states: å±€éƒ¨è§‚æµ‹æ‰¹æ¬¡
            global_states: å…¨å±€çŠ¶æ€æ‰¹æ¬¡
            actions: æ‰§è¡Œçš„åŠ¨ä½œ
            old_probs: æ—§ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
            advantages: GAEä¼˜åŠ¿å‡½æ•°
            returns: ç›®æ ‡å›æŠ¥
            clip_ratio: PPOè£å‰ªæ¯”ä¾‹ (é»˜è®¤0.2)
            entropy_coeff: ç†µæ­£åˆ™åŒ–ç³»æ•° (é»˜è®¤0.01)
            
        Returns:
            dict: æŸå¤±ç»Ÿè®¡ {actor_loss, critic_loss, entropy, approx_kl, clip_fraction}
        """
        clip_ratio = clip_ratio or self.config.get("clip_ratio", 0.2)
        entropy_coeff = entropy_coeff or self.config.get("entropy_coeff", 0.01)

        # ç»Ÿä¸€dtypeï¼Œé¿å…TFè¿ç®—çš„float32/float64æ··ç”¨é”™è¯¯
        states = tf.convert_to_tensor(states, dtype=tf.float32)
        global_states = tf.convert_to_tensor(global_states, dtype=tf.float32)
        # åŠ¨ä½œç”¨äºone_hotç´¢å¼•ï¼Œéœ€æ•´å‹
        actions = tf.convert_to_tensor(actions, dtype=tf.int32)
        old_probs = tf.convert_to_tensor(old_probs, dtype=tf.float32)
        advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)
        returns = tf.convert_to_tensor(returns, dtype=tf.float32)

        with tf.GradientTape() as tape:
            # ========== ActoræŸå¤±è®¡ç®— ==========
            # å‰å‘ä¼ æ’­ï¼šä¸é‡‡æ ·ä¸€è‡´ï¼Œç¦ç”¨Dropoutç­‰æ­£åˆ™åŒ–
            action_probs = self.actor(states, training=False)
            
            # å¤„ç†MultiDiscreteå¤šå¤´è¾“å‡º
            if self.is_multidiscrete:
                action_prob_list = action_probs if isinstance(action_probs, list) else [action_probs]
                num_heads = len(self.action_dims)
                log_probs_list = []
                entropy_list = []

                # æ ‡å‡†åŒ–å„å¤´å¼ é‡å½¢çŠ¶ä¸º (B, action_dim)
                normalized_heads = []
                for head_probs in action_prob_list:
                    hp = tf.convert_to_tensor(head_probs, dtype=tf.float32)
                    if tf.rank(hp) == 1:
                        hp = tf.expand_dims(hp, 0)
                    hp = tf.reshape(hp, (-1, int(self.action_dims[0])))
                    normalized_heads.append(hp)
                action_prob_list = normalized_heads

                # è§£æ agent ç´¢å¼•ä¸æœ‰æ•ˆå¤´æ•°é‡ kï¼ˆè¯¥ç«™ç‚¹è®¾å¤‡æ•°ï¼‰
                gs = tf.convert_to_tensor(global_states, dtype=tf.float32)
                agent_one_hot = gs[:, -self._num_agents:]
                agent_idx = tf.argmax(agent_one_hot, axis=1, output_type=tf.int32)  # (B,)
                k_valid_heads = tf.gather(self._agent_machine_counts, agent_idx)      # (B,)

                # æŒ‰é‡‡æ ·é˜¶æ®µçš„ä¸€è‡´é¡ºåºè¿›è¡Œâ€œæ— æ”¾å›â€æ©ç ä¸é‡å½’ä¸€åŒ–ï¼ˆå«æ— æ•ˆå¤´æ©ç ï¼‰
                mask = tf.zeros_like(action_prob_list[0], dtype=tf.bool)
                for i in range(num_heads):
                    head_probs = action_prob_list[i]
                    action_slice = actions[:, i]
                    action_one_hot = tf.one_hot(action_slice, int(self.action_dims[0]))

                    # å½“å‰å¤´æ˜¯å¦æœ‰æ•ˆ
                    valid = tf.less(tf.cast(i, tf.int32), k_valid_heads)  # (B,)
                    valid_f = tf.cast(valid, tf.float32)
                    valid_f_ex = tf.expand_dims(valid_f, axis=1)

                    # åº”ç”¨æ©ç å¹¶é‡å½’ä¸€åŒ–
                    masked_probs = tf.where(mask, tf.zeros_like(head_probs), head_probs)
                    norm = tf.reduce_sum(masked_probs, axis=1, keepdims=True) + 1e-8
                    masked_probs = masked_probs / norm

                    # é€‰ä¸­åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ï¼ˆåœ¨æ©ç åçš„åˆ†å¸ƒä¸‹ï¼Œä¸”ä»…å¯¹æœ‰æ•ˆå¤´è®¡å…¥ï¼‰
                    log_prob_for_head_full = tf.math.log(tf.reduce_sum(masked_probs * action_one_hot, axis=1) + 1e-8)
                    log_prob_for_head = log_prob_for_head_full * valid_f
                    log_probs_list.append(log_prob_for_head)

                    # ä½¿ç”¨æ©ç åçš„åˆ†å¸ƒè®¡ç®—ç†µï¼Œä¿æŒä¸é‡‡æ ·ä¸€è‡´ï¼ˆæ— æ•ˆå¤´è´¡çŒ®ä¸º0ï¼‰
                    entropy_for_head_full = -tf.reduce_sum(masked_probs * tf.math.log(masked_probs + 1e-8), axis=1)
                    entropy_for_head = entropy_for_head_full * valid_f
                    entropy_list.append(entropy_for_head)

                    # æ›´æ–°æ©ç ï¼ˆä»…å¯¹æœ‰æ•ˆæ ·æœ¬æ›´æ–°ï¼‰ï¼Œä¸”å…è®¸å¤šä¸ªå¤´é€‰æ‹© IDLE(0)
                    is_idle = tf.equal(action_slice, 0)
                    not_idle_ex = tf.expand_dims(tf.cast(tf.logical_not(is_idle), tf.bool), axis=1)
                    mask_update = tf.logical_and(tf.cast(action_one_hot > 0, tf.bool), tf.cast(valid_f_ex > 0, tf.bool))
                    mask_update = tf.logical_and(mask_update, not_idle_ex)
                    mask = tf.logical_or(mask, mask_update)

                # è”åˆå¯¹æ•°æ¦‚ç‡ä¸æ€»ç†µ
                current_probs = tf.add_n(log_probs_list)
                entropy_loss = tf.add_n(entropy_list)
            else:
                # Discreteå•å¤´è¾“å‡º
                action_one_hot = tf.one_hot(tf.squeeze(actions), self.action_dims[0])
                log_prob = tf.math.log(tf.reduce_sum(action_probs * action_one_hot, axis=1) + 1e-8)
                current_probs = log_prob
                entropy_loss = -tf.reduce_sum(action_probs * tf.math.log(action_probs + 1e-8), axis=1)

            # PPOè£å‰ªæŸå¤±
            ratio = tf.exp(current_probs - old_probs)
            clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)
            
            # è®¡ç®—è£å‰ªæ¯”ä¾‹ï¼ˆç”¨äºç›‘æ§ï¼‰
            clipped_mask = tf.greater(tf.abs(ratio - 1.0), clip_ratio)
            clip_fraction = tf.reduce_mean(tf.cast(clipped_mask, tf.float32))

            # Actoræ€»æŸå¤± = ç­–ç•¥æŸå¤± - ç†µå¥–åŠ±
            actor_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))
            entropy_mean = tf.reduce_mean(entropy_loss)
            actor_loss -= entropy_coeff * entropy_mean
            
        # Actoræ¢¯åº¦æ›´æ–°
        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)
        grad_clip_norm = self.config.get("grad_clip_norm", 1.0)
        actor_grads, _ = tf.clip_by_global_norm(actor_grads, grad_clip_norm)
        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))
        
        # ========== CriticæŸå¤±è®¡ç®— ==========
        with tf.GradientTape() as tape:
            values = self.critic(global_states, training=False)
            # æ˜¾å¼ç»Ÿä¸€ dtypeï¼Œé¿å… float64/float32 æ··ç”¨
            values = tf.cast(values, tf.float32)
            returns_expanded = tf.expand_dims(returns, 1) if len(returns.shape) == 1 else returns
            returns_expanded = tf.cast(returns_expanded, tf.float32)
            critic_loss = tf.reduce_mean(tf.square(returns_expanded - values))
        
        # Criticæ¢¯åº¦æ›´æ–°
        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)
        critic_grads, _ = tf.clip_by_global_norm(critic_grads, grad_clip_norm)
        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))
        
        # è¿‘ä¼¼KLï¼šE[old_logp - new_logp]
        approx_kl = tf.reduce_mean(old_probs - current_probs)
        return {
            "actor_loss": actor_loss.numpy(),
            "critic_loss": critic_loss.numpy(),
            "entropy": entropy_mean.numpy(),
            "approx_kl": float(approx_kl.numpy()),
            "clip_fraction": clip_fraction.numpy()
        }

