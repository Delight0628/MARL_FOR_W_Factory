"""
çº¯å‡€çš„å¤šæ™ºèƒ½ä½“PPOè®­ç»ƒè„šæœ¬
"""

import os
# ğŸ”§ V10.2 ç»ˆææ—¥å¿—æ¸…ç†: åœ¨æ‰€æœ‰åº“å¯¼å…¥å‰ï¼Œå¼ºåˆ¶è®¾ç½®æ—¥å¿—çº§åˆ«
# è¿™èƒ½æœ€æœ‰æ•ˆåœ°å±è”½æ‰CUDAå’ŒcuBLASåœ¨å­è¿›ç¨‹ä¸­çš„åˆå§‹åŒ–é”™è¯¯ä¿¡æ¯
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'


import sys
import time
import random
import numpy as np
import tensorflow as tf
import socket
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

# ğŸ”§ V12 æ–°å¢ï¼šTensorBoardæ”¯æŒï¼ˆåŸºäº TF2 æ­£ç¡®æ£€æµ‹ï¼‰
TENSORBOARD_AVAILABLE = hasattr(tf.summary, "create_file_writer")

# æ·»åŠ ç¯å¢ƒè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from environments.w_factory_env import make_parallel_env, WFactoryEnv
from environments.w_factory_config import *
from environments.w_factory_config import validate_config, get_total_parts_count, generate_random_orders, calculate_episode_score, ADAPTIVE_ENTROPY_CONFIG, EVALUATION_CONFIG

class ExperienceBuffer:
    """ğŸ”§ MAPPOç»éªŒç¼“å†²åŒº - æ”¯æŒå…¨å±€çŠ¶æ€"""
    
    def __init__(self):
        self.states = []
        self.global_states = []  # ğŸ”§ æ–°å¢ï¼šå­˜å‚¨å…¨å±€çŠ¶æ€
        self.actions = []
        self.rewards = []
        self.values = []
        self.action_probs = []
        self.dones = []
        self.truncateds = []
        
    def store(self, state, global_state, action, reward, value, action_prob, done, truncated=False):
        self.states.append(state)
        self.global_states.append(global_state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.values.append(value)
        self.action_probs.append(action_prob)
        self.dones.append(done)
        self.truncateds.append(truncated)
    
    def get_batch(self, gamma=0.99, lam=0.95, next_value_if_truncated=None, advantage_clip_val: Optional[float] = None):
        """ğŸ”§ MAPPOæ”¹è¿›ï¼šæ­£ç¡®å¤„ç†è½¨è¿¹æˆªæ–­ï¼Œå¹¶æ”¯æŒä¼˜åŠ¿è£å‰ª"""
        states = np.array(self.states, dtype=np.float32)
        global_states = np.array(self.global_states, dtype=np.float32)
        actions = np.array(self.actions)
        rewards = np.array(self.rewards, dtype=np.float32)
        values = np.array(self.values, dtype=np.float32)
        action_probs = np.array(self.action_probs, dtype=np.float32)
        dones = np.array(self.dones)
        truncateds = np.array(self.truncateds)
        
        advantages = np.zeros_like(rewards)
        last_advantage = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ­£ç¡®å¤„ç†æœ€åä¸€æ­¥
                if truncateds[t] and next_value_if_truncated is not None:
                    # æˆªæ–­ï¼šä½¿ç”¨criticé¢„æµ‹çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ä»·å€¼
                    next_value = next_value_if_truncated
                elif dones[t]:
                    # çœŸæ­£ç»ˆæ­¢ï¼šä»·å€¼ä¸º0
                    next_value = 0
                else:
                    # ğŸ”§ ä¿®å¤ï¼šæ—¢ä¸æˆªæ–­ä¹Ÿä¸ç»ˆæ­¢ï¼ˆæ­£å¸¸trajectoryç»“æŸï¼‰
                    # ä½¿ç”¨bootstrapä»·å€¼ï¼ˆå¦‚æœæä¾›ï¼‰
                    next_value = next_value_if_truncated if next_value_if_truncated is not None else 0
            else:
                next_value = values[t + 1]
            
            # GAEè®¡ç®—
            delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
            advantages[t] = delta + gamma * lam * (1 - dones[t]) * last_advantage
            last_advantage = advantages[t]
        
        returns = advantages + values
        
        # ğŸ”§ MAPPOä¿®å¤ï¼šæ›´ç¨³å¥çš„ä¼˜åŠ¿æ ‡å‡†åŒ–ï¼Œå¤„ç†è¾¹ç•Œæƒ…å†µ
        if len(advantages) > 1:
            adv_mean = np.mean(advantages)
            adv_std = np.std(advantages)
            # åªæœ‰å½“æ ‡å‡†å·®è¶³å¤Ÿå¤§æ—¶æ‰è¿›è¡Œå®Œæ•´æ ‡å‡†åŒ–
            if adv_std > 1e-6:  # æé«˜é˜ˆå€¼ï¼Œé¿å…æ•°å€¼ä¸ç¨³å®š
                advantages = (advantages - adv_mean) / (adv_std + 1e-8)
            else:
                # æ ‡å‡†å·®å¤ªå°æ—¶åªè¿›è¡Œå»å‡å€¼ï¼Œä¸è¿›è¡Œç¼©æ”¾
                advantages = advantages - adv_mean
        # å•æ ·æœ¬æƒ…å†µï¼šä¸è¿›è¡Œä»»ä½•æ ‡å‡†åŒ–ï¼Œä¿æŒåŸå€¼
        
        # ğŸ”§ ç¼ºé™·ä¿®å¤ï¼šä½¿ç”¨é…ç½®åŒ–çš„ä¼˜åŠ¿è£å‰ª
        if advantage_clip_val is not None:
            advantages = np.clip(advantages, -advantage_clip_val, advantage_clip_val)
        
        return states, global_states, actions, action_probs, advantages, returns
    
    def clear(self):
        self.states.clear()
        self.global_states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.values.clear()
        self.action_probs.clear()
        self.dones.clear()
        self.truncateds.clear()

class PPONetwork:
    """ğŸ”§ MAPPOç½‘ç»œå®ç° - åŒ…å«é›†ä¸­å¼Critic"""
    
    # ğŸ”§ V3 ä¿®å¤: lrå‚æ•°ç°åœ¨å¯ä»¥æ˜¯å­¦ä¹ ç‡è°ƒåº¦å™¨
    def __init__(self, state_dim: int, action_dim: int, lr: Any, global_state_dim: int, network_config: Optional[Dict[str, Any]] = None):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.global_state_dim = global_state_dim # ğŸ”§ æ–°å¢
        self.lr = lr
        self.network_config_override = network_config
        
        # æ„å»ºç½‘ç»œ
        self.actor, self.critic = self._build_networks()
        
        # ä¼˜åŒ–å™¨ - ğŸ”§ ä¿®å¤ï¼šå¤„ç†lrä¸ºNoneçš„æƒ…å†µï¼ˆworkerä¸éœ€è¦ä¼˜åŒ–å™¨ï¼‰
        if lr is not None:
            # ä¸“å®¶ä¿®å¤ï¼šä¸ºCriticè®¾ç½®ä¸€ä¸ªè¾ƒä½çš„å­¦ä¹ ç‡ä¹˜æ•°
            critic_lr = lr
            if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):
                # å¦‚æœlræ˜¯è°ƒåº¦å™¨ï¼Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥ä¹˜ï¼Œä½†å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„è°ƒåº¦å™¨æˆ–åœ¨ä¼˜åŒ–å™¨å±‚é¢å¤„ç†
                # Adamä¼˜åŒ–å™¨æ”¯æŒåœ¨åˆ›å»ºæ—¶ä¼ å…¥å­¦ä¹ ç‡è°ƒåº¦å™¨
                pass # ä¼˜åŒ–å™¨å°†ç›´æ¥ä½¿ç”¨è°ƒåº¦å™¨
            
            self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¸ºCriticåˆ›å»ºç›¸åŒç±»å‹çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œä½†ä½¿ç”¨è¾ƒä½çš„ä¹˜æ•°
            if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):
                # å¦‚æœlræ˜¯è°ƒåº¦å™¨ï¼Œä¸ºCriticåˆ›å»ºä¸€ä¸ªå¸¦ä¹˜æ•°çš„è°ƒåº¦å™¨
                critic_lr_multiplier = LEARNING_RATE_CONFIG.get("critic_lr_multiplier", 0.5)
                critic_lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
                    initial_learning_rate=LEARNING_RATE_CONFIG["initial_lr"] * critic_lr_multiplier,
                    decay_steps=lr.decay_steps,
                    end_learning_rate=LEARNING_RATE_CONFIG["end_lr"] * critic_lr_multiplier,
                    power=LEARNING_RATE_CONFIG["decay_power"]
                )
                self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr_schedule)
            else:
                # å¦‚æœlræ˜¯å›ºå®šå€¼ï¼Œåˆ™ä½¿ç”¨å›ºå®šå€¼ä¹˜ä»¥ä¹˜æ•°
                critic_lr_value = lr * LEARNING_RATE_CONFIG.get("critic_lr_multiplier", 0.5)
                self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr_value)
        else:
            self.actor_optimizer = None
            self.critic_optimizer = None
        
    def _build_networks(self):
        """ğŸ”§ MAPPOä¼˜åŒ–ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶å‚æ•°æ„å»ºç½‘ç»œ"""
        # å¯¼å…¥é…ç½®
        if self.network_config_override:
            config = self.network_config_override
        else:
            from environments.w_factory_config import PPO_NETWORK_CONFIG
            config = PPO_NETWORK_CONFIG

        hidden_sizes = config["hidden_sizes"]
        dropout_rate = config.get("dropout_rate", 0.1) # Use .get for safety
        
        # Actorç½‘ç»œ (å»ä¸­å¿ƒåŒ–) - ä½¿ç”¨å±€éƒ¨è§‚æµ‹
        state_input = tf.keras.layers.Input(shape=(self.state_dim,))
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ·»åŠ å±‚å½’ä¸€åŒ–ï¼Œç¨³å®šè®­ç»ƒ
        actor_x = tf.keras.layers.LayerNormalization()(state_input)
        
        actor_x = tf.keras.layers.Dense(
            hidden_sizes[0], 
            activation='relu',
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=np.sqrt(2)),
            bias_initializer=tf.keras.initializers.Constant(0.0)
        )(actor_x)
        actor_x = tf.keras.layers.Dropout(dropout_rate)(actor_x)
        actor_x = tf.keras.layers.Dense(
            hidden_sizes[1], 
            activation='relu',
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=np.sqrt(2)),
            bias_initializer=tf.keras.initializers.Constant(0.0)
        )(actor_x)
        actor_x = tf.keras.layers.Dropout(dropout_rate)(actor_x)
        actor_x = tf.keras.layers.Dense(
            hidden_sizes[2], 
            activation='relu',
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=np.sqrt(2)),
            bias_initializer=tf.keras.initializers.Constant(0.0)
        )(actor_x)
        # ğŸ”§ ç­–ç•¥è¾“å‡ºå±‚ä½¿ç”¨è¾ƒå°çš„åˆå§‹åŒ–å€¼
        action_probs = tf.keras.layers.Dense(
            self.action_dim, 
            activation='softmax',
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=0.01),
            bias_initializer=tf.keras.initializers.Constant(0.0)
        )(actor_x)
        actor = tf.keras.Model(inputs=state_input, outputs=action_probs)

        # Criticç½‘ç»œ (ä¸­å¿ƒåŒ–) - ä½¿ç”¨å…¨å±€çŠ¶æ€
        global_state_input = tf.keras.layers.Input(shape=(self.global_state_dim,))
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šCriticä¹ŸåŠ å±‚å½’ä¸€åŒ–
        critic_x = tf.keras.layers.LayerNormalization()(global_state_input)
        
        critic_x = tf.keras.layers.Dense(
            hidden_sizes[0],
            activation='relu',
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=np.sqrt(2)),
            bias_initializer=tf.keras.initializers.Constant(0.0)
        )(critic_x)
        critic_x = tf.keras.layers.Dropout(dropout_rate)(critic_x)
        critic_x = tf.keras.layers.Dense(
            hidden_sizes[1],
            activation='relu',
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=np.sqrt(2)),
            bias_initializer=tf.keras.initializers.Constant(0.0)
        )(critic_x)
        critic_x = tf.keras.layers.Dropout(dropout_rate)(critic_x)
        critic_x = tf.keras.layers.Dense(
            hidden_sizes[2],
            activation='relu',
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=np.sqrt(2)),
            bias_initializer=tf.keras.initializers.Constant(0.0)
        )(critic_x)
        # Valueè¾“å‡ºå±‚
        value_output = tf.keras.layers.Dense(
            1,
            activation=None,
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=1.0),
            bias_initializer=tf.keras.initializers.Constant(0.0)
        )(critic_x)
        critic = tf.keras.Model(inputs=global_state_input, outputs=value_output)
        
        return actor, critic
    
    def get_action_and_value(self, state: np.ndarray, global_state: np.ndarray) -> Tuple[int, np.float32, np.float32]:
        """è·å–åŠ¨ä½œã€ä»·å€¼å’ŒåŠ¨ä½œæ¦‚ç‡"""
        state_tensor = tf.expand_dims(tf.convert_to_tensor(state), 0)
        probs = self.actor(state_tensor)
        # ğŸ”§ ä¿®å¤ï¼šæ•°å€¼ç¨³å®šæ€§
        probs = tf.clip_by_value(probs, 1e-8, 1.0)
        action = tf.random.categorical(tf.math.log(probs + 1e-8), 1)[0, 0].numpy()
        action_prob = probs[0, action].numpy()

        # ğŸ”§ Criticä½¿ç”¨å…¨å±€çŠ¶æ€
        value = self.critic(tf.expand_dims(tf.convert_to_tensor(global_state), 0))[0, 0].numpy()
        
        return action, np.float32(value), np.float32(action_prob)
    
    def get_value(self, global_state: np.ndarray) -> float:
        """è·å–çŠ¶æ€ä»·å€¼ï¼ˆä»…ä½¿ç”¨å…¨å±€çŠ¶æ€ï¼‰"""
        global_state = tf.expand_dims(global_state, 0)
        return float(self.critic(global_state)[0, 0])
    
    def update(self, states: np.ndarray, global_states: np.ndarray, actions: np.ndarray, 
               old_probs: np.ndarray, advantages: np.ndarray, 
               returns: np.ndarray, clip_ratio: float = None, entropy_coeff: float = None) -> Dict[str, float]:
        """ğŸ”§ MAPPOæ›´æ–°ï¼šCriticä½¿ç”¨å…¨å±€çŠ¶æ€"""
        # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥ä¼˜åŒ–å™¨æ˜¯å¦å­˜åœ¨
        if self.actor_optimizer is None or self.critic_optimizer is None:
            raise ValueError("Optimizers not initialized. Cannot update network.")
            
        # ğŸ”§ V32 ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„PPOå‚æ•°
        if clip_ratio is None:
            clip_ratio = PPO_NETWORK_CONFIG["clip_ratio"]
        # å…³é”®ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„åŠ¨æ€ç†µç³»æ•°
        current_entropy_coeff = entropy_coeff if entropy_coeff is not None else PPO_NETWORK_CONFIG["entropy_coeff"]
        
        # Actoræ›´æ–°
        with tf.GradientTape() as tape:
            probs = self.actor(states, training=True)
            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤
            probs = tf.clip_by_value(probs, 1e-8, 1.0)
            # è®¡ç®—é€‰æ‹©åŠ¨ä½œçš„æ¦‚ç‡ new_probs
            batch_indices = tf.range(tf.shape(actions)[0], dtype=tf.int32)
            indices = tf.stack([batch_indices, tf.cast(actions, tf.int32)], axis=1)
            new_probs = tf.gather_nd(probs, indices)
            # ğŸ”§ ä¿®å¤ï¼šé˜²æ­¢é™¤é›¶å’Œæ•°å€¼çˆ†ç‚¸
            ratio = new_probs / (old_probs + 1e-8)
            
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—KLæ•£åº¦ï¼ˆåŸºäºè¢«é€‰åŠ¨ä½œçš„è¿‘ä¼¼ï¼‰
            old_log_probs = tf.math.log(old_probs + 1e-8)
            new_log_probs = tf.math.log(new_probs + 1e-8)
            approx_kl = tf.reduce_mean(old_log_probs - new_log_probs)
            
            # è®¡ç®—è£å‰ªæ¯”ä¾‹ (ç”¨äºç›‘æ§)
            clipped_mask = tf.greater(tf.abs(ratio - 1.0), clip_ratio)
            clip_fraction = tf.reduce_mean(tf.cast(clipped_mask, tf.float32))

            clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)
            actor_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))
            
            # è®¡ç®—åˆ†ç±»ç†µï¼š-sum p*log p
            entropy_per_sample = -tf.reduce_sum(probs * tf.math.log(probs + 1e-8), axis=1)
            entropy = tf.reduce_mean(entropy_per_sample)
            actor_loss -= current_entropy_coeff * entropy
            
        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)
        # ğŸ”§ æ–°å¢ï¼šæ¢¯åº¦è£å‰ªä»¥æé«˜è®­ç»ƒç¨³å®šæ€§
        grad_clip_norm = PPO_NETWORK_CONFIG.get("grad_clip_norm", 1.0)
        actor_grads, _ = tf.clip_by_global_norm(actor_grads, grad_clip_norm)
        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))
        
        # Criticæ›´æ–° (ä½¿ç”¨å…¨å±€çŠ¶æ€)
        with tf.GradientTape() as tape:
            values = self.critic(global_states, training=True)
            returns_tf = tf.expand_dims(tf.convert_to_tensor(returns, dtype=tf.float32), 1)
            critic_loss = tf.reduce_mean(tf.square(returns_tf - values))
        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)
        # ğŸ”§ æ–°å¢ï¼šæ¢¯åº¦è£å‰ªï¼ˆä½¿ç”¨é…ç½®å€¼ï¼‰
        critic_grads, _ = tf.clip_by_global_norm(critic_grads, grad_clip_norm)
        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))
        
        return {
            "actor_loss": actor_loss.numpy(),
            "critic_loss": critic_loss.numpy(),
            "entropy": entropy.numpy(),
            "approx_kl": approx_kl.numpy(),
            "clip_fraction": clip_fraction.numpy()
        }

# ğŸ”§ V8 æ–°å¢: å¤šè¿›ç¨‹å¹¶è¡Œå·¥ä½œå‡½æ•°
def run_simulation_worker(network_weights: Dict[str, List[np.ndarray]],
                          state_dim: int, action_dim: int, num_steps: int, seed: int, 
                          global_state_dim: int, network_config: Dict[str, Any], curriculum_config: Dict[str, Any] = None) -> Tuple[Dict[str, ExperienceBuffer], float, Optional[np.ndarray], bool, bool]:
    """å¹¶è¡Œä»¿çœŸå·¥ä½œè¿›ç¨‹ - ğŸ”§ MAPPOæ”¹é€ ï¼šæ”¶é›†å…¨å±€çŠ¶æ€"""
    
    # ğŸ”§ ç»ˆæä¿®å¤ï¼šå°†tfå¯¼å…¥ç§»è‡³é¡¶éƒ¨ï¼Œè§£å†³UnboundLocalError
    import tensorflow as tf
    import numpy as np
    import random
    
    # 1. åˆå§‹åŒ–
    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # ç¦ç”¨GPU
    
    tf.random.set_seed(seed)
    env = make_parallel_env(curriculum_config)
    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨åŠ¨æ€å­¦ä¹ ç‡è€Œéå›ºå®šå€¼
    # æ³¨æ„ï¼šworkerä¸éœ€è¦å­¦ä¹ ç‡ï¼Œåªåšæ¨ç†
    network = PPONetwork(state_dim, action_dim, None, global_state_dim, network_config=network_config) # Workerä¸éœ€è¦ä¼˜åŒ–å™¨
    network.actor.set_weights(network_weights['actor'])
    network.critic.set_weights(network_weights['critic']) # ğŸ”§ Criticæƒé‡ä¹Ÿéœ€è¦åŒæ­¥
    
    buffers = {agent: ExperienceBuffer() for agent in env.agents}
    
    observations, infos = env.reset(seed=seed)
    global_state = infos[env.agents[0]]['global_state']
    # ğŸ”§ æ™ºèƒ½ä½“æ¡ä»¶åŒ–ï¼šæ„å»ºone-hotæ˜ å°„
    agent_list = list(env.agents)
    agent_index = {agent_id: idx for idx, agent_id in enumerate(agent_list)}
    # ä¸“å®¶ä¿®å¤ï¼šä¿®æ­£base_global_dimçš„è®¡ç®—æ–¹å¼ï¼Œé¿å…ç¡¬ç¼–ç æˆ–ä¾èµ–ä¸ä¸€è‡´çš„ç¯å¢ƒå®ä¾‹
    base_global_dim = global_state_dim - len(agent_list)
    
    total_reward_collected = 0.0
    collected_steps = 0
    step_count = 0
    
    while collected_steps < num_steps:
        actions = {}
        values = {}
        action_probs = {}
        augmented_global_states = {} # ä¿®å¤ç¼ºé™·ï¼šä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆ†åˆ«å­˜å‚¨å¢å¼ºå…¨å±€çŠ¶æ€
        
        # ğŸ”§ ä¿®å¤ï¼šåŸºç¡€å…¨å±€çŠ¶æ€ï¼ˆä¸å«one-hotï¼‰
        if global_state is not None:
            base_global_state = global_state.copy()
        else:
            base_global_state = np.zeros(base_global_dim, dtype=np.float32)

        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ™ºèƒ½ä½“åŠ¨ä½œçš„åŒæ­¥æ€§
        for agent in env.agents:  # ä½¿ç”¨env.agentsç¡®ä¿é¡ºåºä¸€è‡´
            if agent in observations:
                obs = observations[agent]
                # ğŸ”§ æ‹¼æ¥agent one-hotåˆ°å…¨å±€çŠ¶æ€
                one_hot = np.zeros(len(agent_list), dtype=np.float32)
                one_hot[agent_index[agent]] = 1.0
                # æ³¨æ„ï¼šglobal_state_dim å·²ç»åŒ…å«one-hoté•¿åº¦
                augmented_global_state = np.concatenate([base_global_state, one_hot]).astype(np.float32)
                augmented_global_states[agent] = augmented_global_state # ä¿®å¤ç¼ºé™·ï¼šå­˜å‚¨
                action, value, action_prob = network.get_action_and_value(obs, augmented_global_state)
                actions[agent] = action
                values[agent] = value
                action_probs[agent] = action_prob
            
        next_observations, rewards, terminations, truncations, infos = env.step(actions)
        step_count += 1
        collected_steps += 1
        global_state = infos[env.agents[0]]['global_state']
        
        
        total_reward_collected += sum(rewards.values())

        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰æ™ºèƒ½ä½“çš„æ•°æ®ä¸€è‡´æ€§
        for agent in env.agents:
            if agent in observations and agent in actions:
                terminated = terminations.get(agent, False)
                truncated = truncations.get(agent, False)
                reward = rewards.get(agent, 0)
                # ğŸ”§ é‡è¦ï¼šå­˜å‚¨æ—¶ä½¿ç”¨agentæ¡ä»¶åŒ–çš„å…¨å±€çŠ¶æ€
                agent_specific_global_state = augmented_global_states.get(agent)
                if agent_specific_global_state is not None:
                    buffers[agent].store(
                        observations[agent], 
                        agent_specific_global_state.copy(),  # ä¿®å¤ç¼ºé™·ï¼šä½¿ç”¨æ­£ç¡®çš„å¢å¼ºå…¨å±€çŠ¶æ€
                        actions[agent], 
                        reward,
                        values[agent], 
                        action_probs[agent], 
                        terminated,
                        truncated
                    )

        observations = next_observations

        # ğŸ”§ ä¿®å¤ï¼šä¸è¯„ä¼°ä¸€è‡´çš„ç»ˆæ­¢æ¡ä»¶
        if any(terminations.values()) or any(truncations.values()):
            
            # ğŸ”§ MAPPOå…³é”®ä¿®å¤ï¼šæ­£ç¡®å¤„ç†æˆªæ–­æ—¶çš„bootstrapä»·å€¼
            # æ³¨æ„ï¼šè¿™é‡Œæš‚æ—¶ä¸å¤„ç†ï¼Œè®©bufferè‡ªå·±åœ¨get_batchæ—¶å¤„ç†
            pass
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šepisodeç»“æŸæ—¶åº”è¯¥breakï¼Œè€Œä¸æ˜¯resetç»§ç»­æ”¶é›†
            # ä¸€ä¸ªworkerè°ƒç”¨åº”è¯¥åªæ”¶é›†å•ä¸ªtrajectoryçš„æ•°æ®
            break

    # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šè¿”å›æœ€åä¸€ä¸ªå…¨å±€çŠ¶æ€å’Œæˆªæ–­æ ‡å¿—ï¼Œç”¨äºä»·å€¼å¼•å¯¼
    # åªè¦trajectoryæœªçœŸæ­£ç»ˆæ­¢ï¼ˆå³å­˜åœ¨æˆªæ–­æˆ–ä»…å› é‡‡æ ·æ­¥æ•°è¾¾åˆ°ä¸Šé™è€Œé€€å‡ºï¼‰ï¼Œå°±æä¾›bootstrapä»·å€¼
    was_truncated = any(truncations.values()) or not any(terminations.values())
    # è¿”å›åŸºç¡€å…¨å±€çŠ¶æ€ï¼ˆä¸å«one-hotï¼‰ï¼Œä¸»è¿›ç¨‹å°†ä¸ºå„agentæ·»åŠ one-hotåè®¡ç®—bootstrap
    next_global_state_for_bootstrap = global_state if was_truncated else None
    
    # ç»Ÿè®¡æœ¬workeræ˜¯å¦å®Œæˆäº†å…¨éƒ¨é›¶ä»¶ï¼ˆç”¨äºæ—¥å¿—ä¸ç»ˆå±€å¥–åŠ±æ ¸éªŒï¼‰
    try:
        total_required_worker = sum(o.quantity for o in env.sim.orders)
        completed_all_worker = (len(env.sim.completed_parts) >= total_required_worker)
    except Exception:
        completed_all_worker = False
    
    env.close()
    return buffers, total_reward_collected, next_global_state_for_bootstrap, was_truncated, completed_all_worker

class SimplePPOTrainer:
    """ğŸ”§ V31 è‡ªé€‚åº”PPOè®­ç»ƒå™¨ï¼šæ ¹æ®è®­ç»ƒçŠ¶æ€è‡ªåŠ¨è°ƒæ•´è®­ç»ƒç­–ç•¥"""
    
    # ğŸ”§ V31 æ–°å¢ï¼šæ”¯æŒè‡ªé€‚åº”è®­ç»ƒç›®æ ‡å’ŒåŠ¨æ€è½®æ•°è°ƒæ•´
    def __init__(self, initial_lr: float, total_train_episodes: int, steps_per_episode: int, training_targets: dict = None):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        
        # ğŸ”§ V32 ä½¿ç”¨é…ç½®æ–‡ä»¶çš„ç³»ç»Ÿèµ„æºé…ç½®
        self.num_workers = SYSTEM_CONFIG["num_parallel_workers"]
        print(f"ğŸ”§ ä½¿ç”¨ {self.num_workers} ä¸ªå¹¶è¡Œç¯å¢ƒè¿›è¡Œæ•°æ®é‡‡é›†")
        
        # ğŸ”§ V32 ä½¿ç”¨é…ç½®æ–‡ä»¶çš„TensorFlowçº¿ç¨‹é…ç½®
        tf.config.threading.set_inter_op_parallelism_threads(SYSTEM_CONFIG["tf_inter_op_threads"])
        tf.config.threading.set_intra_op_parallelism_threads(SYSTEM_CONFIG["tf_intra_op_threads"])
        print(f"ğŸ”§ TensorFlowå°†ä½¿ç”¨ {SYSTEM_CONFIG['tf_inter_op_threads']}ä¸ªinterçº¿ç¨‹, {SYSTEM_CONFIG['tf_intra_op_threads']}ä¸ªintraçº¿ç¨‹")
        
        # ç¯å¢ƒæ¢æµ‹
        # ä¹‹å‰çš„ä»£ç ä¾èµ–åŠ¨æ€é…ç½®ï¼Œç°åœ¨æˆ‘ä»¬ç›´æ¥åˆ›å»º
        temp_env = make_parallel_env()
        self.state_dim = temp_env.observation_space(temp_env.possible_agents[0]).shape[0]
        self.action_dim = temp_env.action_space(temp_env.possible_agents[0]).n
        self.agent_ids = temp_env.possible_agents
        self.num_agents = len(self.agent_ids)
        # ğŸ”§ Criticæ™ºèƒ½ä½“æ¡ä»¶åŒ–ï¼šå°†æ™ºèƒ½ä½“one-hotå¹¶å…¥å…¨å±€çŠ¶æ€è¾“å…¥ç»´åº¦
        self.global_state_dim = temp_env.global_state_space.shape[0] + self.num_agents
        temp_env.close()
        
        print("ğŸ”§ ç¯å¢ƒç©ºé—´æ£€æµ‹:")
        print(f"   è§‚æµ‹ç»´åº¦: {self.state_dim}")
        print(f"   åŠ¨ä½œç»´åº¦: {self.action_dim}")
        print(f"   æ™ºèƒ½ä½“æ•°é‡: {len(self.agent_ids)}")
        print(f"   å…¨å±€çŠ¶æ€ç»´åº¦(å«agent one-hot): {self.global_state_dim}")
        
        # ğŸ”§ V26 ç»ˆæä¿®å¤ï¼šç§»é™¤åŠ¨æ€å‚æ•°è°ƒæ•´
        optimized_episodes = total_train_episodes
        optimized_steps = steps_per_episode
        # ç»Ÿä¸€è¯„ä¼°/é‡‡é›†æœ€å¤§æ­¥æ•°
        self.max_steps_for_eval = int(optimized_steps)
        
        # ğŸ”§ V32 ä½¿ç”¨é…ç½®æ–‡ä»¶çš„å­¦ä¹ ç‡è°ƒåº¦é…ç½®
        self.lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
            initial_learning_rate=LEARNING_RATE_CONFIG["initial_lr"],
            decay_steps=optimized_episodes * optimized_steps,
            end_learning_rate=LEARNING_RATE_CONFIG["end_lr"],
            power=LEARNING_RATE_CONFIG["decay_power"]
        )

        # å…±äº«ç½‘ç»œ
        self.shared_network = PPONetwork(
            state_dim=self.state_dim,
            action_dim=self.action_dim,
            lr=self.lr_schedule,
            global_state_dim=self.global_state_dim
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.training_losses = []
        self.iteration_times = []  # ğŸ”§ V5 æ–°å¢ï¼šè®°å½•æ¯è½®è®­ç»ƒæ—¶é—´
        self.kpi_history = []      # ğŸ”§ V5 æ–°å¢ï¼šè®°å½•æ¯è½®KPIå†å²
        self.initial_lr = initial_lr  # ğŸ”§ V19 ä¿®å¤: ä¿å­˜åˆå§‹å­¦ä¹ ç‡
        self.start_time = time.time()
        self.start_time_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ğŸ”§ æ ¸å¿ƒæ”¹é€ ï¼šæ–°å¢"æœ€ç»ˆé˜¶æ®µ"æœ€ä½³KPIè·Ÿè¸ªå™¨
        self.final_stage_best_kpi = {
            'mean_completed_parts': -1.0,
            'mean_makespan': float('inf'),
            'mean_utilization': 0.0,
            'mean_tardiness': float('inf')
        }
        self.final_stage_best_score = float('-inf')
        self.final_stage_best_episode = -1 # ğŸ”§ æ–°å¢ï¼šè®°å½•æœ€ä½³KPIçš„å›åˆæ•°
        
        # ğŸ”§ æ ¸å¿ƒæ”¹é€ ï¼šæ–°å¢"åŒè¾¾æ ‡"æœ€ä½³KPIè·Ÿè¸ªå™¨
        self.best_kpi_dual_objective = {
            'mean_completed_parts': -1.0,
            'mean_makespan': float('inf'),
            'mean_utilization': 0.0,
            'mean_tardiness': float('inf')
        }
        self.best_score_dual_objective = float('-inf')
        self.best_episode_dual_objective = -1

        # ğŸ”§ æ ¸å¿ƒé‡æ„ï¼šè®­ç»ƒæµç¨‹ç”±é…ç½®æ–‡ä»¶é©±åŠ¨
        self.training_flow_config = TRAINING_FLOW_CONFIG
        self.training_targets = self.training_flow_config["general_params"] # é€šç”¨å‚æ•°
        
        # ğŸ”§ V31 æ–°å¢ï¼šè‡ªé€‚åº”è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.adaptive_state = {
            "target_achieved_count": 0,          # è¿ç»­è¾¾åˆ°ç›®æ ‡çš„æ¬¡æ•°
            "best_performance": 0.0,             # å†å²æœ€ä½³æ€§èƒ½
            "last_improvement_episode": 0,       # ä¸Šæ¬¡æ”¹è¿›çš„è½®æ•°
            "performance_history": [],           # æ€§èƒ½å†å²è®°å½•
            "training_phase": "exploration",     # å½“å‰è®­ç»ƒé˜¶æ®µï¼šexploration, exploitation, fine_tuning
            "stagnation_counter": 0,             # åœæ»è®¡æ•°å™¨
            "last_stagnation_performance": -1.0, # ä¸Šä¸€æ¬¡åœæ»æ—¶çš„æ€§èƒ½
        }
        # --- æ–¹æ¡ˆäºŒï¼šå‡çº§è‡ªé€‚åº”ç†µæ‰€éœ€å˜é‡ ---
        self.epochs_without_improvement = 0
        self.stagnation_level = 0  # æ–°å¢ï¼šåœæ»ç­‰çº§ï¼Œç”¨äºé˜¶æ¢¯å¼æå‡ç†µ
        
        # --- æ–°å¢ï¼šåŸºç¡€è®­ç»ƒ + éšæœºé¢†åŸŸå¼ºåŒ– é˜¶æ®µç®¡ç† ---
        self.foundation_training_completed = False  # åŸºç¡€è®­ç»ƒæ˜¯å¦å®Œæˆ
        self.generalization_phase_active = False   # æ˜¯å¦è¿›å…¥æ³›åŒ–å¼ºåŒ–é˜¶æ®µ
        self.foundation_achievement_count = 0      # åŸºç¡€è®­ç»ƒè¿ç»­è¾¾æ ‡æ¬¡æ•°
        self.generalization_achievement_count = 0  # æ³›åŒ–é˜¶æ®µè¿ç»­è¾¾æ ‡æ¬¡æ•°
        
        # --- æ–°å¢ï¼šä¸ºæ–°ä¸¤é˜¶æ®µæ–¹æ¡ˆçš„ç‹¬ç«‹æ¨¡å‹ä¿å­˜è¿½è¸ª ---
        self.best_score_foundation_phase = float('-inf')    # åŸºç¡€è®­ç»ƒé˜¶æ®µæœ€ä½³åˆ†æ•°
        self.best_kpi_foundation_phase = {}         # åŸºç¡€è®­ç»ƒé˜¶æ®µæœ€ä½³KPI
        self.best_episode_foundation_phase = -1    # åŸºç¡€è®­ç»ƒé˜¶æ®µæœ€ä½³å›åˆ
        
        self.best_score_generalization_phase = float('-inf')  # æ³›åŒ–å¼ºåŒ–é˜¶æ®µæœ€ä½³åˆ†æ•°
        self.best_kpi_generalization_phase = {}       # æ³›åŒ–å¼ºåŒ–é˜¶æ®µæœ€ä½³KPI
        self.best_episode_generalization_phase = -1  # æ³›åŒ–å¼ºåŒ–é˜¶æ®µæœ€ä½³å›åˆ
        
        # --- æ–°å¢ï¼šè¯¾ç¨‹å­¦ä¹ é˜¶æ®µçš„è‡ªé€‚åº”æ¯•ä¸šè·Ÿè¸ªå™¨ ---
        self.curriculum_stage_achievement_count = 0
        
        # ğŸ”§ V34 åˆå§‹åŒ–åŠ¨æ€è®­ç»ƒå‚æ•°
        self.current_entropy_coeff = PPO_NETWORK_CONFIG["entropy_coeff"] # åˆå§‹åŒ–åŠ¨æ€ç†µç³»æ•°
        self.current_learning_rate = LEARNING_RATE_CONFIG["initial_lr"] # ğŸ”§ V34 ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å­¦ä¹ ç‡é…ç½®
        
        # ğŸ”§ æ–°å¢ï¼šç†µç³»æ•°é€€ç«è®¡åˆ’ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        self.entropy_decay_rate = 0.9995  # ğŸ”§ æ›´æ…¢çš„è¡°å‡ç‡ï¼Œä¿æŒæ›´é•¿æ—¶é—´çš„æ¢ç´¢
        self.min_entropy_coeff = 0.05     # ğŸ”§ æ›´é«˜çš„æœ€å°ç†µç³»æ•°ï¼Œé¿å…è¿‡æ—©æ”¶æ•›
        
        
        # ğŸ”§ V40 æ–°å¢ï¼šå›åˆäº‹ä»¶æ—¥å¿—è®°å½•å™¨
        self.episode_events = []
        
        # åˆ›å»ºä¿å­˜ç›®å½• (V31æ–°å¢ï¼šä»¥è®­ç»ƒå¼€å§‹æ—¶é—´åˆ›å»ºä¸“ç”¨æ–‡ä»¶å¤¹)
        self.base_models_dir = "mappo/ppo_models"
        self.models_dir = f"{self.base_models_dir}/{self.start_time_str}"
        os.makedirs(self.models_dir, exist_ok=True)
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜ç›®å½•: {self.models_dir}")
        
        # ğŸ”§ V12 æ–°å¢ï¼šTensorBoardæ”¯æŒ
        self.tensorboard_dir = f"mappo/tensorboard_logs/{self.timestamp}"
        os.makedirs(self.tensorboard_dir, exist_ok=True)
        if TENSORBOARD_AVAILABLE:
            self.train_writer = None
            self.current_tensorboard_run_name = None
            # ä¸ºæœ¬æ¬¡è¿è¡Œåˆ†é…å”¯ä¸€ç«¯å£
            try:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.bind(("127.0.0.1", 0))
                self.tensorboard_port = sock.getsockname()[1]
                sock.close()
            except Exception:
                # å›é€€åˆ°å¸¸è§ç«¯å£èŒƒå›´å†…çš„ä¼ªéšæœºç«¯å£
                self.tensorboard_port = 6006 + (hash(self.timestamp) % 1000)
            print(f"ğŸ“Š TensorBoardå‘½ä»¤: tensorboard --logdir=\"{self.tensorboard_dir}\" --port={self.tensorboard_port}")
        else:
            self.train_writer = None
            print("âš ï¸  TensorBoardä¸å¯ç”¨")
    
    def should_continue_training(self, episode: int, current_score: float, completion_rate: float) -> tuple:
        """ğŸ”§ ä¿®å¤ï¼šåŸºäºTRAINING_FLOW_CONFIGçš„é˜¶æ®µæ ‡å‡†è¯„ä¼°æ˜¯å¦ç»§ç»­è®­ç»ƒ"""
        general = self.training_flow_config["general_params"]
        state = self.adaptive_state

        # åŸºæœ¬é™åˆ¶æ£€æŸ¥
        if episode >= general["max_episodes"]:
            return False, f"å·²è¾¾åˆ°æœ€å¤§è®­ç»ƒè½®æ•°({general['max_episodes']})", 0

        # æŒ‰é˜¶æ®µé€‰æ‹©æ ‡å‡†
        if self.generalization_phase_active:
            criteria = self.training_flow_config["generalization_phase"]["completion_criteria"]
        else:
            criteria = self.training_flow_config["foundation_phase"]["graduation_criteria"]

        target_score = criteria["target_score"]
        min_completion_rate = criteria.get("min_completion_rate", 100.0)
        target_consistency = criteria["target_consistency"]

        # è¾¾æ ‡è®¡æ•°é€»è¾‘
        if completion_rate >= min_completion_rate and current_score >= target_score:
            state["target_achieved_count"] += 1
            print(f"ğŸ¯ è¾¾æ ‡: å®Œæˆç‡ {completion_rate:.1f}% & åˆ†æ•° {current_score:.3f} (è¿ç»­ç¬¬{state['target_achieved_count']}/{target_consistency}æ¬¡)")
            if state["target_achieved_count"] >= target_consistency:
                return False, f"è¿ç»­{target_consistency}æ¬¡è¾¾åˆ°é˜¶æ®µæ ‡å‡†", 0
        else:
            state["target_achieved_count"] = 0

        # æ—©åœé€»è¾‘ï¼ˆåŸºäºåˆ†æ•°åœæ»ï¼‰
        state["performance_history"].append(current_score)
        if len(state["performance_history"]) > general["performance_window"]:
            state["performance_history"].pop(0)

        if current_score > state["best_performance"]:
            state["best_performance"] = current_score
            state["last_improvement_episode"] = episode

        improvement_gap = episode - state["last_improvement_episode"]
        if improvement_gap >= general["early_stop_patience"]:
            if len(state["performance_history"]) >= general["performance_window"]:
                recent_avg_score = sum(state["performance_history"]) / len(state["performance_history"])
                if recent_avg_score < target_score * 0.8:
                    return False, f"è¿ç»­{improvement_gap}è½®æ— æ”¹è¿›ï¼Œä¸”å¹³å‡åˆ†æ•°ä½äº{target_score*0.8:.3f}", 0

        return True, f"å½“å‰åˆ†æ•° {current_score:.3f}, å®Œæˆç‡ {completion_rate:.1f}%", 0
    
    def check_foundation_training_completion(self, kpi_results: Dict[str, float], current_score: float) -> bool:
        """æ£€æŸ¥åŸºç¡€è®­ç»ƒæ˜¯å¦è¾¾åˆ°æ¯•ä¸šæ ‡å‡†ï¼Œç”±é…ç½®æ–‡ä»¶é©±åŠ¨"""
        criteria = self.training_flow_config["foundation_phase"]["graduation_criteria"]
        
        total_parts_target = get_total_parts_count()
        completion_rate_kpi = (kpi_results.get('mean_completed_parts', 0) / total_parts_target) * 100 if total_parts_target > 0 else 0
        
        target_score = criteria["target_score"]
        stability_goal = criteria["target_consistency"]
        tardiness_threshold = criteria["tardiness_threshold"]
        min_completion_rate = criteria["min_completion_rate"]
        current_tardiness = kpi_results.get('mean_tardiness', float('inf'))

        conditions_met = {
            f"å®Œæˆç‡è¾¾æ ‡(>={min_completion_rate}%)": completion_rate_kpi >= min_completion_rate,
            f"åˆ†æ•°è¾¾æ ‡(>={target_score})": current_score >= target_score,
            f"å»¶æœŸè¾¾æ ‡(<={tardiness_threshold}min)": current_tardiness <= tardiness_threshold
        }

        if all(conditions_met.values()):
            self.foundation_achievement_count += 1
            print(f"ğŸ¯ åŸºç¡€è®­ç»ƒè¾¾æ ‡: å®Œæˆç‡ {completion_rate_kpi:.1f}%, åˆ†æ•° {current_score:.3f}, å»¶æœŸ {current_tardiness:.1f}min (è¿ç»­ç¬¬{self.foundation_achievement_count}/{stability_goal}æ¬¡)")
        else:
            if self.foundation_achievement_count > 0:
                reasons = [k for k, v in conditions_met.items() if not v]
                print(f"âŒ åŸºç¡€è®­ç»ƒè¿ç»­è¾¾æ ‡ä¸­æ–­. æœªè¾¾æ ‡é¡¹: {', '.join(reasons)}")
            self.foundation_achievement_count = 0

        if self.foundation_achievement_count >= stability_goal:
            print(f"ğŸ† åŸºç¡€è®­ç»ƒå®Œæˆï¼è¿ç»­{stability_goal}æ¬¡è¾¾åˆ°æ‰€æœ‰æ ‡å‡†ï¼Œå‡†å¤‡è¿›å…¥æ³›åŒ–å¼ºåŒ–é˜¶æ®µã€‚")
            return True
        return False
    
    def check_generalization_training_completion(self, current_score: float, completion_rate: float) -> bool:
        """æ£€æŸ¥æ³›åŒ–è®­ç»ƒæ˜¯å¦å·²è¾¾åˆ°æœ€ç»ˆè®­ç»ƒå®Œæˆçš„æ¡ä»¶ï¼Œç”±é…ç½®æ–‡ä»¶é©±åŠ¨"""
        criteria = self.training_flow_config["generalization_phase"]["completion_criteria"]
        
        target_score = criteria["target_score"]
        stability_goal = criteria["target_consistency"]
        min_completion_rate = criteria["min_completion_rate"]
        
        if completion_rate >= min_completion_rate and current_score >= target_score:
            self.generalization_achievement_count += 1
            print(f"ğŸŒŸ æ³›åŒ–é˜¶æ®µè¾¾æ ‡: å®Œæˆç‡ {completion_rate:.1f}% & åˆ†æ•° {current_score:.3f} (è¿ç»­ç¬¬{self.generalization_achievement_count}/{stability_goal}æ¬¡)")
            
            if self.generalization_achievement_count >= stability_goal:
                print(f"ğŸ‰ æ³›åŒ–è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²å…·å¤‡ä¼˜ç§€çš„æ³›åŒ–èƒ½åŠ›ã€‚")
                return True
        else:
            self.generalization_achievement_count = 0
        
        return False
    
    def create_environment(self, curriculum_stage=None):
        """åˆ›å»ºç¯å¢ƒï¼ˆæ”¯æŒè¯¾ç¨‹å­¦ä¹ ï¼‰"""
        config = {}
        
        # ğŸ”§ V16ï¼šå®ç°è¯¾ç¨‹å­¦ä¹ çš„ç¯å¢ƒé…ç½®
        # æ ¸å¿ƒé‡æ„ï¼šè¯¾ç¨‹å­¦ä¹ é€»è¾‘ç°åœ¨ç”± TRAINING_FLOW_CONFIG æ§åˆ¶
        cl_config = self.training_flow_config["foundation_phase"]["curriculum_learning"]
        if curriculum_stage is not None and cl_config["enabled"]:
            stages = cl_config["stages"]
            stage = stages[curriculum_stage] if curriculum_stage < len(stages) else stages[-1]
            config['curriculum_stage'] = stage
            config['orders_scale'] = stage.get('orders_scale', 1.0)
            config['time_scale'] = stage.get('time_scale', 1.0)
            print(f"ğŸ“š è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ {curriculum_stage+1}: {stage['name']} (è®¢å•æ¯”ä¾‹: {stage['orders_scale']}, æ—¶é—´å€æ•°: {stage['time_scale']})")
        
        # ç»Ÿä¸€æ³¨å…¥ MAX_SIM_STEPS
        config['MAX_SIM_STEPS'] = self.max_steps_for_eval
        env = make_parallel_env(config)
        buffers = {
            agent: ExperienceBuffer() 
            for agent in env.possible_agents
        }
        return env, buffers
    
    def collect_and_process_experience(self, num_steps: int, curriculum_config: Dict[str, Any] = None) -> Tuple[float, Optional[Dict[str, np.ndarray]]]:
        """
        ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šå¹¶è¡Œæ”¶é›†ç»éªŒï¼Œå¹¶åœ¨ä¸»è¿›ç¨‹ä¸­ç»Ÿä¸€å¤„ç†ä»·å€¼å¼•å¯¼å’ŒGAEè®¡ç®—
        - è¿”å›ä¸€ä¸ªå¤„ç†å®Œæˆã€å¯ä»¥ç›´æ¥ç”¨äºæ›´æ–°çš„è®­ç»ƒæ‰¹æ¬¡
        """
        from environments.w_factory_config import PPO_NETWORK_CONFIG

        network_weights = {
            'actor': self.shared_network.actor.get_weights(),
            'critic': self.shared_network.critic.get_weights()
        }
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨å…¥å‚ä½œä¸ºæ¯ä¸ª worker çš„æœ€å¤§æ­¥æ•°
        steps_per_worker = int(num_steps)
        
        total_reward = 0
        
        # åˆå§‹åŒ–ç”¨äºèšåˆæ‰€æœ‰workeræ•°æ®çš„åˆ—è¡¨
        all_states, all_global_states, all_actions, all_old_probs, all_advantages, all_returns = [], [], [], [], [], []

        # ğŸ”§ ä½¿ç”¨ spawn ä¸Šä¸‹æ–‡ï¼Œé¿å… TF åœ¨ fork ä¸‹çš„ä¸å®‰å…¨
        with ProcessPoolExecutor(max_workers=self.num_workers, mp_context=multiprocessing.get_context("spawn")) as executor:
            futures = []
            for i in range(self.num_workers):
                seed = random.randint(0, 1_000_000)
                # ä¸º worker ä¼ å…¥ç»Ÿä¸€çš„ MAX_SIM_STEPS
                worker_config = (curriculum_config.copy() if curriculum_config else {})
                worker_config['MAX_SIM_STEPS'] = steps_per_worker
                future = executor.submit(
                    run_simulation_worker,
                    network_weights,
                    self.state_dim,
                    self.action_dim,
                    steps_per_worker,
                    seed,
                    self.global_state_dim,
                    PPO_NETWORK_CONFIG.copy(),
                    worker_config
                )
                futures.append(future)

            completed_workers = 0
            finished_workers = 0
            for future in as_completed(futures):
                try:
                    # æ¥æ”¶workerè¿”å›çš„åŸå§‹ç»éªŒã€ä¸‹ä¸€ä¸ªå…¨å±€çŠ¶æ€å’Œæˆªæ–­æ ‡å¿—
                    worker_buffers, worker_reward, next_global_state, was_truncated, worker_completed_all = future.result()
                    total_reward += worker_reward
                    completed_workers += 1 if worker_completed_all else 0
                    finished_workers += 1
                    
                    # åœ¨ä¸»è¿›ç¨‹ä¸­ä¸ºè¯¥workerçš„æ¯ä¸ªæ™ºèƒ½ä½“è®¡ç®—GAEå’Œå›æŠ¥
                    for agent_id in self.agent_ids:
                        if agent_id in worker_buffers:
                            buffer = worker_buffers[agent_id]
                            if not buffer.states:  # è·³è¿‡ç©ºç¼“å†²åŒº
                                continue
                            
                            # ğŸ”§ ä½¿ç”¨æ­£ç¡®çš„å¼•å¯¼ä»·å€¼ï¼ˆé€æ™ºèƒ½ä½“ one-hot æ¡ä»¶åŒ–ï¼‰
                            if was_truncated and next_global_state is not None:
                                # ä¸“å®¶ä¿®å¤ï¼šä½¿ç”¨åœ¨ä¸»è®­ç»ƒå™¨ä¸­å®šä¹‰çš„agent_idså’Œnum_agentsï¼Œç¡®ä¿ç´¢å¼•ä¸€è‡´æ€§
                                one_hot = np.zeros(self.num_agents, dtype=np.float32)
                                one_hot[self.agent_ids.index(agent_id)] = 1.0
                                augmented_next_state = np.concatenate([next_global_state, one_hot]).astype(np.float32)
                                bootstrap_value = self.shared_network.get_value(augmented_next_state)
                            else:
                                bootstrap_value = None

                            # ğŸ”§ ç¼ºé™·ä¿®å¤ï¼šå°†é…ç½®ä¸­çš„ä¼˜åŠ¿è£å‰ªå€¼ä¼ é€’ç»™get_batch
                            advantage_clip_val = PPO_NETWORK_CONFIG.get("advantage_clip_val")
                            states, global_states, actions, old_probs, advantages, returns = buffer.get_batch(
                                next_value_if_truncated=bootstrap_value,
                                advantage_clip_val=advantage_clip_val
                            )
                            
                            # å°†å¤„ç†å¥½çš„æ•°æ®èšåˆåˆ°æ€»æ‰¹æ¬¡ä¸­
                            all_states.extend(states)
                            all_global_states.extend(global_states)
                            all_actions.extend(actions)
                            all_old_probs.extend(old_probs)
                            all_advantages.extend(advantages)
                            all_returns.extend(returns)
                            
                except Exception as e:
                    print(f"âŒ ä¸€ä¸ªå¹¶è¡Œå·¥ä½œè¿›ç¨‹å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()

        if not all_states:
            # è¿”å›æ—¶å°†å®Œæˆç»Ÿè®¡ç¼–ç åœ¨Noneæ‰¹æ¬¡æ—è¾¹ï¼ˆé€šè¿‡æ€»å¥–åŠ±çš„infoåœ¨å¤–å±‚æ‰“å°ï¼‰
            self._last_collect_finished_workers = finished_workers
            self._last_collect_completed_workers = completed_workers
            avg_reward = total_reward / finished_workers if finished_workers > 0 else 0.0
            return avg_reward, None

        # å°†èšåˆåçš„æ•°æ®åˆ—è¡¨è½¬æ¢ä¸ºNumPyæ•°ç»„ï¼Œå½¢æˆæœ€ç»ˆçš„è®­ç»ƒæ‰¹æ¬¡
        batch = {
            "states": np.array(all_states),
            "global_states": np.array(all_global_states),
            "actions": np.array(all_actions),
            "old_probs": np.array(all_old_probs),
            "advantages": np.array(all_advantages),
            "returns": np.array(all_returns),
        }
        # è®°å½•æœ¬è½®é‡‡é›†å®Œæˆworkerä¸è¾¾æˆworkeræ•°é‡ï¼Œä¾›å¤–å±‚æ—¥å¿—æ‰“å°
        self._last_collect_finished_workers = finished_workers
        self._last_collect_completed_workers = completed_workers
        avg_reward = total_reward / finished_workers if finished_workers > 0 else 0.0
        return avg_reward, batch
    
    def update_policy(self, batch: Dict[str, np.ndarray], entropy_coeff: float) -> Dict[str, float]:
        """
        ä¸“å®¶ä¿®å¤ï¼šæ¥æ”¶å·²å¤„ç†å¥½çš„æ•°æ®æ‰¹æ¬¡ï¼Œæ‰§è¡Œæ ‡å‡†çš„PPOæ›´æ–°æµç¨‹
        - ç§»é™¤äº†æ•°æ®èšåˆå’ŒGAEè®¡ç®—é€»è¾‘ï¼Œå› ä¸ºè¿™äº›å·²åœ¨ `collect_and_process_experience` ä¸­å®Œæˆ
        """
        # 1. ä»æ‰¹æ¬¡ä¸­è§£åŒ…æ•°æ®
        all_states = batch["states"]
        all_global_states = batch["global_states"]
        all_actions = batch["actions"]
        all_old_probs = batch["old_probs"]
        all_advantages = batch["advantages"]
        all_returns = batch["returns"]

        total_samples = len(all_states)
        if total_samples == 0:
            return {}

        # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡
        total_actor_loss, total_critic_loss, total_entropy = 0, 0, 0
        total_approx_kl, total_clip_fraction = 0, 0
        update_count = 0

        # 2. æ ‡å‡†PPOæ›´æ–°å¾ªç¯ (Epochs + Mini-batch)
        ppo_epochs = PPO_NETWORK_CONFIG.get("ppo_epochs", 10)
        num_minibatches = PPO_NETWORK_CONFIG.get("num_minibatches", 4)
        
        if total_samples < num_minibatches:
            num_minibatches = 1
            
        batch_size = total_samples // num_minibatches

        for epoch in range(ppo_epochs):
            # 2.1. æ•°æ®éšæœºåŒ– (Shuffle)
            indices = np.arange(total_samples)
            np.random.shuffle(indices)

            shuffled_states = all_states[indices]
            shuffled_global_states = all_global_states[indices]
            shuffled_actions = all_actions[indices]
            shuffled_old_probs = all_old_probs[indices]
            shuffled_advantages = all_advantages[indices]
            shuffled_returns = all_returns[indices]

            # 2.2. Mini-batch è®­ç»ƒ
            for i in range(0, total_samples, batch_size):
                start = i
                end = i + batch_size
                
                if end > total_samples:
                    end = total_samples
                if start == end:
                    continue

                # æå–Mini-batchæ•°æ®
                mini_batch_states = shuffled_states[start:end]
                mini_batch_global_states = shuffled_global_states[start:end]
                mini_batch_actions = shuffled_actions[start:end]
                mini_batch_old_probs = shuffled_old_probs[start:end]
                mini_batch_advantages = shuffled_advantages[start:end]
                mini_batch_returns = shuffled_returns[start:end]

                # 2.3. æ‰§è¡Œç½‘ç»œæ›´æ–°
                loss_info = self.shared_network.update(
                    mini_batch_states,
                    mini_batch_global_states,
                    mini_batch_actions,
                    mini_batch_old_probs,
                    mini_batch_advantages,
                    mini_batch_returns,
                    entropy_coeff=entropy_coeff
                )

                # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
                if loss_info:
                    total_actor_loss += loss_info["actor_loss"]
                    total_critic_loss += loss_info["critic_loss"]
                    total_entropy += loss_info["entropy"]
                    total_approx_kl += loss_info["approx_kl"]
                    total_clip_fraction += loss_info["clip_fraction"]
                    update_count += 1
        
        # è¿”å›å¹³å‡æŸå¤±
        if update_count > 0:
            return {
                "actor_loss": total_actor_loss / update_count,
                "critic_loss": total_critic_loss / update_count,
                "entropy": total_entropy / update_count,
                "approx_kl": total_approx_kl / update_count,
                "clip_fraction": total_clip_fraction / update_count,
            }
        return {}
    
    def _independent_exam_evaluation(self, env, curriculum_config, seed):
        """ğŸ”§ V33 æ–°å¢ï¼šç‹¬ç«‹çš„è€ƒè¯•è¯„ä¼°ï¼Œç¡®ä¿æ¯è½®éƒ½æ˜¯å…¨æ–°çš„ä»¿çœŸ"""
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)
        
        observations, _ = env.reset(seed=seed)
        episode_reward = 0
        step_count = 0
        
        while step_count < self.max_steps_for_eval:
            actions = {}
            
            # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼Œä½†åŸºäºæ–°çš„éšæœºç¯å¢ƒçŠ¶æ€
            for agent in env.agents:
                if agent in observations:
                    state = tf.expand_dims(observations[agent], 0)
                    action_probs = self.shared_network.actor(state)
                    # ğŸ”§ ä½¿ç”¨ç¡®å®šæ€§è¯„ä¼°ï¼Œä½†ä¿ç•™å°‘é‡æ¢ç´¢
                    if random.random() < EVALUATION_CONFIG["exploration_rate"]:
                        action = int(tf.random.categorical(tf.math.log(action_probs + 1e-8), 1)[0])
                    else:
                        action = int(tf.argmax(action_probs[0]))
                    actions[agent] = action
            
            observations, rewards, terminations, truncations, infos = env.step(actions)
            episode_reward += sum(rewards.values())
            step_count += 1
            
            if any(terminations.values()) or any(truncations.values()):
                break
        
        # è·å–æœ€ç»ˆç»Ÿè®¡
        final_stats = env.sim.get_final_stats()
        return {
            'mean_reward': episode_reward,
            'mean_makespan': final_stats.get('makespan', 0),
            'mean_utilization': final_stats.get('mean_utilization', 0),
            'mean_completed_parts': final_stats.get('total_parts', 0),
            'mean_tardiness': final_stats.get('total_tardiness', 0)
        }
    
    def quick_kpi_evaluation(self, num_episodes: int = 3, curriculum_config: Dict[str, Any] = None) -> Dict[str, float]:
        """ğŸ”§ V39ä¿®å¤ï¼šå¿«é€ŸKPIè¯„ä¼°ï¼ˆæ”¯æŒè¯¾ç¨‹å­¦ä¹ é…ç½®å’Œé™é»˜æ¨¡å¼ï¼‰"""
        # ğŸ”§ V39ä¿®å¤ï¼šåˆ›å»ºç¯å¢ƒæ—¶ä¼ é€’è¯¾ç¨‹é…ç½®ï¼ŒåŒ…æ‹¬é™é»˜æ¨¡å¼
        # è¯¾ç¨‹é…ç½®ç›´æ¥é€šè¿‡make_parallel_envä¼ é€’ï¼Œç”±ç¯å¢ƒå†…éƒ¨å¤„ç†
        if curriculum_config:
            curriculum_config = curriculum_config.copy()
            env = make_parallel_env(curriculum_config)
        else:
            # ğŸ”§ V39 ä¿®å¤ä¸€ä¸ªæ½œåœ¨bugï¼šæ­£ç¡®è§£åŒ…create_environmentçš„è¿”å›å€¼
            env, _ = self.create_environment()
        
        total_rewards = []
        makespans = []
        utilizations = []
        completed_parts_list = []
        tardiness_list = []
        
        for episode in range(num_episodes):
            observations, _ = env.reset()
            episode_reward = 0
            step_count = 0
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„æ­¥æ•°é™åˆ¶
            while step_count < self.max_steps_for_eval:
                actions = {}
                
                # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥è¯„ä¼°
                for agent in env.agents:
                    if agent in observations:
                        state = tf.expand_dims(observations[agent], 0)
                        action_probs = self.shared_network.actor(state)
                        action = int(tf.argmax(action_probs[0]))
                        actions[agent] = action
                
                observations, rewards, terminations, truncations, infos = env.step(actions)
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if any(terminations.values()) or any(truncations.values()):
                    break
            
            # è·å–æœ€ç»ˆç»Ÿè®¡
            final_stats = env.sim.get_final_stats()
            total_rewards.append(episode_reward)
            makespans.append(final_stats.get('makespan', 0))
            utilizations.append(final_stats.get('mean_utilization', 0))
            completed_parts_list.append(final_stats.get('total_parts', 0))
            tardiness_list.append(final_stats.get('total_tardiness', 0))
        
        # ğŸ”§ V37 æ–°å¢ï¼šæ£€æŸ¥ç¯å¢ƒé‡ç½®ä¿¡å·
        strategy_reset_signal = getattr(env.sim, '_trigger_strategy_reset', False)
        if strategy_reset_signal:
            self._env_strategy_reset_signal = True
        
        env.close()
        
        return {
            'mean_reward': np.mean(total_rewards),
            'mean_makespan': np.mean(makespans),
            'mean_utilization': np.mean(utilizations),
            'mean_completed_parts': np.mean(completed_parts_list),
            'mean_tardiness': np.mean(tardiness_list)
        }
    
    def simple_evaluation(self, num_episodes: int = 5) -> Dict[str, float]:
        """ğŸ”§ ä¿®å¤ç‰ˆï¼šç®€å•è¯„ä¼°ï¼Œè¿”å›æ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡"""
        env, _ = self.create_environment()
        
        total_rewards = []
        total_steps = []
        makespans = []
        completed_parts = []
        utilizations = []
        tardiness_list = []
        
        for episode in range(num_episodes):
            observations, _ = env.reset()
            episode_reward = 0
            step_count = 0
            
            while step_count < self.max_steps_for_eval:
                actions = {}
                
                # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥è¯„ä¼°
                for agent in env.agents:
                    if agent in observations:
                        state = tf.expand_dims(observations[agent], 0)
                        action_probs = self.shared_network.actor(state)
                        action = int(tf.argmax(action_probs[0]))
                        actions[agent] = action
                
                observations, rewards, terminations, truncations, infos = env.step(actions)
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if any(terminations.values()) or any(truncations.values()):
                    break
            
            # ğŸ”§ ä¿®å¤ï¼šè·å–å®Œæ•´çš„ä¸šåŠ¡æŒ‡æ ‡
            final_stats = env.sim.get_final_stats()
            total_rewards.append(episode_reward)
            total_steps.append(step_count)
            makespans.append(final_stats.get('makespan', 0))
            completed_parts.append(final_stats.get('total_parts', 0))
            utilizations.append(final_stats.get('mean_utilization', 0))
            tardiness_list.append(final_stats.get('total_tardiness', 0))
        
        env.close()
        
        return {
            'mean_reward': np.mean(total_rewards),
            'std_reward': np.std(total_rewards),
            'mean_steps': np.mean(total_steps),
            'mean_makespan': np.mean(makespans),
            'mean_completed_parts': np.mean(completed_parts),
            'mean_utilization': np.mean(utilizations),
            'mean_tardiness': np.mean(tardiness_list)
        }
    
    
    def train(self, max_episodes: int = 1000, steps_per_episode: int = 200, 
              eval_frequency: int = 20, adaptive_mode: bool = True):
        """ğŸ”§ V31 è‡ªé€‚åº”è®­ç»ƒä¸»å¾ªç¯ï¼šæ ¹æ®æ€§èƒ½è‡ªåŠ¨è°ƒæ•´è®­ç»ƒç­–ç•¥å’Œè½®æ•°"""
        # ğŸ”§ V31 è‡ªé€‚åº”æ¨¡å¼ï¼šæœ€å¤§è½®æ•°ä½œä¸ºä¸Šé™ï¼Œå®é™…è½®æ•°æ ¹æ®æ€§èƒ½åŠ¨æ€å†³å®š

        if adaptive_mode:
            self.training_targets["max_episodes"] = max_episodes
        
        # ğŸ”§ V16ï¼šæ˜¾ç¤ºè¯¾ç¨‹å­¦ä¹ é…ç½®
        curriculum_config = self.training_flow_config["foundation_phase"]["curriculum_learning"]
        if curriculum_config.get("enabled", False):
            print(f"ğŸ“š è¯¾ç¨‹å­¦ä¹ å·²å¯ç”¨ï¼Œå…±{len(curriculum_config['stages'])}ä¸ªé˜¶æ®µ:")
            for i, stage in enumerate(curriculum_config["stages"]):
                print(f"   é˜¶æ®µ{i+1}: {stage['name']} - è®¢å• {stage['orders_scale']*100:.0f}%")
        print("=" * 80)
        
        if not validate_config():
            print("âŒ é…ç½®éªŒè¯å¤±è´¥")
            return
        
        # è®­ç»ƒå¼€å§‹æ—¶é—´è®°å½•
        training_start_time = time.time()
        training_start_datetime = datetime.now()
        print(f"ğŸ• è®­ç»ƒå¼€å§‹æ—¶é—´: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        
        # ğŸ”§ V16ï¼šè¯¾ç¨‹å­¦ä¹ ç®¡ç†
        curriculum_config = self.training_flow_config["foundation_phase"]["curriculum_learning"]
        curriculum_enabled = curriculum_config.get("enabled", False)
        current_stage = 0
        stage_episode_count = 0
        
        # ğŸ”§ V8 ä¼˜åŒ–: ä¸å†éœ€è¦åˆ›å»ºä¸»ç¯å¢ƒï¼Œåªåˆ›å»ºç¼“å†²åŒº
        buffers = {
            agent: ExperienceBuffer() 
            for agent in self.agent_ids
        }
        
        best_reward = float('-inf')
        best_makespan = float('inf')
        
        # ğŸ”§ V27 æ ¸å¿ƒä¿®å¤ï¼šä¸ºè¯¾ç¨‹å­¦ä¹ çš„æ¯ä¸ªé˜¶æ®µç‹¬ç«‹è·Ÿè¸ªæœ€ä½³åˆ†æ•°
        stage_best_scores = [float('-inf')] * len(curriculum_config["stages"]) if curriculum_enabled else []
        
        # ğŸ”§ åˆå§‹åŒ–ç”¨äºè¯¾ç¨‹å­¦ä¹ æ¯•ä¸šæ£€æŸ¥çš„æ€§èƒ½æŒ‡æ ‡ï¼Œæ¯•ä¸šæ£€æŸ¥å°†ä½¿ç”¨ä¸Šä¸€ä¸ªå›åˆçš„å‡†ç¡®æ•°æ®
        last_kpi_results = {}
        last_current_score = 0.0
        
        try:
            for episode in range(max_episodes):
                iteration_start_time = time.time()
                
                # --- æ ¸å¿ƒåˆ›æ–°ï¼šåŸºç¡€è®­ç»ƒ + éšæœºé¢†åŸŸå¼ºåŒ– é€»è¾‘ ---
                current_curriculum_config = None
                
                # é¦–å…ˆå¤„ç†è¯¾ç¨‹å­¦ä¹ é€»è¾‘ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if curriculum_enabled and not self.foundation_training_completed:
                    stage_config = curriculum_config["stages"][current_stage]
                    
                    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³è‡ªé€‚åº”æ¯•ä¸šæ¡ä»¶
                    if self.check_curriculum_stage_graduation(last_kpi_results, last_current_score, stage_config):
                        print(f"âœ… é˜¶æ®µ '{stage_config['name']}' æ¯•ä¸šæ ‡å‡†è¾¾æˆï¼")
                        
                        if stage_config.get('is_final_stage', False):
                            print("ğŸ† è¯¾ç¨‹å­¦ä¹ å®Œæˆï¼ç°åœ¨å¼€å§‹åŸºç¡€èƒ½åŠ›è®¤è¯ï¼Œé€šè¿‡åè¿›å…¥æ³›åŒ–å¼ºåŒ–é˜¶æ®µã€‚")
                            # æ ‡è®°è¯¾ç¨‹å­¦ä¹ éƒ¨åˆ†ç»“æŸï¼Œåç»­é€»è¾‘å°†æ¥ç®¡å¹¶å¯åŠ¨åŸºç¡€èƒ½åŠ›è®¤è¯
                            self.foundation_training_completed = True 
                        else:
                            # æ™‹çº§åˆ°ä¸‹ä¸€ä¸ªè¯¾ç¨‹é˜¶æ®µ
                            current_stage += 1
                            stage_episode_count = 0
                            self.curriculum_stage_achievement_count = 0  # ä¸ºæ–°é˜¶æ®µé‡ç½®è®¡æ•°å™¨
                            next_stage_name = curriculum_config["stages"][current_stage]['name']
                            print(f"ğŸš€ è¿›å…¥ä¸‹ä¸€é˜¶æ®µ: '{next_stage_name}'")
                    
                    # è·å–å½“å‰é˜¶æ®µé…ç½® (é˜¶æ®µå¯èƒ½å·²æ›´æ–°)
                    stage = curriculum_config["stages"][current_stage]
                    current_curriculum_config = {
                        'orders_scale': stage.get('orders_scale', 1.0),
                        'time_scale': stage.get('time_scale', 1.0),
                        'stage_name': stage.get('name', f'Stage {current_stage}')
                    }
                    
                    # è¯¦ç»†çš„é˜¶æ®µåˆ‡æ¢å’ŒçŠ¶æ€æ—¥å¿—
                    if stage_episode_count == 0:
                        print(f"ğŸ“š [å›åˆ {episode+1}] ğŸ”„ è¯¾ç¨‹å­¦ä¹ é˜¶æ®µåˆ‡æ¢!")
                        print(f"   æ–°é˜¶æ®µ: {stage['name']}")
                        print(f"   è®¢å•æ¯”ä¾‹: {stage['orders_scale']} (ç›®æ ‡é›¶ä»¶æ•°: {int(get_total_parts_count() * stage['orders_scale'])})")
                        print(f"   æ—¶é—´æ¯”ä¾‹: {stage['time_scale']} (æ—¶é—´é™åˆ¶: {int(SIMULATION_TIME * stage['time_scale'])}åˆ†é’Ÿ)")
                        print(f"ğŸ”§ å½“å‰è¯¾ç¨‹é…ç½®å°†ä¼ é€’ç»™æ‰€æœ‰worker: orders_scale={stage['orders_scale']}, time_scale={stage['time_scale']}")
                        print("-" * 60)
                    
                    # ğŸ”§ V17æ–°å¢ï¼šæ¯10è½®æ˜¾ç¤ºé˜¶æ®µçŠ¶æ€
                    if episode % 10 == 0:
                        print(f"ğŸ“š è¯¾ç¨‹çŠ¶æ€: {stage['name']} (ç¬¬ {stage_episode_count} å›åˆ)")
                        print(f"   å½“å‰éš¾åº¦: {int(get_total_parts_count() * stage['orders_scale'])}é›¶ä»¶, {stage['time_scale']:.1f}xæ—¶é—´")    
                    stage_episode_count += 1
                
                # --- æ ¸å¿ƒè®­ç»ƒé˜¶æ®µåˆ¤æ–­ ---
                
                # æ£€æŸ¥è¯¾ç¨‹å­¦ä¹ æ˜¯å¦å·²å®Œæˆæ‰€æœ‰é˜¶æ®µ
                curriculum_just_completed = False
                if curriculum_enabled and self.foundation_training_completed and not self.generalization_phase_active:
                    # è¿™æ˜¯ä¸€ä¸ªè¿‡æ¸¡çŠ¶æ€ï¼Œè¡¨ç¤ºè¯¾ç¨‹å­¦ä¹ åˆšåˆšå®Œæˆï¼Œä½†è¿˜æœªæ­£å¼è¿›å…¥æ³›åŒ–é˜¶æ®µ
                    # åœ¨è¿™ä¸ªçŠ¶æ€ä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åŸºç¡€èƒ½åŠ›è®¤è¯çš„é…ç½®
                    curriculum_just_completed = True

                if not self.foundation_training_completed or curriculum_just_completed:
                    # é˜¶æ®µ1ï¼šåŸºç¡€èƒ½åŠ›è®­ç»ƒé˜¶æ®µ
                    # å¦‚æœè¯¾ç¨‹å­¦ä¹ æœªå¯ç”¨ï¼Œæˆ–åˆšåˆšå®Œæˆï¼Œåˆ™ä½¿ç”¨æ ‡å‡†çš„åŸºç¡€è®¢å•è¿›è¡Œè®­ç»ƒ
                    if not curriculum_enabled or curriculum_just_completed:
                        foundation_config = {
                            'orders_scale': 1.0,
                            'time_scale': 1.0,
                            'stage_name': 'åŸºç¡€èƒ½åŠ›è®¤è¯',
                            'custom_orders': BASE_ORDERS
                        }
                        current_curriculum_config = foundation_config
                    
                    # åœ¨æ¯ä¸ªå›åˆéƒ½æ·»åŠ å½“å‰å›åˆæ•°ï¼Œä¾›ç¯å¢ƒå†…éƒ¨ä½¿ç”¨
                        if current_curriculum_config:
                            current_curriculum_config['current_episode'] = episode
                    
                    if episode % 20 == 0:
                        phase_name = "è¯¾ç¨‹å­¦ä¹ ä¸­" if curriculum_enabled and not curriculum_just_completed else "åŸºç¡€èƒ½åŠ›è®¤è¯ä¸­"
                        foundation_criteria = self.training_flow_config["foundation_phase"]["graduation_criteria"]
                        print(f"ğŸ“š {phase_name}: è¿ç»­è¾¾æ ‡ {self.foundation_achievement_count}/{foundation_criteria['target_consistency']} æ¬¡")
                
                elif not self.generalization_phase_active:
                    # åŸºç¡€è®­ç»ƒåˆšå®Œæˆï¼Œå‡†å¤‡è¿›å…¥æ³›åŒ–é˜¶æ®µ
                    self.generalization_phase_active = True
                    print("\n" + "="*80)
                    print(f"ğŸš€ [å›åˆ {episode+1}] åŸºç¡€è®­ç»ƒå·²å®Œæˆï¼Œæ­£å¼è¿›å…¥éšæœºé¢†åŸŸå¼ºåŒ–é˜¶æ®µ!")
                    print("   æ¯è½®å°†ä½¿ç”¨å…¨æ–°çš„éšæœºè®¢å•é…ç½®ï¼Œå¹¶å¯ç”¨ç¯å¢ƒæ‰°åŠ¨ã€‚")
                    print("   è¿™å°†å…¨é¢é”»ç‚¼æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚")
                    print("="*80 + "\n")
                
                if self.generalization_phase_active:
                    # é˜¶æ®µ2ï¼šéšæœºé¢†åŸŸå¼ºåŒ–é˜¶æ®µ
                    # æ¯è½®ç”Ÿæˆå…¨æ–°çš„éšæœºè®¢å•é…ç½®
                    random_orders = generate_random_orders()
                    generalization_config = {
                        'custom_orders': random_orders,
                        'randomize_env': True,  # å¯ç”¨ç¯å¢ƒæ‰°åŠ¨
                        'stage_name': f'éšæœºé¢†åŸŸå¼ºåŒ–-R{episode}',
                        'current_episode': episode
                    }
                    
                    current_curriculum_config = generalization_config
                    
                    if episode % 20 == 0:
                        total_parts = sum(order["quantity"] for order in random_orders)
                        generalization_criteria = self.training_flow_config["generalization_phase"]["completion_criteria"]
                        print(f"ğŸ² éšæœºé¢†åŸŸå¼ºåŒ–: æœ¬è½®{len(random_orders)}ä¸ªè®¢å•ï¼Œå…±{total_parts}ä¸ªé›¶ä»¶")
                        print(f"   æ³›åŒ–é˜¶æ®µè¿ç»­è¾¾æ ‡: {self.generalization_achievement_count}/{generalization_criteria['target_consistency']} æ¬¡")
                

                collect_start_time = time.time()
                episode_reward, batch = self.collect_and_process_experience(steps_per_episode, current_curriculum_config)
                collect_duration = time.time() - collect_start_time
                
                # ğŸ”§ V6 å®‰å…¨çš„ç­–ç•¥æ›´æ–°ï¼ˆåŒ…å«å†…å­˜æ£€æŸ¥ï¼‰
                update_start_time = time.time()
                if batch is not None:
                    losses = self.update_policy(batch, entropy_coeff=self.current_entropy_coeff)
                else:
                    # ç©ºæ‰¹æ¬¡é˜²å¾¡ï¼šæä¾›å®‰å…¨çš„é»˜è®¤æŒ‡æ ‡å¹¶è·³è¿‡æ›´æ–°
                    losses = {
                        'actor_loss': 0.0,
                        'critic_loss': 0.0,
                        'entropy': float(self.current_entropy_coeff),
                        'approx_kl': 0.0,
                        'clip_fraction': 0.0,
                    }
                update_duration = time.time() - update_start_time
                
                # è®°å½•ç»Ÿè®¡
                iteration_end_time = time.time()
                iteration_duration = iteration_end_time - iteration_start_time
                self.iteration_times.append(iteration_duration)
                self.episode_rewards.append(episode_reward)

                
                # æå‰è¿›è¡ŒKPIè¯„ä¼°ï¼Œä»¥ä¾¿æ•´åˆTensorBoardæ—¥å¿—
                kpi_results = self.quick_kpi_evaluation(num_episodes=1, curriculum_config=current_curriculum_config)
                self.kpi_history.append(kpi_results)

                # ğŸ”§ æ ¸å¿ƒæ”¹é€ ï¼šè®¡ç®—å½“å‰å›åˆçš„ç»¼åˆè¯„åˆ†
                current_score = calculate_episode_score(kpi_results, config=current_curriculum_config)
                
                # ğŸ”§ BUGä¿®å¤ï¼šä¿å­˜æœ¬å›åˆçš„KPIç»“æœï¼Œä¾›ä¸‹ä¸€å›åˆçš„æ¯•ä¸šæ£€æŸ¥ä½¿ç”¨
                last_kpi_results = kpi_results
                last_current_score = current_score
                
                # --- æ ¸å¿ƒåˆ›æ–°ï¼šæ£€æŸ¥é˜¶æ®µè½¬æ¢å’Œè®­ç»ƒå®Œæˆæ¡ä»¶ ---
                target_parts_for_check = self._get_target_parts(current_curriculum_config)
                
                completion_rate_for_check = (kpi_results.get('mean_completed_parts', 0) / target_parts_for_check) * 100 if target_parts_for_check > 0 else 0
                
                # ğŸ”§ ä¿®å¤ï¼šåªæœ‰åœ¨æœ€ç»ˆé˜¶æ®µæˆ–è¯¾ç¨‹å­¦ä¹ å®Œæˆåæ‰æ£€æŸ¥åŸºç¡€è®­ç»ƒå®Œæˆ
                should_check_foundation_completion = False
                if not self.foundation_training_completed:
                    if curriculum_enabled:
                        # è¯¾ç¨‹å­¦ä¹ æ¨¡å¼ï¼šåªæœ‰åœ¨æœ€ç»ˆé˜¶æ®µæ‰æ£€æŸ¥åŸºç¡€è®­ç»ƒå®Œæˆ
                        if current_stage < len(curriculum_config["stages"]):
                            current_stage_info = curriculum_config["stages"][current_stage]
                            if current_stage_info.get('is_final_stage', False):
                                should_check_foundation_completion = True
                        # æˆ–è€…è¯¾ç¨‹å­¦ä¹ å·²å®Œæˆæ‰€æœ‰é˜¶æ®µ
                        elif current_stage >= len(curriculum_config["stages"]):
                            should_check_foundation_completion = True
                    else:
                        # éè¯¾ç¨‹å­¦ä¹ æ¨¡å¼ï¼šç›´æ¥æ£€æŸ¥
                        should_check_foundation_completion = True
                    
                    if should_check_foundation_completion:
                        # ğŸ”§ BUGä¿®å¤ï¼šä¸è¯¾ç¨‹å­¦ä¹ é€»è¾‘ç»Ÿä¸€ï¼Œä½¿ç”¨ä¸Šä¸€ä¸ªå›åˆçš„KPIç»“æœæ¥åˆ¤æ–­æ˜¯å¦æ¯•ä¸š
                        if self.check_foundation_training_completion(last_kpi_results, last_current_score):
                            self.foundation_training_completed = True
                
                # æ£€æŸ¥æ³›åŒ–è®­ç»ƒæ˜¯å¦å®Œæˆï¼ˆè¿™å°†è§¦å‘æ•´ä¸ªè®­ç»ƒçš„ç»“æŸï¼‰
                training_should_end = False
                if self.generalization_phase_active:
                    if self.check_generalization_training_completion(current_score, completion_rate_for_check):
                        training_should_end = True
                
                # --- ğŸ”§ ä¿®å¤ï¼šè‡ªé€‚åº”ç†µçš„åœæ»è®¡æ•°å™¨ä»…åœ¨å…è®¸ç†µå¢åŠ çš„é˜¶æ®µç´¯ç§¯ ---
                # 1. åˆ¤æ–­æ˜¯å¦å¤„äºå…è®¸ç†µå¢åŠ çš„é˜¶æ®µ
                # è¯¾ç¨‹å­¦ä¹ ä¸‹ï¼šä»…å½“å¤„äºæœ€ç»ˆé˜¶æ®µæˆ–å·²ç»è¿›å…¥æ³›åŒ–é˜¶æ®µæ‰å…è®¸ï¼›
                # éè¯¾ç¨‹å­¦ä¹ ï¼šå…¨ç¨‹å…è®¸ã€‚
                curriculum_is_final_stage = False
                if curriculum_enabled and not self.foundation_training_completed and current_stage < len(curriculum_config["stages"]):
                    curriculum_is_final_stage = bool(curriculum_config["stages"][current_stage].get("is_final_stage", False))

                allow_entropy_increase = (not curriculum_enabled) or curriculum_is_final_stage or self.generalization_phase_active
                
                # 2. åªåœ¨å…è®¸ç†µå¢åŠ çš„é˜¶æ®µæ‰ç´¯ç§¯åœæ»è®¡æ•°
                if allow_entropy_increase:
                    self.epochs_without_improvement += 1
                else:
                    # éç†µå¢åŠ é˜¶æ®µï¼Œé‡ç½®è®¡æ•°å™¨ï¼ˆé¿å…ç´¯ç§¯æ— æ„ä¹‰çš„åœæ»ï¼‰
                    self.epochs_without_improvement = 0
                    self.stagnation_level = 0
                
                # 3. è‡ªé€‚åº”ç†µè°ƒæ•´é€»è¾‘
                adaptive_entropy_enabled = ADAPTIVE_ENTROPY_CONFIG["enabled"]
                start_episode = ADAPTIVE_ENTROPY_CONFIG["start_episode"]
                patience = ADAPTIVE_ENTROPY_CONFIG["patience"]
                boost_factor = ADAPTIVE_ENTROPY_CONFIG["boost_factor"]

                # æ­£ç¡®çš„è§¦å‘ç‚¹ï¼šåœ¨ç¬¬ start_episode + patience å›åˆä¹‹åæ‰å¯èƒ½è§¦å‘
                if adaptive_entropy_enabled and allow_entropy_increase and episode >= (start_episode + patience):
                    # å½“å‰çš„å®Œæˆç‡ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦é™ä½ç†µ
                    target_parts_for_entropy = self._get_target_parts(current_curriculum_config)
                    completion_rate_for_entropy = kpi_results['mean_completed_parts'] / (target_parts_for_entropy + 1e-6)

                    # æ£€æŸ¥æ˜¯å¦åœæ»
                    if self.epochs_without_improvement >= patience:
                        self.stagnation_level += 1
                        boost_multiplier = 1.0 + boost_factor * self.stagnation_level
                        self.current_entropy_coeff = min(
                            self.current_entropy_coeff * boost_multiplier,
                            PPO_NETWORK_CONFIG["entropy_coeff"] * 5 # è®¾ç½®ä¸€ä¸ªç¡¬ä¸Šé™ï¼Œä¾‹å¦‚åŸå§‹çš„5å€
                        )
                        print(f"ğŸ“ˆ åœæ»ç­‰çº§ {self.stagnation_level}! æ€§èƒ½å·²åœæ» {self.epochs_without_improvement} å›åˆã€‚")
                        print(f"   é‡‡å–å¼ºåŠ›æªæ–½: å°†ç†µæå‡è‡³ {self.current_entropy_coeff:.4f} (æå‡å› å­: {boost_multiplier:.2f})")
                        # æ ¸å¿ƒä¿®å¤ï¼šé‡ç½®è®¡æ•°å™¨ï¼Œç»™äºˆæ¨¡å‹é€‚åº”æ–°ç†µå€¼çš„çª—å£æœŸ
                        self.epochs_without_improvement = 0
                    
                    # ğŸ”§ ç¼ºé™·å››ä¿®å¤ï¼šä½¿ç”¨é…ç½®åŒ–çš„ç†µè¡°å‡é€»è¾‘
                    elif completion_rate_for_entropy > ADAPTIVE_ENTROPY_CONFIG["high_completion_threshold"]:
                        self.current_entropy_coeff = max(
                            self.current_entropy_coeff * ADAPTIVE_ENTROPY_CONFIG["high_completion_decay"],
                            ADAPTIVE_ENTROPY_CONFIG["min_entropy"]
                        )
                
                # ç¡®ä¿ç†µä¸ä¼šä½äºè®¾å®šçš„æœ€å°å€¼
                self.current_entropy_coeff = max(self.current_entropy_coeff, ADAPTIVE_ENTROPY_CONFIG["min_entropy"])

                
                # ğŸ”§ V36 ç»Ÿä¸€TensorBoardæ—¥å¿—è®°å½•ï¼Œå¹¶æ ¹æ®è¯¾ç¨‹é˜¶æ®µåŠ¨æ€åˆ‡æ¢run
                if TENSORBOARD_AVAILABLE:
                    # æ ¹æ®è¯¾ç¨‹é˜¶æ®µåˆ‡æ¢runï¼Œåœ¨æ‚¬åœæç¤ºä¸­æ˜¾ç¤ºé˜¶æ®µå
                    run_name = "train_default" # Fallback run name
                    if curriculum_enabled and current_curriculum_config:
                        # Get stage name and sanitize it for use as a directory name
                        run_name = current_curriculum_config['stage_name'].replace(" ", "_")
                    
                    if self.train_writer is None or self.current_tensorboard_run_name != run_name:
                        if self.train_writer is not None:
                            self.train_writer.close()
                        
                        logdir = os.path.join(self.tensorboard_dir, run_name)
                        self.train_writer = tf.summary.create_file_writer(logdir)
                        self.current_tensorboard_run_name = run_name
                        print(f"ğŸ“Š TensorBoard runå·²åˆ‡æ¢è‡³: '{run_name}'")

                    if self.train_writer:
                        with self.train_writer.as_default():
                            # è®­ç»ƒæ ¸å¿ƒæŒ‡æ ‡
                            tf.summary.scalar('Training/Avg_Episode_Reward', episode_reward, step=episode)
                            tf.summary.scalar('Training/Actor_Loss', losses['actor_loss'], step=episode)
                            tf.summary.scalar('Training/Critic_Loss', losses['critic_loss'], step=episode)
                            tf.summary.scalar('Training/Entropy', losses['entropy'], step=episode)
                            tf.summary.scalar('Training/KL_Divergence', losses['approx_kl'], step=episode)
                            tf.summary.scalar('Training/Clip_Fraction', losses['clip_fraction'], step=episode)
                            # æ€§èƒ½æŒ‡æ ‡
                            tf.summary.scalar('Performance/Iteration_Duration', iteration_duration, step=episode)
                            tf.summary.scalar('Performance/CPU_Collection_Time', collect_duration, step=episode)
                            tf.summary.scalar('Performance/GPU_Update_Time', update_duration, step=episode)
                            # ä¸šåŠ¡KPIæŒ‡æ ‡
                            tf.summary.scalar('KPI/Makespan', kpi_results['mean_makespan'], step=episode)
                            tf.summary.scalar('KPI/Completed_Parts', kpi_results['mean_completed_parts'], step=episode)
                            tf.summary.scalar('KPI/Utilization', kpi_results['mean_utilization'], step=episode)
                            tf.summary.scalar('KPI/Tardiness', kpi_results['mean_tardiness'], step=episode)
                            # è®°å½•ç»¼åˆè¯„åˆ†
                            tf.summary.scalar('KPI/Score', current_score, step=episode)
                            
                            self.train_writer.flush()
                
                # --- æ ¸å¿ƒåˆ›æ–°ï¼šæ–°çš„è®­ç»ƒç»“æŸé€»è¾‘ ---
                if training_should_end:
                    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²é€šè¿‡åŸºç¡€è®­ç»ƒå’Œæ³›åŒ–å¼ºåŒ–ä¸¤ä¸ªé˜¶æ®µçš„è®¤è¯ã€‚")
                    break
                
                # æ£€æŸ¥æœ€å¤§è½®æ•°é™åˆ¶
                if episode >= max_episodes - 1:
                    print(f"\nâ° è¾¾åˆ°æœ€å¤§è®­ç»ƒè½®æ•° {max_episodes}ï¼Œè®­ç»ƒç»“æŸã€‚")
                    break
                
                # ğŸ”§ V36 æ–°å¢ï¼šè®°å½•å½“å‰è¯¾ç¨‹é˜¶æ®µä¿¡æ¯ä¾›å…¶ä»–æ–¹æ³•ä½¿ç”¨
                if current_curriculum_config:
                    self._current_orders_scale = current_curriculum_config.get('orders_scale', 1.0)
                
                # ğŸ”§ é‡æ„ç‰ˆï¼šç®€åŒ–çš„æ€§èƒ½ç›‘æ§ï¼Œç§»é™¤å¤æ‚çš„é‡å¯æœºåˆ¶
                # åŸºç¡€æ€§èƒ½è·Ÿè¸ªï¼ˆç”¨äºè°ƒè¯•å’Œç›‘æ§ï¼‰
                current_performance = kpi_results.get('mean_completed_parts', 0)
                if not hasattr(self, '_performance_history'):
                    self._performance_history = []
                
                self._performance_history.append(current_performance)
                # åªä¿ç•™æœ€è¿‘20è½®çš„å†å²
                if len(self._performance_history) > 20:
                    self._performance_history.pop(0)
                
                
                # ğŸ”§ V38ä¿®å¤ï¼šæ¯30å›åˆè¿›è¡Œä¸€æ¬¡å®Œæ•´éš¾åº¦è¯„ä¼°ï¼ˆé™é»˜æ¨¡å¼ï¼Œé¿å…è¾“å‡ºæ±¡æŸ“ï¼‰
                if curriculum_enabled and episode > 0 and episode % 30 == 0:
                    print("\n" + "="*60)
                    print("ğŸ“ è¿›è¡Œå®Œæ•´éš¾åº¦è¯„ä¼°ï¼ˆ100%è®¢å•ï¼Œæ ‡å‡†æ—¶é—´ï¼‰...")
                    full_config = {
                        'orders_scale': 1.0,
                        'time_scale': 1.0,
                        'stage_name': 'å®Œæ•´è¯„ä¼°',
                        'silent_evaluation': True  # ğŸ”§ V38 å…³é”®ï¼šå¯ç”¨é™é»˜æ¨¡å¼
                    }
                    full_kpi = self.quick_kpi_evaluation(num_episodes=3, curriculum_config=full_config)
                    
                    # è®¡ç®—çœŸå®æ€§èƒ½æŒ‡æ ‡
                    real_completion = full_kpi.get('mean_completed_parts', 0)
                    real_completion_rate = real_completion / get_total_parts_count() * 100
                    real_makespan = full_kpi.get('mean_makespan', 0)
                    real_utilization = full_kpi.get('mean_utilization', 0)
                    
                    # ğŸ”§ V34 ä¿®å¤ï¼šè·å–å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¿®å¤è®¾å¤‡åˆ©ç”¨ç‡æ˜¾ç¤ºå¼‚å¸¸
                    real_tardiness = full_kpi.get('mean_tardiness', 0)  
                    real_reward = full_kpi.get('mean_reward', 0)
                    
                    print(f"ğŸ¯ å®Œæ•´éš¾åº¦è¯„ä¼°ç»“æœï¼ˆ3è½®å¹³å‡ï¼‰:")
                    print(f"   å¹³å‡å®Œæˆé›¶ä»¶: {real_completion:.1f}/{get_total_parts_count()} ({real_completion_rate:.1f}%)")
                    print(f"   å¹³å‡æ€»å®Œå·¥æ—¶é—´: {real_makespan:.1f}åˆ†é’Ÿ")
                    print(f"   å¹³å‡è®¾å¤‡åˆ©ç”¨ç‡: {real_utilization*100:.1f}%")
                    print(f"   å¹³å‡è®¢å•å»¶æœŸæ—¶é—´: {real_tardiness:.1f}åˆ†é’Ÿ") 
                    print(f"   å¹³å‡å¥–åŠ±: {real_reward:.1f}")
                    
                    # è¯„ä¼°è¿›å±•
                    if real_completion_rate > 90:
                        print(f"ğŸ† ä¼˜ç§€ï¼æ¥è¿‘å®Œå…¨æŒæ¡ä»»åŠ¡!")
                    elif real_completion_rate > 60:
                        print(f"ğŸ’ª è‰¯å¥½ï¼å·²å…·å¤‡åŸºæœ¬èƒ½åŠ›!")
                    elif real_completion_rate > 30:
                        print(f"ğŸ“ˆ è¿›æ­¥ä¸­ï¼ç»§ç»­åŠªåŠ›!")
                    else:
                        print(f"ğŸ“š ä»éœ€æ›´å¤šè®­ç»ƒ!")
                    print("="*60 + "\n")
                
                # ğŸ”§ V12 TensorBoard KPIè®°å½• (V36å·²æ•´åˆ)
                
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®æ›´æ–°æœ€ä½³è®°å½•ï¼ˆåªæœ‰å½“makespan > 0æ—¶æ‰æ›´æ–°ï¼‰
                current_makespan = kpi_results['mean_makespan']
                if current_makespan > 0 and current_makespan < best_makespan:
                    best_makespan = current_makespan
                
                # ------------------- ç»Ÿä¸€æ—¥å¿—è¾“å‡ºå¼€å§‹ -------------------
                
                # å‡†å¤‡KPIæ•°æ®ç”¨äºæ—¥å¿—æ˜¾ç¤º
                makespan = kpi_results['mean_makespan']
                completed_parts = kpi_results['mean_completed_parts']
                utilization = kpi_results['mean_utilization']
                tardiness = kpi_results['mean_tardiness']
                # current_score å·²ç»åœ¨å‰é¢é€šè¿‡ _calculate_score è®¡ç®—è¿‡äº†
                
                if not hasattr(self, 'best_score'):
                    self.best_score = float('-inf')

                model_update_info = ""
                timestamp = datetime.now().strftime("%m%d_%H%M") # è·å–å½“å‰æ—¶é—´æˆ³
                # ğŸ”§ æ ¸å¿ƒæ”¹é€ ï¼šåŒºåˆ†"å…¨å±€æœ€ä½³"å’Œ"æœ€ç»ˆé˜¶æ®µæœ€ä½³"
                # 1. æ›´æ–°å…¨å±€æœ€ä½³åˆ†æ•°ï¼ˆç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼‰
                if current_score > self.best_score:
                    self.best_score = current_score

                # === æ ¸å¿ƒé‡æ„ï¼šæ¨¡å‹ä¿å­˜é€»è¾‘ ===
                
                model_update_info = ""
                
                if curriculum_enabled:
                    # --- å¯ç”¨è¯¾ç¨‹å­¦ä¹ æ—¶çš„ä¿å­˜é€»è¾‘ ---
                    if not self.foundation_training_completed:
                        # 1. ä¿å­˜å½“å‰è¯¾ç¨‹é˜¶æ®µçš„æœ€ä½³æ¨¡å‹
                        if current_score > stage_best_scores[current_stage]:
                            stage_best_scores[current_stage] = current_score
                            stage_name = current_curriculum_config['stage_name'].replace(" ", "_")
                            model_path = self.save_model(f"{self.models_dir}/{timestamp}_{stage_name}_best")
                            if model_path:
                                stage_display_name = current_curriculum_config['stage_name']
                                model_update_info = f"âœ… {stage_display_name}é˜¶æ®µæœ€ä½³! æ¨¡å‹ä¿å­˜è‡³: {model_path}"
                                # ğŸ”§ ä¿®å¤ï¼šåªåœ¨æœ€ç»ˆé˜¶æ®µé‡ç½®åœæ»è®¡æ•°å™¨
                                if curriculum_is_final_stage:
                                    self.epochs_without_improvement = 0
                                    self.stagnation_level = 0
                    elif self.generalization_phase_active:
                        # 2. æ³›åŒ–å¼ºåŒ–é˜¶æ®µçš„æ¨¡å‹ä¿å­˜
                        if current_score > self.best_score_generalization_phase:
                            self.best_score_generalization_phase = current_score
                            self.best_kpi_generalization_phase = kpi_results.copy()
                            self.best_episode_generalization_phase = episode + 1
                            model_path = self.save_model(f"{self.models_dir}/{timestamp}general_train_best")
                            if model_path:
                                model_update_info = f"ğŸ† æ³›åŒ–å¼ºåŒ–é˜¶æ®µæœ€ä½³! æ¨¡å‹ä¿å­˜è‡³: {model_path}"
                                # ğŸ”§ ä¿®å¤ï¼šæ³›åŒ–é˜¶æ®µä¿å­˜æœ€ä½³æ¨¡å‹æ—¶é‡ç½®åœæ»è®¡æ•°å™¨
                                self.epochs_without_improvement = 0
                                self.stagnation_level = 0
                else:  # curriculum_enabled is False
                    # --- æœªå¯ç”¨è¯¾ç¨‹å­¦ä¹ æ—¶çš„ä¿å­˜é€»è¾‘ ---
                    if not self.foundation_training_completed:
                        # 1. åŸºç¡€è®­ç»ƒé˜¶æ®µçš„æ¨¡å‹ä¿å­˜
                        if current_score > self.best_score_foundation_phase:
                            self.best_score_foundation_phase = current_score
                            self.best_kpi_foundation_phase = kpi_results.copy()
                            self.best_episode_foundation_phase = episode + 1
                            model_path = self.save_model(f"{self.models_dir}/{timestamp}base_train_best")
                            if model_path:
                                model_update_info = f"âœ… åŸºç¡€è®­ç»ƒé˜¶æ®µæœ€ä½³! æ¨¡å‹ä¿å­˜è‡³: {model_path}"
                                # ğŸ”§ ä¿®å¤ï¼šéè¯¾ç¨‹å­¦ä¹ æ¨¡å¼ä¸‹ï¼ŒåŸºç¡€é˜¶æ®µä¹Ÿå¯ä»¥é‡ç½®ï¼ˆå› ä¸ºallow_entropy_increase=Trueï¼‰
                                self.epochs_without_improvement = 0
                                self.stagnation_level = 0
                    elif self.generalization_phase_active:
                        # 2. æ³›åŒ–å¼ºåŒ–é˜¶æ®µçš„æ¨¡å‹ä¿å­˜
                        if current_score > self.best_score_generalization_phase:
                            self.best_score_generalization_phase = current_score
                            self.best_kpi_generalization_phase = kpi_results.copy()
                            self.best_episode_generalization_phase = episode + 1
                            model_path = self.save_model(f"{self.models_dir}/{timestamp}general_train_best")
                            if model_path:
                                model_update_info = f"ğŸ† æ³›åŒ–å¼ºåŒ–é˜¶æ®µæœ€ä½³! æ¨¡å‹ä¿å­˜è‡³: {model_path}"
                                # ğŸ”§ ä¿®å¤ï¼šæ³›åŒ–é˜¶æ®µä¿å­˜æœ€ä½³æ¨¡å‹æ—¶é‡ç½®åœæ»è®¡æ•°å™¨
                                self.epochs_without_improvement = 0
                                self.stagnation_level = 0
                
                # 3. å…¨å±€"åŒè¾¾æ ‡"æœ€ä½³æ¨¡å‹ä¿å­˜ï¼ˆç‹¬ç«‹äºæ‰€æœ‰å…¶ä»–é€»è¾‘ï¼‰
                #    é¦–å…ˆï¼Œè·å–å½“å‰å›åˆçš„æ­£ç¡®ç›®æ ‡é›¶ä»¶æ•°
                target_parts_for_dual_check = self._get_target_parts(current_curriculum_config)
                
                completion_rate_kpi = (kpi_results.get('mean_completed_parts', 0) / target_parts_for_dual_check) * 100 if target_parts_for_dual_check > 0 else 0
                
                # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®è¯¾ç¨‹å­¦ä¹ çŠ¶æ€å†³å®šæ˜¯å¦ä¿å­˜"åŒè¾¾æ ‡"æ¨¡å‹
                save_condition_met = False
                if not curriculum_enabled:
                    # æœªå¯ç”¨è¯¾ç¨‹å­¦ä¹ ï¼šå…¨ç¨‹å…è®¸ä¿å­˜
                    save_condition_met = True
                else:
                    # å¯ç”¨è¯¾ç¨‹å­¦ä¹ ï¼šåªåœ¨æœ€ç»ˆé˜¶æ®µæˆ–æ³›åŒ–é˜¶æ®µå…è®¸ä¿å­˜
                    is_final_curriculum_stage = False
                    if not self.foundation_training_completed and current_stage < len(curriculum_config["stages"]):
                        current_stage_info = curriculum_config["stages"][current_stage]
                        is_final_curriculum_stage = current_stage_info.get('is_final_stage', False)
                    
                    if is_final_curriculum_stage or self.generalization_phase_active or curriculum_just_completed:
                        save_condition_met = True
                
                dual_objective_model_update_info = ""
                if save_condition_met and completion_rate_kpi >= 100 and current_score > self.best_score_dual_objective:
                    self.best_score_dual_objective = current_score
                    self.best_kpi_dual_objective = kpi_results.copy()
                    self.best_episode_dual_objective = episode + 1
                    dual_objective_best_path = self.save_model(f"{self.models_dir}/{timestamp}Twin_best")
                    if dual_objective_best_path:
                        dual_objective_model_update_info = f" â­å®Œæˆæ‰€æœ‰é›¶ä»¶å¾—åˆ†æœ€ä½³!æ¨¡å‹ä¿å­˜è‡³: {dual_objective_best_path}"
                        
                        # ğŸ”§ ä¿®å¤ï¼šåŒè¾¾æ ‡æ¨¡å‹ä¿å­˜æ—¶é‡ç½®åœæ»è®¡æ•°å™¨ï¼ˆå¦‚æœå¤„äºå…è®¸ç†µå¢åŠ çš„é˜¶æ®µï¼‰
                        if allow_entropy_increase:
                            print(f"ğŸ‰ æ–°çš„åŒè¾¾æ ‡æœ€ä½³æ¨¡å‹! é‡ç½®åœæ»è®¡æ•°ã€‚")
                            self.epochs_without_improvement = 0
                            self.stagnation_level = 0  # åˆ›ä¸‹æ–°é«˜ï¼Œ"è­¦æŠ¥"è§£é™¤
                
                # ------------------- ç»Ÿä¸€æ—¥å¿—è¾“å‡ºå¼€å§‹ -------------------

                 # ç¬¬ä¸€è¡Œï¼šå›åˆä¿¡æ¯å’Œæ€§èƒ½æ•°æ®
                # é‡‡é›†ç»Ÿè®¡ï¼ˆå¹¶è¡Œworkerå®Œæˆä¸è¾¾æˆæƒ…å†µï¼‰
                finished_workers = getattr(self, '_last_collect_finished_workers', self.num_workers)
                completed_workers = getattr(self, '_last_collect_completed_workers', 0)
                per_worker_avg_reward = (episode_reward / finished_workers) if finished_workers > 0 else episode_reward
                line1 = (
                    f"ğŸ”‚ è®­ç»ƒå›åˆ {episode + 1:3d}/{max_episodes} | å¹³å‡å¥–åŠ±: {episode_reward:.1f}"
                    f" (å‡å€¼/worker: {per_worker_avg_reward:.1f}, å®Œæˆå…¨éƒ¨: {completed_workers}/{finished_workers})"
                    f" | ActoræŸå¤±: {losses['actor_loss']:.4f}| â±ï¸æœ¬è½®ç”¨æ™‚: {iteration_duration:.1f}s"
                    f" (CPUé‡‡é›†: {collect_duration:.1f}s, GPUæ›´æ–°: {update_duration:.1f}s)"
                )

                # ç¬¬äºŒè¡Œï¼šKPIæ•°æ®å’Œé˜¶æ®µä¿¡æ¯ (æ ¸å¿ƒä¿®å¤ï¼šåŠ¨æ€æ˜¾ç¤ºç›®æ ‡é›¶ä»¶æ•°)
                target_parts_for_log = self._get_target_parts(current_curriculum_config)
                stage_info_str = ""
                if current_curriculum_config and 'stage_name' in current_curriculum_config:
                    stage_name = current_curriculum_config['stage_name']
                    # ğŸ”§ ä¿®å¤ï¼šæ˜¾ç¤ºä¸¤çº§é˜¶æ®µä¿¡æ¯ï¼ˆè¯¾ç¨‹å­¦ä¹ é˜¶æ®µ + åŸºç¡€è®­ç»ƒé˜¶æ®µï¼‰
                    if curriculum_enabled and not curriculum_just_completed:
                        curriculum_stage_name = curriculum_config["stages"][current_stage]['name']
                        foundation_phase = 'åŸºç¡€è®­ç»ƒ' if not self.foundation_training_completed else 'æ³›åŒ–è®­ç»ƒ'
                        stage_info_str = f"   | è¯¾ç¨‹: '{curriculum_stage_name}' | å¤§é˜¶æ®µ: '{foundation_phase}'"
                    else:
                        stage_info_str = f"   | é˜¶æ®µ: '{stage_name}'"
                
                target_parts_str = f"/{target_parts_for_log}"
                line2 = f"ğŸ“Š æ­¤å›åˆKPIè¯„ä¼° - æ€»å®Œå·¥æ—¶é—´: {makespan:.1f}min  | è®¾å¤‡åˆ©ç”¨ç‡: {utilization:.1%} | è®¢å•å»¶æœŸæ—¶é—´: {tardiness:.1f}min |  å®Œæˆé›¶ä»¶æ•°: {completed_parts:.0f}{target_parts_str}{stage_info_str}"

                # ç¬¬ä¸‰è¡Œï¼šè¯„åˆ†å’Œæ¨¡å‹æ›´æ–°ä¿¡æ¯
                phase_best_str = ""
                if curriculum_enabled:
                    # ğŸ”§ ä¿®å¤ï¼šå¯ç”¨è¯¾ç¨‹å­¦ä¹ æ—¶ï¼Œæ˜¾ç¤ºå½“å‰è¯¾ç¨‹é˜¶æ®µçš„æœ€ä½³åˆ†æ•°
                    if not self.foundation_training_completed:
                        stage_display_name = current_curriculum_config.get('stage_name', 'å½“å‰é˜¶æ®µ')
                        stage_best_str = f" ({stage_display_name}æœ€ä½³: {stage_best_scores[current_stage]:.3f})"
                        line3_score = f"ğŸš¥ å›åˆè¯„åˆ†: {current_score:.3f} (å…¨å±€æœ€ä½³: {self.best_score:.3f}){stage_best_str}"
                    elif self.generalization_phase_active:
                        phase_best_str = f" (æ³›åŒ–é˜¶æ®µæœ€ä½³: {self.best_score_generalization_phase:.3f})"
                        line3_score = f"ğŸš¥ å›åˆè¯„åˆ†: {current_score:.3f} (å…¨å±€æœ€ä½³: {self.best_score:.3f}){phase_best_str}"
                else:
                    # ğŸ”§ ä¿®å¤ï¼šæœªå¯ç”¨è¯¾ç¨‹å­¦ä¹ æ—¶ï¼Œæ˜¾ç¤ºåŸºç¡€è®­ç»ƒé˜¶æ®µçš„æœ€ä½³åˆ†æ•°
                    if not self.foundation_training_completed:
                        phase_best_str = f" (åŸºç¡€é˜¶æ®µæœ€ä½³: {self.best_score_foundation_phase:.3f})"
                    elif self.generalization_phase_active:
                        phase_best_str = f" (æ³›åŒ–é˜¶æ®µæœ€ä½³: {self.best_score_generalization_phase:.3f})"
                    line3_score = f"ğŸš¥ å›åˆè¯„åˆ†: {current_score:.3f} (å…¨å±€æœ€ä½³: {self.best_score:.3f}){phase_best_str}"
                
                # åˆå¹¶æ‰€æœ‰æ¨¡å‹æ›´æ–°ä¿¡æ¯
                combined_model_info = model_update_info + dual_objective_model_update_info
                line3 = f"{line3_score}{combined_model_info}" if combined_model_info else line3_score

                avg_time = np.mean(self.iteration_times)
                remaining_episodes = max_episodes - (episode + 1)
                estimated_remaining = remaining_episodes * avg_time
                progress_percent = ((episode + 1) / max_episodes) * 100
                current_time = datetime.now().strftime('%H:%M:%S')
                finish_str = ""
                if remaining_episodes > 0:
                    finish_time = time.time() + estimated_remaining
                    finish_str = time.strftime('%H:%M:%S', time.localtime(finish_time))
                line4 = f"ğŸ”® å½“å‰è®­ç»ƒè¿›åº¦: {progress_percent:.1f}% | å½“å‰æ—¶é—´ï¼š{current_time} | é¢„è®¡å®Œæˆæ—¶é—´: {finish_str}"

                # æ‰“å°æ—¥å¿—
                print(line1)
                print(line2)
                print(line3)
                print(line4)
                print() # æ¯ä¸ªå›åˆåæ·»åŠ ä¸€ä¸ªç©ºè¡Œ
                
                # ------------------- ç»Ÿä¸€æ—¥å¿—è¾“å‡ºç»“æŸ -------------------
                        
            
            # ğŸ”§ ä¿®å¤ç‰ˆï¼šç®€åŒ–çš„è®­ç»ƒå®Œæˆç»Ÿè®¡
            training_end_time = time.time()
            training_end_datetime = datetime.now()
            total_training_time = training_end_time - training_start_time
            
            print("\n" + "=" * 80)
            print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
            print(f"ğŸ• è®­ç»ƒå¼€å§‹: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ğŸ è®­ç»ƒç»“æŸ: {training_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f}åˆ†é’Ÿ ({total_training_time:.1f}ç§’)")
            
            # è®­ç»ƒæ•ˆç‡ç»Ÿè®¡
            if self.iteration_times:
                avg_iteration_time = np.mean(self.iteration_times)
                print(f"âš¡ å¹³å‡æ¯è½®: {avg_iteration_time:.1f}s | è®­ç»ƒæ•ˆç‡: {len(self.iteration_times)/total_training_time*60:.1f}è½®/åˆ†é’Ÿ")

            # ğŸ”§ Bugä¿®å¤ï¼šè¾“å‡ºæœ€ç»ˆçš„ã€å¯é çš„æœ€ä½³KPI
            print("\n" + "="*40)
            print("ğŸ† æœ€ç»ˆæœ€ä½³KPIè¡¨ç° (åŒé‡æ ‡å‡†æœ€ä½³) ğŸ†")
            print("="*40)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹è¾¾åˆ°äº†åŒé‡æ ‡å‡†ï¼Œå¹¶å®ç°ä¼˜é›…é™çº§
            if self.best_episode_dual_objective != -1:
                best_kpi = self.best_kpi_dual_objective
                best_episode_to_report = self.best_episode_dual_objective
            elif self.best_episode_generalization_phase != -1:
                print("âš ï¸ æœªæ‰¾åˆ°åŒé‡æ ‡å‡†æ¨¡å‹ï¼Œå°†æŠ¥å‘Šã€æ³›åŒ–é˜¶æ®µã€‘çš„æœ€ä½³æ¨¡å‹ã€‚")
                best_kpi = self.best_kpi_generalization_phase
                best_episode_to_report = self.best_episode_generalization_phase
            elif self.best_episode_foundation_phase != -1:
                print("âš ï¸ æœªæ‰¾åˆ°åŒé‡æ ‡å‡†æˆ–æ³›åŒ–é˜¶æ®µæ¨¡å‹ï¼Œå°†æŠ¥å‘Šã€åŸºç¡€è®­ç»ƒé˜¶æ®µã€‘çš„æœ€ä½³æ¨¡å‹ã€‚")
                best_kpi = self.best_kpi_foundation_phase
                best_episode_to_report = self.best_episode_foundation_phase
            else:
                print("âš ï¸ æœªèƒ½è®°å½•ä»»ä½•é˜¶æ®µçš„æœ€ä½³æ¨¡å‹ã€‚")
                # ä½¿ç”¨ä¸€ä¸ªç©ºçš„KPIå­—å…¸æ¥é¿å…é”™è¯¯
                best_kpi = self.best_kpi_dual_objective 
                best_episode_to_report = -1

            target_parts_final = get_total_parts_count() # æœ€ç»ˆè¯„ä¼°æ€»æ˜¯åŸºäºå®Œæ•´ä»»åŠ¡
            completion_rate_final = (best_kpi.get('mean_completed_parts', 0) / target_parts_final) * 100 if target_parts_final > 0 else 0
            
            print(f"   (åœ¨ç¬¬ {best_episode_to_report} å›åˆå–å¾—)") # ğŸ”§ æ–°å¢
            print(f"   å®Œæˆé›¶ä»¶: {best_kpi.get('mean_completed_parts', 0):.1f} / {target_parts_final} ({completion_rate_final:.1f}%)")
            print(f"   æ€»å®Œå·¥æ—¶é—´: {best_kpi.get('mean_makespan', 0):.1f} åˆ†é’Ÿ")
            print(f"   è®¾å¤‡åˆ©ç”¨ç‡: {best_kpi.get('mean_utilization', 0):.1%}")
            print(f"   è®¢å•å»¶æœŸæ—¶é—´: {best_kpi.get('mean_tardiness', 0):.1f} åˆ†é’Ÿ")
            print("="*40)
            
            # --- æ ¸å¿ƒä¿®å¤ï¼šè¾“å‡ºæ¯ä¸ªé˜¶æ®µçš„æœ€ä½³KPI ---
            print("\n" + "="*40)
            print("ğŸ† å„é˜¶æ®µæœ€ä½³KPIè¡¨ç° ğŸ†")
            print("="*40)

            # åŸºç¡€è®­ç»ƒé˜¶æ®µæœ€ä½³
            if self.best_episode_foundation_phase != -1:
                print("\n--- åŸºç¡€è®­ç»ƒé˜¶æ®µ ---")
                best_kpi = self.best_kpi_foundation_phase
                target_parts = get_total_parts_count()
                completion_rate = (best_kpi.get('mean_completed_parts', 0) / target_parts) * 100 if target_parts > 0 else 0
                print(f"   (åœ¨ç¬¬ {self.best_episode_foundation_phase} å›åˆå–å¾—)")
                print(f"   å®Œæˆé›¶ä»¶: {best_kpi.get('mean_completed_parts', 0):.1f} / {target_parts} ({completion_rate:.1f}%)")
                print(f"   æ€»å®Œå·¥æ—¶é—´: {best_kpi.get('mean_makespan', 0):.1f} åˆ†é’Ÿ")
                print(f"   è®¾å¤‡åˆ©ç”¨ç‡: {best_kpi.get('mean_utilization', 0):.1%}")
                print(f"   è®¢å•å»¶æœŸæ—¶é—´: {best_kpi.get('mean_tardiness', 0):.1f} åˆ†é’Ÿ")
                print(f"   ç»¼åˆè¯„åˆ†: {self.best_score_foundation_phase:.3f}")

            # æ³›åŒ–å¼ºåŒ–é˜¶æ®µæœ€ä½³
            if self.best_episode_generalization_phase != -1:
                print("\n--- æ³›åŒ–å¼ºåŒ–é˜¶æ®µ ---")
                best_kpi = self.best_kpi_generalization_phase
                # æ³¨æ„ï¼šæ³›åŒ–é˜¶æ®µçš„ç›®æ ‡é›¶ä»¶æ•°æ˜¯åŠ¨æ€çš„ï¼Œæ­¤å¤„ä»…ä¸ºå‚è€ƒ
                print(f"   (åœ¨ç¬¬ {self.best_episode_generalization_phase} å›åˆå–å¾—)")
                print(f"   å®Œæˆé›¶ä»¶: {best_kpi.get('mean_completed_parts', 0):.1f}")
                print(f"   æ€»å®Œå·¥æ—¶é—´: {best_kpi.get('mean_makespan', 0):.1f} åˆ†é’Ÿ")
                print(f"   è®¾å¤‡åˆ©ç”¨ç‡: {best_kpi.get('mean_utilization', 0):.1%}")
                print(f"   è®¢å•å»¶æœŸæ—¶é—´: {best_kpi.get('mean_tardiness', 0):.1f} åˆ†é’Ÿ")
                print(f"   ç»¼åˆè¯„åˆ†: {self.best_score_generalization_phase:.3f}")
            
            # æ–°å¢ï¼šå¦‚æœå¯ç”¨äº†è¯¾ç¨‹å­¦ä¹ ï¼Œåˆ™å±•ç¤ºæ¯ä¸ªè¯¾ç¨‹é˜¶æ®µçš„æœ€ä½³åˆ†æ•°
            if curriculum_enabled:
                 print("\n--- è¯¾ç¨‹å­¦ä¹ å„é˜¶æ®µæœ€ä½³åˆ†æ•° ---")
                 for i, score in enumerate(stage_best_scores):
                     if score > -np.inf:
                         stage_name = curriculum_config["stages"][i]['name']
                         print(f"   é˜¶æ®µ '{stage_name}': {score:.3f}")
                     else:
                         stage_name = curriculum_config["stages"][i]['name']
                         print(f"   é˜¶æ®µ '{stage_name}': æœªè®°å½•æœ€ä½³åˆ†æ•°")


            # æœ€ç»ˆé»„é‡‘æ ‡å‡†ï¼šåŒè¾¾æ ‡æ¨¡å‹
            print("\n" + "="*40)
            print("â­ æœ€ç»ˆé»„é‡‘æ ‡å‡†æ¨¡å‹ (å®Œæˆæ‰€æœ‰é›¶ä»¶ä¸”å¾—åˆ†æœ€é«˜) â­")
            print("="*40)
            
            if self.best_episode_dual_objective != -1:
                best_kpi = self.best_kpi_dual_objective
                best_episode_to_report = self.best_episode_dual_objective
                
                # åœ¨åŒè¾¾æ ‡çš„æƒ…å†µä¸‹ï¼Œç›®æ ‡é›¶ä»¶æ•°æ˜¯ç¡®å®šçš„
                target_parts_final = get_total_parts_count()
                completion_rate_final = (best_kpi.get('mean_completed_parts', 0) / target_parts_final) * 100 if target_parts_final > 0 else 0
            
                print(f"   (åœ¨ç¬¬ {best_episode_to_report} å›åˆå–å¾—)") 
                print(f"   å®Œæˆé›¶ä»¶: {best_kpi.get('mean_completed_parts', 0):.1f} / {target_parts_final} ({completion_rate_final:.1f}%)")
                print(f"   æ€»å®Œå·¥æ—¶é—´: {best_kpi.get('mean_makespan', 0):.1f} åˆ†é’Ÿ")
                print(f"   è®¾å¤‡åˆ©ç”¨ç‡: {best_kpi.get('mean_utilization', 0):.1%}")
                print(f"   è®¢å•å»¶æœŸæ—¶é—´: {best_kpi.get('mean_tardiness', 0):.1f} åˆ†é’Ÿ")
                print(f"   ç»¼åˆè¯„åˆ†: {self.best_score_dual_objective:.3f}")
            else:
                print("   âš ï¸ æœ¬æ¬¡è®­ç»ƒæœªäº§ç”Ÿæ»¡è¶³'å®Œæˆæ‰€æœ‰é›¶ä»¶'æ¡ä»¶çš„æœ€ä½³æ¨¡å‹ã€‚")

            print("="*40)
            
            return {
                'training_time': total_training_time,
                'kpi_history': self.kpi_history,
                'iteration_times': self.iteration_times,
                'best_kpi': best_kpi
            }
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None
        
        finally:
            # ğŸ”§ V8 ä¼˜åŒ–: ä¸»å¾ªç¯ä¸­æ²¡æœ‰envéœ€è¦å…³é—­
            pass
    
    def save_model(self, filepath: str) -> str:
        """ä¿å­˜æ¨¡å‹å¹¶è¿”å›è·¯å¾„"""
        actor_path = f"{filepath}_actor.keras"
        try:
            self.shared_network.actor.save(actor_path)
            self.shared_network.critic.save(f"{filepath}_critic.keras")
            return actor_path
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return ""

    def _get_target_parts(self, curriculum_config: Optional[Dict]) -> int:
        """ç»Ÿä¸€è·å–å½“å‰å›åˆçš„ç›®æ ‡é›¶ä»¶æ•°"""
        if curriculum_config and 'custom_orders' in curriculum_config:
            # æ³›åŒ–é˜¶æ®µæˆ–è‡ªå®šä¹‰è®¢å•
            return get_total_parts_count(curriculum_config['custom_orders'])
        elif curriculum_config and 'orders_scale' in curriculum_config:
            # è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ
            base_parts = get_total_parts_count()
            return int(base_parts * curriculum_config['orders_scale'])
        else:
            # é»˜è®¤æˆ–åŸºç¡€è®­ç»ƒé˜¶æ®µ
            return get_total_parts_count()

    def check_curriculum_stage_graduation(self, kpi_results: Dict[str, float], current_score: float, stage_config: Dict[str, Any]) -> bool:
        """æ£€æŸ¥å½“å‰è¯¾ç¨‹å­¦ä¹ é˜¶æ®µæ˜¯å¦è¾¾åˆ°æ¯•ä¸šæ ‡å‡†"""
        criteria = stage_config.get("graduation_criteria")
        if not criteria:
            return False # å¦‚æœæ²¡æœ‰å®šä¹‰æ ‡å‡†ï¼Œåˆ™æ— æ³•æ¯•ä¸š

        # è·å–å½“å‰é˜¶æ®µçš„ç›®æ ‡é›¶ä»¶æ•°
        target_parts = int(get_total_parts_count() * stage_config.get('orders_scale', 1.0))
        completion_rate_kpi = (kpi_results.get('mean_completed_parts', 0) / target_parts) * 100 if target_parts > 0 else 0
        
        target_score = criteria["target_score"]
        stability_goal = criteria["target_consistency"]
        min_completion_rate = criteria["min_completion_rate"]
        # æ–°å¢ï¼šå¤„ç†å»¶æœŸé˜ˆå€¼
        tardiness_threshold = criteria.get("tardiness_threshold")
        current_tardiness = kpi_results.get('mean_tardiness', float('inf'))

        conditions_met = {
            f"å®Œæˆç‡(>={min_completion_rate}%)": completion_rate_kpi >= min_completion_rate,
            f"åˆ†æ•°(>={target_score})": current_score >= target_score,
        }
        
        if tardiness_threshold is not None:
            conditions_met[f"å»¶æœŸ(<={tardiness_threshold}min)"] = current_tardiness <= tardiness_threshold

        if all(conditions_met.values()):
            self.curriculum_stage_achievement_count += 1
            print(f"[CURRICULUM] é˜¶æ®µ '{stage_config['name']}' è¾¾æ ‡: å®Œæˆç‡ {completion_rate_kpi:.1f}%, åˆ†æ•° {current_score:.3f} (è¿ç»­ç¬¬{self.curriculum_stage_achievement_count}/{stability_goal}æ¬¡)")
        else:
            if self.curriculum_stage_achievement_count > 0:
                reasons = [k for k, v in conditions_met.items() if not v]
                print(f"[CURRICULUM] é˜¶æ®µ '{stage_config['name']}' è¿ç»­è¾¾æ ‡ä¸­æ–­. æœªè¾¾æ ‡é¡¹: {', '.join(reasons)}")
            self.curriculum_stage_achievement_count = 0

        return self.curriculum_stage_achievement_count >= stability_goal

def main():
    
    print(f"âœ¨ è®­ç»ƒè¿›ç¨‹PID: {os.getpid()}")

    # è®¾ç½®éšæœºç§å­
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    tf.random.set_seed(RANDOM_SEED)
    
    try:
        # æ ¸å¿ƒé‡æ„ï¼šä»TRAINING_FLOW_CONFIGè·å–è®­ç»ƒå‚æ•°
        max_episodes = TRAINING_FLOW_CONFIG["general_params"]["max_episodes"]
        steps_per_episode = TRAINING_FLOW_CONFIG["general_params"]["steps_per_episode"]
        eval_frequency = TRAINING_FLOW_CONFIG["general_params"]["eval_frequency"]
        
        # è®­ç»ƒç›®æ ‡ç°åœ¨åˆ†æ•£åœ¨TRAINING_FLOW_CONFIGä¸­ï¼Œä¸å†éœ€è¦ç‹¬ç«‹çš„training_targetså­—å…¸
        

        print("=" * 80)
        foundation_criteria = TRAINING_FLOW_CONFIG["foundation_phase"]["graduation_criteria"]
        generalization_criteria = TRAINING_FLOW_CONFIG["generalization_phase"]["completion_criteria"]
        
        print(f"ğŸ¯ åŸºç¡€è®­ç»ƒç›®æ ‡: ç»¼åˆè¯„åˆ† > {foundation_criteria['target_score']:.2f}, "
              f"å®Œæˆç‡ > {foundation_criteria['min_completion_rate']:.0f}%, "
              f"å»¶æœŸ < {foundation_criteria['tardiness_threshold']:.0f}min, "
              f"è¿ç»­{foundation_criteria['target_consistency']}æ¬¡")
              
        print(f"ğŸ¯ æ³›åŒ–è®­ç»ƒç›®æ ‡: ç»¼åˆè¯„åˆ† > {generalization_criteria['target_score']:.2f}, "
              f"å®Œæˆç‡ > {generalization_criteria['min_completion_rate']:.0f}%, "
              f"è¿ç»­{generalization_criteria['target_consistency']}æ¬¡")

        print(f"ğŸ“Š è½®æ•°ä¸Šé™: {max_episodes}è½®")
        print("=" * 80)
        print("ğŸ”§ æ ¸å¿ƒé…ç½®:")
        print("  å·¥ä½œç«™:")
        for station, config in WORKSTATIONS.items():
            print(f"    - {station}: æ•°é‡={config['count']}, å®¹é‡={config['capacity']}")

        print("  å¥–åŠ±ç³»ç»Ÿ:")
        for key, value in REWARD_CONFIG.items():
            print(f"    - {key}: {value}")
        
        cl_config = TRAINING_FLOW_CONFIG["foundation_phase"]["curriculum_learning"]
        
        print("  å¯ç”¨/ç¦ç”¨æ¨¡å—:")
        print(f"    - è¯¾ç¨‹å­¦ä¹ : {'å¯ç”¨' if cl_config.get('enabled', False) else 'ç¦ç”¨'}")
        print(f"    - è®¾å¤‡æ•…éšœ: {'å¯ç”¨' if EQUIPMENT_FAILURE.get('enabled', False) else 'ç¦ç”¨'}")
        print(f"    - ç´§æ€¥æ’å•: {'å¯ç”¨' if EMERGENCY_ORDERS.get('enabled', False) else 'ç¦ç”¨'}")
        print("-" * 40)
        
        trainer = SimplePPOTrainer(
            initial_lr=LEARNING_RATE_CONFIG["initial_lr"],
            total_train_episodes=max_episodes,
            steps_per_episode=steps_per_episode,
            training_targets=None  # ä¸å†éœ€è¦ï¼Œç”±å†…éƒ¨è¯»å–é…ç½®æ–‡ä»¶
        )
        
        # ğŸ”§ V31 å¯åŠ¨è‡ªé€‚åº”è®­ç»ƒï¼šç³»ç»Ÿå°†æ ¹æ®æ€§èƒ½è‡ªåŠ¨å†³å®šä½•æ—¶åœæ­¢
        results = trainer.train(
            max_episodes=max_episodes,
            steps_per_episode=steps_per_episode,
            eval_frequency=eval_frequency,
            adaptive_mode=True
        )
        
        if results:
            print("\nğŸ‰ è‡ªé€‚åº”è®­ç»ƒæˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“Š å®é™…è®­ç»ƒè½®æ•°: {len(trainer.iteration_times)}")
            final_completion_rate = (results['best_kpi'].get('mean_completed_parts', 0) / get_total_parts_count()) * 100 if get_total_parts_count() > 0 else 0
            print(f"ğŸ¯ æœ€ç»ˆç›®æ ‡è¾¾æˆ: {trainer.adaptive_state['target_achieved_count']}æ¬¡è¿ç»­è¾¾æ ‡ (åŸºäºæœ€ç»ˆé˜¶æ®µåˆ†æ•°)")
            
            best_episode_final = trainer.best_episode_dual_objective if trainer.best_episode_dual_objective != -1 else trainer.final_stage_best_episode
            print(f"ğŸ“ˆ å†å²æœ€ä½³æ€§èƒ½ (åŒé‡æ ‡å‡†ï¼Œç¬¬ {best_episode_final} å›åˆ): {final_completion_rate:.1f}% ({results['best_kpi'].get('mean_completed_parts', 0):.1f}ä¸ªé›¶ä»¶)")
        else:
            print("\nâŒ è®­ç»ƒå¤±è´¥")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # ğŸ”§ V10 å…³é”®ä¿®å¤: è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•ä¸º'spawn'ï¼Œé¿å…TensorFlowçš„forkä¸å®‰å…¨é—®é¢˜
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    main()