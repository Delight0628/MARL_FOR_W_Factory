"""
MAPPOè®­ç»ƒå™¨æ¨¡å—
==================
å®ç°SimplePPOTrainerç±»ï¼Œç®¡ç†å®Œæ•´çš„è®­ç»ƒæµç¨‹

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä¸¤é˜¶æ®µæ¸è¿›å¼è®­ç»ƒï¼ˆåŸºç¡€æ³›åŒ– â†’ åŠ¨æ€äº‹ä»¶å¼ºåŒ–ï¼‰
2. è¯¾ç¨‹å­¦ä¹ æ”¯æŒï¼ˆé€æ­¥å¢åŠ ä»»åŠ¡éš¾åº¦ï¼‰
3. è‡ªé€‚åº”ç†µè°ƒæ•´ï¼ˆæ¢ç´¢-åˆ©ç”¨å¹³è¡¡ï¼‰
4. åˆ†é˜¶æ®µæ¨¡å‹ä¿å­˜ï¼ˆåŒè¾¾æ ‡æœ€ä½³æ¨¡å‹è·Ÿè¸ªï¼‰
5. TensorBoardå®æ—¶ç›‘æ§
6. å¹¶è¡Œç»éªŒé‡‡é›†ï¼ˆProcessPoolExecutorï¼‰
"""

import os
import sys
import time
import random
import socket
import numpy as np
import tensorflow as tf
import gymnasium as gym
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor

# æ·»åŠ ç¯å¢ƒè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from environments.w_factory_env import make_parallel_env
from environments.w_factory_config import *
from mappo.ppo_buffer import ExperienceBuffer
from mappo.ppo_network import PPONetwork
from mappo.ppo_worker import _collect_experience_wrapper

TENSORBOARD_AVAILABLE = hasattr(tf.summary, "create_file_writer")


class RunningMeanStd:
    """
    ğŸ”§ æ–°å¢ï¼šè¿è¡Œå‡å€¼å’Œæ ‡å‡†å·®å½’ä¸€åŒ–å™¨
    ç”¨äºå¯¹è§‚æµ‹ã€å›æŠ¥ç­‰è¿›è¡Œåœ¨çº¿å½’ä¸€åŒ–ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§
    """
    def __init__(self, shape: tuple, epsilon: float = 1e-8):
        self.shape = shape
        self.epsilon = epsilon
        self.mean = np.zeros(shape, dtype=np.float32)
        self.var = np.ones(shape, dtype=np.float32)
        self.count = 0
    
    def update(self, x: np.ndarray):
        """æ›´æ–°ç»Ÿè®¡é‡"""
        batch_mean = np.mean(x, axis=0)
        batch_var = np.var(x, axis=0)
        batch_count = x.shape[0]
        
        # å¢é‡æ›´æ–°
        delta = batch_mean - self.mean
        total_count = self.count + batch_count
        
        self.mean += delta * batch_count / max(total_count, 1)
        m_a = self.var * self.count
        m_b = batch_var * batch_count
        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / max(total_count, 1)
        self.var = M2 / max(total_count, 1)
        self.count = total_count
    
    def normalize(self, x: np.ndarray) -> np.ndarray:
        """å½’ä¸€åŒ–è¾“å…¥"""
        return (x - self.mean) / np.sqrt(self.var + self.epsilon)


class SimplePPOTrainer:
    """
    MAPPOè‡ªé€‚åº”è®­ç»ƒå™¨
    
    å®ç°ä¸¤é˜¶æ®µæ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼š
    
    ã€é˜¶æ®µä¸€ã€‘åŸºç¡€èƒ½åŠ›è®­ç»ƒï¼ˆéšæœºè®¢å•æ³›åŒ–ï¼‰
    - ç›®æ ‡ï¼šå­¦ä¹ åŸºæœ¬è°ƒåº¦èƒ½åŠ›
    - æ–¹æ³•ï¼šéšæœºè®¢å• + å¤šä»»åŠ¡æ··åˆï¼ˆå«BASE_ORDERSé”šç‚¹ï¼‰
    - æ¯•ä¸šæ ‡å‡†ï¼šè¿ç»­Næ¬¡è¾¾åˆ°åˆ†æ•°/å®Œæˆç‡/å»¶æœŸé˜ˆå€¼
    
    ã€é˜¶æ®µäºŒã€‘åŠ¨æ€äº‹ä»¶é²æ£’æ€§è®­ç»ƒ
    - ç›®æ ‡ï¼šæå‡åº”å¯¹çªå‘äº‹ä»¶çš„èƒ½åŠ›
    - æ–¹æ³•ï¼šéšæœºè®¢å• + è®¾å¤‡æ•…éšœ + ç´§æ€¥æ’å•
    - å®Œæˆæ ‡å‡†ï¼šè¿ç»­Mæ¬¡è¾¾åˆ°æ›´é«˜çš„æ€§èƒ½æŒ‡æ ‡
    
    è‡ªé€‚åº”æœºåˆ¶ï¼š
    - å­¦ä¹ ç‡è¡°å‡ï¼šPolynomialDecay
    - ç†µç³»æ•°è°ƒæ•´ï¼šæ ¹æ®åœæ»ç­‰çº§é˜¶æ¢¯å¼æå‡/è¡°å‡
    - è¯¾ç¨‹å­¦ä¹ ï¼šé€æ­¥å¢åŠ è®¢å•æ•°é‡å’Œæ—¶é—´å‹åŠ›
    """
    
    def __init__(self, initial_lr: float, total_train_episodes: int, steps_per_episode: int, training_targets: dict = None, models_root_dir: Optional[str] = None, logs_root_dir: Optional[str] = None):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„ç³»ç»Ÿèµ„æºé…ç½®
        self.num_workers = SYSTEM_CONFIG["num_parallel_workers"]
        print(f"ä½¿ç”¨ {self.num_workers} ä¸ªå¹¶è¡Œç¯å¢ƒè¿›è¡Œæ•°æ®é‡‡é›†")
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„TensorFlowçº¿ç¨‹é…ç½®
        tf.config.threading.set_inter_op_parallelism_threads(SYSTEM_CONFIG["tf_inter_op_threads"])
        tf.config.threading.set_intra_op_parallelism_threads(SYSTEM_CONFIG["tf_intra_op_threads"])
        print(f"TensorFlowå°†ä½¿ç”¨ {SYSTEM_CONFIG['tf_inter_op_threads']}ä¸ªinterçº¿ç¨‹, {SYSTEM_CONFIG['tf_intra_op_threads']}ä¸ªintraçº¿ç¨‹")
        
        # ç¯å¢ƒæ¢æµ‹
        # ä¹‹å‰çš„ä»£ç ä¾èµ–åŠ¨æ€é…ç½®ï¼Œç°åœ¨æˆ‘ä»¬ç›´æ¥åˆ›å»º
        temp_env = make_parallel_env()
        self.state_dim = temp_env.observation_space(temp_env.possible_agents[0]).shape[0]
        # ç›´æ¥ä½¿ç”¨ç¯å¢ƒçš„åŠ¨ä½œç©ºé—´å¯¹è±¡ä»¥æ”¯æŒ MultiDiscrete
        self.action_space = temp_env.action_space(temp_env.possible_agents[0])
        self.agent_ids = temp_env.possible_agents
        self.num_agents = len(self.agent_ids)
        # Criticæ™ºèƒ½ä½“æ¡ä»¶åŒ–ï¼šå°†æ™ºèƒ½ä½“one-hotå¹¶å…¥å…¨å±€çŠ¶æ€è¾“å…¥ç»´åº¦
        self.global_state_dim = temp_env.global_state_space.shape[0] + self.num_agents
        temp_env.close()
        
        print("ç¯å¢ƒç©ºé—´æ£€æµ‹:")
        print(f"   è§‚æµ‹ç»´åº¦: {self.state_dim}")
        print(f"   åŠ¨ä½œç©ºé—´: {self.action_space}")
        print(f"   æ™ºèƒ½ä½“æ•°é‡: {len(self.agent_ids)}")
        print(f"   å…¨å±€çŠ¶æ€ç»´åº¦(å«agent one-hot): {self.global_state_dim}")
        
        # ç§»é™¤åŠ¨æ€å‚æ•°è°ƒæ•´
        optimized_episodes = total_train_episodes
        optimized_steps = steps_per_episode
        # è¯„ä¼°æ­¥é•¿ä¸é‡‡é›†æ­¥é•¿å¯¹é½ï¼Œé¿å…è®­ç»ƒ/è¯„ä¼°ä¸ä¸€è‡´
        self.max_steps_for_eval = optimized_steps
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„å­¦ä¹ ç‡è°ƒåº¦é…ç½®
        self.lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
            initial_learning_rate=LEARNING_RATE_CONFIG["initial_lr"],
            decay_steps=optimized_episodes * optimized_steps,
            end_learning_rate=LEARNING_RATE_CONFIG["end_lr"],
            power=LEARNING_RATE_CONFIG["decay_power"]
        )

        # å…±äº«ç½‘ç»œï¼ˆä¼ é€’åŠ¨ä½œç©ºé—´å¯¹è±¡ï¼Œæ”¯æŒMultiDiscreteï¼‰
        self.shared_network = PPONetwork(
            state_dim=self.state_dim,
            action_space=self.action_space,
            lr=self.lr_schedule,
            global_state_dim=self.global_state_dim
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.training_losses = []
        self.iteration_times = []  # è®°å½•æ¯è½®è®­ç»ƒæ—¶é—´
        self.kpi_history = []      # è®°å½•æ¯è½®KPIå†å²
        self.initial_lr = initial_lr  # ä¿å­˜åˆå§‹å­¦ä¹ ç‡
        self.start_time = time.time()
        self.start_time_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        

        self.final_stage_best_kpi = {
            'mean_completed_parts': -1.0,
            'mean_makespan': float('inf'),
            'mean_utilization': 0.0,
            'mean_tardiness': float('inf')
        }
        self.final_stage_best_score = float('-inf')
        self.final_stage_best_episode = -1 # è®°å½•æœ€ä½³KPIçš„å›åˆæ•°
        
        # "åŒè¾¾æ ‡"æœ€ä½³KPIè·Ÿè¸ªå™¨
        self.best_kpi_dual_objective = {
            'mean_completed_parts': -1.0,
            'mean_makespan': float('inf'),
            'mean_utilization': 0.0,
            'mean_tardiness': float('inf')
        }
        self.best_score_dual_objective = float('-inf')
        self.best_episode_dual_objective = -1

        # è®­ç»ƒæµç¨‹ç”±é…ç½®æ–‡ä»¶é©±åŠ¨
        self.training_flow_config = TRAINING_FLOW_CONFIG
        self.training_targets = self.training_flow_config["general_params"] # é€šç”¨å‚æ•°
        
        # è‡ªé€‚åº”è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.adaptive_state = {
            "target_achieved_count": 0,          # è¿ç»­è¾¾åˆ°ç›®æ ‡çš„æ¬¡æ•°
            "best_performance": 0.0,             # å†å²æœ€ä½³æ€§èƒ½
            "last_improvement_episode": 0,       # ä¸Šæ¬¡æ”¹è¿›çš„è½®æ•°
            "performance_history": [],           # æ€§èƒ½å†å²è®°å½•
            "training_phase": "exploration",     # å½“å‰è®­ç»ƒé˜¶æ®µï¼šexploration, exploitation, fine_tuning
            "stagnation_counter": 0,             # åœæ»è®¡æ•°å™¨
            "last_stagnation_performance": -1.0, # ä¸Šä¸€æ¬¡åœæ»æ—¶çš„æ€§èƒ½
        }
        # --- æ–¹æ¡ˆäºŒï¼šå‡çº§è‡ªé€‚åº”ç†µæ‰€éœ€å˜é‡ ---
        self.epochs_without_improvement = 0
        self.stagnation_level = 0  # æ–°å¢ï¼šåœæ»ç­‰çº§ï¼Œç”¨äºé˜¶æ¢¯å¼æå‡ç†µ
        
        # --- æ–°å¢ï¼šåŸºç¡€è®­ç»ƒ + éšæœºé¢†åŸŸå¼ºåŒ– é˜¶æ®µç®¡ç† ---
        self.foundation_training_completed = False  # åŸºç¡€è®­ç»ƒæ˜¯å¦å®Œæˆ
        self.generalization_phase_active = False   # æ˜¯å¦è¿›å…¥æ³›åŒ–å¼ºåŒ–é˜¶æ®µ
        self.foundation_achievement_count = 0      # åŸºç¡€è®­ç»ƒè¿ç»­è¾¾æ ‡æ¬¡æ•°
        self.generalization_achievement_count = 0  # æ³›åŒ–é˜¶æ®µè¿ç»­è¾¾æ ‡æ¬¡æ•°
        
        # --- æ–°å¢ï¼šä¸ºæ–°ä¸¤é˜¶æ®µæ–¹æ¡ˆçš„ç‹¬ç«‹æ¨¡å‹ä¿å­˜è¿½è¸ª ---
        self.best_score_foundation_phase = float('-inf')    # åŸºç¡€è®­ç»ƒé˜¶æ®µæœ€ä½³åˆ†æ•°
        self.best_kpi_foundation_phase = {}         # åŸºç¡€è®­ç»ƒé˜¶æ®µæœ€ä½³KPI
        self.best_episode_foundation_phase = -1    # åŸºç¡€è®­ç»ƒé˜¶æ®µæœ€ä½³å›åˆ
        
        self.best_score_generalization_phase = float('-inf')  # æ³›åŒ–å¼ºåŒ–é˜¶æ®µæœ€ä½³åˆ†æ•°
        self.best_kpi_generalization_phase = {}       # æ³›åŒ–å¼ºåŒ–é˜¶æ®µæœ€ä½³KPI
        self.best_episode_generalization_phase = -1  # æ³›åŒ–å¼ºåŒ–é˜¶æ®µæœ€ä½³å›åˆ
        
        # --- æ–°å¢ï¼šè¯¾ç¨‹å­¦ä¹ é˜¶æ®µçš„è‡ªé€‚åº”æ¯•ä¸šè·Ÿè¸ªå™¨ ---
        self.curriculum_stage_achievement_count = 0
        
        # åˆå§‹åŒ–åŠ¨æ€è®­ç»ƒå‚æ•°
        self.current_entropy_coeff = PPO_NETWORK_CONFIG["entropy_coeff"] # åˆå§‹åŒ–åŠ¨æ€ç†µç³»æ•°
        self.current_learning_rate = LEARNING_RATE_CONFIG["initial_lr"] # ä½¿ç”¨æ­£ç¡®çš„å­¦ä¹ ç‡é…ç½®
        
        # ç†µç³»æ•°é€€ç«è®¡åˆ’ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        self.entropy_decay_rate = 0.9995  # ğŸ”§ æ›´æ…¢çš„è¡°å‡ç‡ï¼Œä¿æŒæ›´é•¿æ—¶é—´çš„æ¢ç´¢
        self.min_entropy_coeff = 0.05     # ğŸ”§ æ›´é«˜çš„æœ€å°ç†µç³»æ•°ï¼Œé¿å…è¿‡æ—©æ”¶æ•›
        
        
        # å›åˆäº‹ä»¶æ—¥å¿—è®°å½•å™¨
        self.episode_events = []
        
        # åˆ›å»ºä¿å­˜ç›®å½• (ä»¥è®­ç»ƒå¼€å§‹æ—¶é—´åˆ›å»ºä¸“ç”¨æ–‡ä»¶å¤¹)
        self.base_models_dir = models_root_dir if models_root_dir else "mappo/ppo_models"
        self.models_dir = f"{self.base_models_dir}/{self.start_time_str}"
        os.makedirs(self.models_dir, exist_ok=True)
        print(f"æ¨¡å‹ä¿å­˜ç›®å½•: {self.models_dir}")
        
        # TensorBoardæ”¯æŒ
        self.tensorboard_dir = (
            os.path.join(logs_root_dir, "tensorboard_logs", self.timestamp)
            if logs_root_dir else f"mappo/tensorboard_logs/{self.timestamp}"
        )
        os.makedirs(self.tensorboard_dir, exist_ok=True)
        if TENSORBOARD_AVAILABLE:
            self.train_writer = None
            self.current_tensorboard_run_name = None
            # ä¸ºæœ¬æ¬¡è¿è¡Œåˆ†é…å”¯ä¸€ç«¯å£
            try:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.bind(("127.0.0.1", 0))
                self.tensorboard_port = sock.getsockname()[1]
                sock.close()
            except Exception:
                # å›é€€åˆ°å¸¸è§ç«¯å£èŒƒå›´å†…çš„ä¼ªéšæœºç«¯å£
                self.tensorboard_port = 6006 + (hash(self.timestamp) % 1000)
            print(f"ğŸ“Š TensorBoardå‘½ä»¤: tensorboard --logdir=\"{self.tensorboard_dir}\" --port={self.tensorboard_port}")
        else:
            self.train_writer = None
            print("âš ï¸  TensorBoardä¸å¯ç”¨")
        
        # 10-22-10-52 ä¿®å¤ï¼šåˆ‡æ¢åˆ°è¿›ç¨‹æ± ï¼Œå½»åº•è§£å†³TensorFlowæƒé‡å†²çªé—®é¢˜
        # è¯´æ˜ï¼šä½¿ç”¨è¿›ç¨‹æ± ç¡®ä¿æ¯ä¸ªworkeråœ¨å®Œå…¨ç‹¬ç«‹çš„Pythonè¿›ç¨‹ä¸­è¿è¡Œ
        # ä¼˜ç‚¹ï¼šå®Œå…¨éš”ç¦»ï¼Œé¿å…TensorFlowå˜é‡åå†²çªå’Œæƒé‡åŠ è½½é—®é¢˜
        try:
            import multiprocessing as _mp
            _mp.set_start_method('spawn', force=True)
        except Exception:
            pass
        self.pool = ProcessPoolExecutor(max_workers=self.num_workers)

        # ğŸ”§ åˆå§‹åŒ–è®­ç»ƒæ‰€éœ€çš„å…³é”®æˆå‘˜
        self.seed = RANDOM_SEED
        self.total_steps = 0
        self.network_config = PPO_NETWORK_CONFIG
        
        # ğŸ”§ æ–°å¢ï¼šå½’ä¸€åŒ–å™¨ï¼ˆè§‚æµ‹ã€å›æŠ¥å½’ä¸€åŒ–ï¼Œä¼˜åŠ¿ç™½åŒ–ï¼‰
        self.obs_normalizer = RunningMeanStd(shape=(self.state_dim,))
        self.global_obs_normalizer = RunningMeanStd(shape=(self.global_state_dim,))
        self.reward_normalizer = RunningMeanStd(shape=(1,))
        self.normalize_obs = True
        self.normalize_rewards = True
        self.normalize_advantages = True  # ä¼˜åŠ¿ç™½åŒ–ï¼ˆå·²åœ¨bufferä¸­å®ç°ï¼Œè¿™é‡Œåªæ˜¯æ ‡å¿—ï¼‰
        
        # 10-23-18-00 æ ¸å¿ƒæ”¹è¿›ï¼šå¤šä»»åŠ¡æ··åˆæœºåˆ¶è´¯ç©¿ä¸¤ä¸ªé˜¶æ®µ
        # ä»foundation_phaseå’Œgeneralization_phaseåˆ†åˆ«è¯»å–é…ç½®
        # ä¸¤é˜¶æ®µéƒ½ä½¿ç”¨25% BASE_ORDERS workerä½œä¸ºç¨³å®šé”šç‚¹
        self.foundation_multi_task_config = TRAINING_FLOW_CONFIG["foundation_phase"].get("multi_task_mixing", {"enabled": False, "base_worker_fraction": 0.0, "randomize_base_env": False})
        self.generalization_multi_task_config = TRAINING_FLOW_CONFIG["generalization_phase"].get("multi_task_mixing", {"enabled": False, "base_worker_fraction": 0.0, "randomize_base_env": False})
        
        # è®¡ç®—BASE_ORDERS workeræ•°é‡ï¼ˆä¸¤é˜¶æ®µä½¿ç”¨ç›¸åŒé…ç½®ï¼‰
        base_fraction = float(self.foundation_multi_task_config.get("base_worker_fraction", 0.0))
        base_fraction = min(max(base_fraction, 0.0), 1.0)
 
    
    def should_continue_training(self, episode: int, current_score: float, completion_rate: float) -> tuple:
        """åŸºäºè®­ç»ƒæµç¨‹é…ç½®çš„é˜¶æ®µæ ‡å‡†è¯„ä¼°æ˜¯å¦ç»§ç»­è®­ç»ƒ"""
        general = self.training_flow_config["general_params"]
        state = self.adaptive_state

        # åŸºæœ¬é™åˆ¶æ£€æŸ¥
        if episode >= general["max_episodes"]:
            return False, f"å·²è¾¾åˆ°æœ€å¤§è®­ç»ƒè½®æ•°({general['max_episodes']})", 0

        # æŒ‰é˜¶æ®µé€‰æ‹©æ ‡å‡†
        if self.generalization_phase_active:
            criteria = self.training_flow_config["generalization_phase"]["completion_criteria"]
        else:
            criteria = self.training_flow_config["foundation_phase"]["graduation_criteria"]

        target_score = criteria["target_score"]
        min_completion_rate = criteria.get("min_completion_rate", 100.0)
        target_consistency = criteria["target_consistency"]

        # è¾¾æ ‡è®¡æ•°é€»è¾‘
        if completion_rate >= min_completion_rate and current_score >= target_score:
            state["target_achieved_count"] += 1
            print(f"ğŸ¯ è¾¾æ ‡: å®Œæˆç‡ {completion_rate:.1f}% & åˆ†æ•° {current_score:.3f} (è¿ç»­ç¬¬{state['target_achieved_count']}/{target_consistency}æ¬¡)")
            if state["target_achieved_count"] >= target_consistency:
                return False, f"è¿ç»­{target_consistency}æ¬¡è¾¾åˆ°é˜¶æ®µæ ‡å‡†", 0
        else:
            state["target_achieved_count"] = 0

        # æ—©åœé€»è¾‘ï¼ˆåŸºäºåˆ†æ•°åœæ»ï¼‰
        state["performance_history"].append(current_score)
        if len(state["performance_history"]) > general["performance_window"]:
            state["performance_history"].pop(0)

        if current_score > state["best_performance"]:
            state["best_performance"] = current_score
            state["last_improvement_episode"] = episode

        improvement_gap = episode - state["last_improvement_episode"]
        if improvement_gap >= general["early_stop_patience"]:
            if len(state["performance_history"]) >= general["performance_window"]:
                recent_avg_score = sum(state["performance_history"]) / len(state["performance_history"])
                if recent_avg_score < target_score * 0.8:
                    return False, f"è¿ç»­{improvement_gap}è½®æ— æ”¹è¿›ï¼Œä¸”å¹³å‡åˆ†æ•°ä½äº{target_score*0.8:.3f}", 0

        return True, f"å½“å‰åˆ†æ•° {current_score:.3f}, å®Œæˆç‡ {completion_rate:.1f}%", 0
    
    def check_foundation_training_completion(self, kpi_results: Dict[str, float], current_score: float, curriculum_config: Optional[Dict] = None) -> bool:
        """
        æ£€æŸ¥åŸºç¡€è®­ç»ƒæ˜¯å¦è¾¾åˆ°æ¯•ä¸šæ ‡å‡†ï¼Œç”±é…ç½®æ–‡ä»¶é©±åŠ¨
        
        10-27-17-30 ä¿®å¤ï¼šä½¿ç”¨å®é™…è¯„ä¼°è®¢å•çš„ç›®æ ‡é›¶ä»¶æ•°ï¼Œè€Œéå›ºå®šçš„BASE_ORDERSï¼Œç¡®ä¿å®Œæˆç‡è®¡ç®—ä¸å®é™…ä»»åŠ¡ä¸€è‡´
        """
        criteria = self.training_flow_config["foundation_phase"]["graduation_criteria"]
        
        # 10-27-17-30 ä¿®å¤ï¼šä½¿ç”¨_get_target_partsè·å–æ­£ç¡®çš„ç›®æ ‡é›¶ä»¶æ•°ï¼ˆä¸å®é™…è¯„ä¼°è®¢å•ä¸€è‡´ï¼‰
        total_parts_target = self._get_target_parts(curriculum_config)
        completion_rate_kpi = (kpi_results.get('mean_completed_parts', 0) / total_parts_target) * 100 if total_parts_target > 0 else 0
        # ğŸ”§ ä¸Šé™è£å‰ªï¼Œé¿å…å› åŠ¨æ€æ’å•å¯¼è‡´>100%çš„æ˜¾ç¤ºä¸åˆ¤å®š
        completion_rate_kpi = float(min(100.0, completion_rate_kpi))
        
        target_score = criteria["target_score"]
        stability_goal = criteria["target_consistency"]
        tardiness_threshold = criteria["tardiness_threshold"]
        min_completion_rate = criteria["min_completion_rate"]
        current_tardiness = kpi_results.get('mean_tardiness', float('inf'))

        conditions_met = {
            f"å®Œæˆç‡è¾¾æ ‡(>={min_completion_rate}%)": completion_rate_kpi >= min_completion_rate,
            f"åˆ†æ•°è¾¾æ ‡(>={target_score})": current_score >= target_score,
            f"å»¶æœŸè¾¾æ ‡(<={tardiness_threshold}min)": current_tardiness <= tardiness_threshold
        }

        if all(conditions_met.values()):
            self.foundation_achievement_count += 1
            print(f"ğŸ¯ åŸºç¡€è®­ç»ƒè¾¾æ ‡: å®Œæˆç‡ {completion_rate_kpi:.1f}%, åˆ†æ•° {current_score:.3f}, å»¶æœŸ {current_tardiness:.1f}min (è¿ç»­ç¬¬{self.foundation_achievement_count}/{stability_goal}æ¬¡)")
        else:
            if self.foundation_achievement_count > 0:
                reasons = [k for k, v in conditions_met.items() if not v]
                print(f"âŒ åŸºç¡€è®­ç»ƒè¿ç»­è¾¾æ ‡ä¸­æ–­. æœªè¾¾æ ‡é¡¹: {', '.join(reasons)}")
            self.foundation_achievement_count = 0

        if self.foundation_achievement_count >= stability_goal:
            print(f"ğŸ† åŸºç¡€è®­ç»ƒå®Œæˆï¼è¿ç»­{stability_goal}æ¬¡è¾¾åˆ°æ‰€æœ‰æ ‡å‡†ï¼Œå‡†å¤‡è¿›å…¥æ³›åŒ–å¼ºåŒ–é˜¶æ®µã€‚")
            return True
        return False
    
    def check_generalization_training_completion(self, current_score: float, completion_rate: float) -> bool:
        """æ£€æŸ¥æ³›åŒ–è®­ç»ƒæ˜¯å¦å·²è¾¾åˆ°æœ€ç»ˆè®­ç»ƒå®Œæˆçš„æ¡ä»¶ï¼Œç”±é…ç½®æ–‡ä»¶é©±åŠ¨"""
        criteria = self.training_flow_config["generalization_phase"]["completion_criteria"]
        
        target_score = criteria["target_score"]
        stability_goal = criteria["target_consistency"]
        min_completion_rate = criteria["min_completion_rate"]
        
        if completion_rate >= min_completion_rate and current_score >= target_score:
            self.generalization_achievement_count += 1
            print(f"ğŸŒŸ æ³›åŒ–é˜¶æ®µè¾¾æ ‡: å®Œæˆç‡ {completion_rate:.1f}% & åˆ†æ•° {current_score:.3f} (è¿ç»­ç¬¬{self.generalization_achievement_count}/{stability_goal}æ¬¡)")
            
            if self.generalization_achievement_count >= stability_goal:
                print(f"ğŸ‰ æ³›åŒ–è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²å…·å¤‡ä¼˜ç§€çš„æ³›åŒ–èƒ½åŠ›ã€‚")
                return True
        else:
            self.generalization_achievement_count = 0
        
        return False
    
    def create_environment(self, curriculum_stage=None):
        """åˆ›å»ºç¯å¢ƒï¼ˆæ”¯æŒè¯¾ç¨‹å­¦ä¹ ï¼‰"""
        config = {}
        
        # ğŸ”§ V16ï¼šå®ç°è¯¾ç¨‹å­¦ä¹ çš„ç¯å¢ƒé…ç½®
        # æ ¸å¿ƒé‡æ„ï¼šè¯¾ç¨‹å­¦ä¹ é€»è¾‘ç°åœ¨ç”± TRAINING_FLOW_CONFIG æ§åˆ¶
        cl_config = self.training_flow_config["foundation_phase"]["curriculum_learning"]
        if curriculum_stage is not None and cl_config["enabled"]:
            stages = cl_config["stages"]
            stage = stages[curriculum_stage] if curriculum_stage < len(stages) else stages[-1]
            config['curriculum_stage'] = stage
            config['orders_scale'] = stage.get('orders_scale', 1.0)
            config['time_scale'] = stage.get('time_scale', 1.0)
            print(f"ğŸ“š è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ {curriculum_stage+1}: {stage['name']} (è®¢å•æ¯”ä¾‹: {stage['orders_scale']}, æ—¶é—´å€æ•°: {stage['time_scale']})")
        
        # ç»Ÿä¸€æ³¨å…¥ MAX_SIM_STEPS
        config['MAX_SIM_STEPS'] = self.max_steps_for_eval
        env = make_parallel_env(config)
        buffers = {
            agent: ExperienceBuffer() 
            for agent in env.possible_agents
        }
        return env, buffers
    
    def collect_and_process_experience(self, num_steps: int, curriculum_config: Dict[str, Any] = None) -> Tuple[float, Optional[Dict[str, np.ndarray]]]:
        """
        å¹¶è¡Œé‡‡é›†ç»éªŒå¹¶è®¡ç®—GAEä¼˜åŠ¿å‡½æ•°
        
        æ ¸å¿ƒè®¾è®¡ï¼šç»Ÿä¸€MDPä¿è¯è®­ç»ƒç¨³å®šæ€§
        - åŒä¸€å›åˆæ‰€æœ‰workerä½¿ç”¨ç›¸åŒè®¢å•é…ç½®ï¼ˆé¿å…æ¢¯åº¦å†²çªï¼‰
        - å›åˆé—´è½®æ¢ä»»åŠ¡ç±»å‹ï¼ˆBASE_ORDERS vs éšæœºè®¢å•ï¼‰
        - æ ¹æ®è®­ç»ƒé˜¶æ®µå¯ç”¨åŠ¨æ€äº‹ä»¶ï¼ˆè®¾å¤‡æ•…éšœã€ç´§æ€¥æ’å•ï¼‰
        
        æ‰§è¡Œæµç¨‹ï¼š
        1. æ ¹æ®å½“å‰é˜¶æ®µç”Ÿæˆæœ¬å›åˆç»Ÿä¸€è®¢å•é…ç½®
        2. å¹¶è¡Œæäº¤Nä¸ªworkerä»»åŠ¡åˆ°è¿›ç¨‹æ± 
        3. æ”¶é›†æ‰€æœ‰workerè¿”å›çš„ç»éªŒç¼“å†²åŒº
        4. å¯¹æ¯ä¸ªæ™ºèƒ½ä½“çš„ç¼“å†²åŒºè®¡ç®—GAEä¼˜åŠ¿å‡½æ•°
        5. èšåˆæ‰€æœ‰æ•°æ®ä¸ºç»Ÿä¸€è®­ç»ƒæ‰¹æ¬¡
        
        Args:
            num_steps: æ¯ä¸ªworkeré‡‡é›†çš„æœ€å¤§æ­¥æ•°
            curriculum_config: è¯¾ç¨‹å­¦ä¹ é…ç½®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            tuple: (avg_reward, batch)
                - avg_reward: æ‰€æœ‰workerçš„å¹³å‡å¥–åŠ±
                - batch: è®­ç»ƒæ‰¹æ¬¡å­—å…¸æˆ–Noneï¼ˆå¤±è´¥æ—¶ï¼‰
        """
        # æ ¸å¿ƒè®¾è®¡ï¼šç»Ÿä¸€MDP
        # 1. åŒä¸€å›åˆæ‰€æœ‰workerä½¿ç”¨ç›¸åŒè®¢å•é…ç½®ï¼ˆé¿å…æ¢¯åº¦å†²çªï¼‰
        # 2. å›åˆé—´è½®æ¢ä»»åŠ¡ç±»å‹ï¼ˆBASE_ORDERS vs éšæœºè®¢å•ï¼‰
        # 3. æ¯ä¸ªæ–°å›åˆé‡æ–°ç”Ÿæˆéšæœºè®¢å•ï¼ˆä¿è¯æ³›åŒ–ï¼‰
        
        # æ ¹æ®å½“å‰é˜¶æ®µé€‰æ‹©æœ¬å›åˆä½¿ç”¨çš„è®¢å•é…ç½®
        if not self.foundation_training_completed:
            # é˜¶æ®µä¸€ï¼šåŸºç¡€è®­ç»ƒï¼ˆéšæœºè®¢å•æ³›åŒ–ï¼‰
            current_mixing_config = self.foundation_multi_task_config
        else:
            # é˜¶æ®µäºŒï¼šæ³›åŒ–å¼ºåŒ–ï¼ˆåŠ¨æ€äº‹ä»¶ï¼‰
            current_mixing_config = self.generalization_multi_task_config
        
        # 10-23-20-00 æ ¸å¿ƒæ”¹è¿›ï¼šå›åˆçº§åˆ«çš„ä»»åŠ¡è½®æ¢ï¼ˆè€Œéworkerçº§åˆ«ï¼‰
        # ä½¿ç”¨å›åˆæ•°æ¥å†³å®šæœ¬å›åˆä½¿ç”¨å“ªç§è®¢å•é…ç½®
        use_base_orders_this_episode = False
        if current_mixing_config.get("enabled", False):
            # æ ¹æ®base_worker_fractionå†³å®šä½¿ç”¨BASE_ORDERSçš„é¢‘ç‡
            base_fraction = current_mixing_config.get("base_worker_fraction", 0.25)
            # æ¯4å›åˆä¸­æœ‰1å›åˆä½¿ç”¨BASE_ORDERSï¼ˆå¦‚æœbase_fraction=0.25ï¼‰
            cycle_length = int(1.0 / base_fraction) if base_fraction > 0 else 999999
            episode_in_cycle = (self.total_steps // num_steps) % cycle_length
            use_base_orders_this_episode = (episode_in_cycle == 0)
        
        # ç”Ÿæˆæœ¬å›åˆç»Ÿä¸€çš„è®¢å•é…ç½®
        if use_base_orders_this_episode:
            # æœ¬å›åˆæ‰€æœ‰workerä½¿ç”¨BASE_ORDERS
            episode_orders = BASE_ORDERS
            episode_tag = "BASE_ORDERS"
        else:
            # 10-27-16-45 ä¿®å¤ï¼šä¸ºéšæœºè®¢å•ç”Ÿæˆå¼•å…¥ç¡®å®šæ€§ç§å­ï¼ˆæŒ‰å›åˆç´¢å¼•ï¼‰ï¼Œæé«˜å¯å¤ç°æ€§
            # è¯´æ˜ï¼šä»…åœ¨è°ƒç”¨ generate_random_orders() å‰æš‚æ—¶è®¾ç½®éšæœºç§å­ï¼Œè°ƒç”¨åæ¢å¤åŸçŠ¶æ€
            # è¿™æ ·å¯ä»¥ç¡®ä¿ç›¸åŒçš„è®­ç»ƒseedä¸å›åˆåºå·å¾—åˆ°ä¸€è‡´çš„è®¢å•é›†åˆï¼ŒåŒæ—¶ä¸å½±å“å…¨å±€éšæœºæµç¨‹
            episode_index = (self.total_steps // num_steps)
            _py_state = random.getstate()
            _np_state = np.random.get_state()
            try:
                random.seed(self.seed + 10007 * episode_index)
                np.random.seed(self.seed + 20011 * episode_index)
                episode_orders = generate_random_orders()
            finally:
                try:
                    random.setstate(_py_state)
                except Exception:
                    pass
                try:
                    np.random.set_state(_np_state)
                except Exception:
                    pass
            episode_tag = "éšæœºè®¢å•"
        
        # 10-23-20-00 ç¡®å®šåŠ¨æ€äº‹ä»¶é…ç½®ï¼ˆæ‰€æœ‰workerç»Ÿä¸€ï¼‰
        episode_equipment_failure_enabled = False
        episode_emergency_orders_enabled = False
        if self.generalization_phase_active:
            # é˜¶æ®µäºŒï¼šå¯ç”¨åŠ¨æ€äº‹ä»¶
            dynamic_events = TRAINING_FLOW_CONFIG["generalization_phase"].get("dynamic_events", {})
            episode_equipment_failure_enabled = dynamic_events.get("equipment_failure_enabled", False)
            episode_emergency_orders_enabled = dynamic_events.get("emergency_orders_enabled", False)
        
        # --- 1. å¹¶è¡Œè¿è¡Œworkeræ”¶é›†æ•°æ® ---
        try:
            # 10-22-10-55 ä¿®å¤ï¼šä½¿ç”¨æ¨¡å—çº§åˆ«åŒ…è£…å‡½æ•°ï¼ˆçº¿ç¨‹æ± æ¨¡å¼ï¼‰
            worker_args_list = []
            for i in range(self.num_workers):
                # é»˜è®¤ä½¿ç”¨å½“å‰çš„è¯¾ç¨‹é…ç½®
                worker_curriculum_config = curriculum_config.copy() if curriculum_config else {}
                worker_curriculum_config['worker_id'] = i
                
                # 10-23-20-00 æ ¸å¿ƒä¿®å¤ï¼šæ‰€æœ‰workerä½¿ç”¨ç›¸åŒçš„è®¢å•é…ç½®
                # è®¾è®¡ç†å¿µï¼š
                #   - é¿å…å¤šMDPæ¢¯åº¦å†²çª
                #   - é€šè¿‡å›åˆé—´è½®æ¢å®ç°å¤šä»»åŠ¡æ··åˆï¼ˆè€Œéworkeré—´ï¼‰
                #   - ä¿è¯è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§
                
                # 10-23-20-00 æ‰€æœ‰workerä½¿ç”¨æœ¬å›åˆç»Ÿä¸€çš„è®¢å•å’ŒåŠ¨æ€äº‹ä»¶é…ç½®
                worker_curriculum_config['custom_orders'] = episode_orders
                worker_curriculum_config['randomize_env'] = (episode_tag != "BASE_ORDERS")
                worker_curriculum_config['equipment_failure_enabled'] = episode_equipment_failure_enabled
                worker_curriculum_config['emergency_orders_enabled'] = episode_emergency_orders_enabled
                # ç»Ÿä¸€æ­¥é•¿ï¼šä¸é‡‡é›†æ­¥æ•°ä¸€è‡´
                worker_curriculum_config['MAX_SIM_STEPS'] = num_steps
                
                # 10-25-16-10 çº¿ç¨‹æ± æ¨¡å¼ä¼˜åŒ–ï¼šç›´æ¥ä¼ é€’æ¨¡å‹å¯¹è±¡è€Œéæƒé‡ï¼Œé¿å…é‡å¤æ„å»ºç½‘ç»œ
                # if self.pool_type == "ThreadPool":
                #     # çº¿ç¨‹æ± ï¼šä¼ é€’æ¨¡å‹å¯¹è±¡ï¼ˆçº¿ç¨‹å…±äº«å†…å­˜ï¼Œæ— éœ€åºåˆ—åŒ–ï¼‰
                #     worker_curriculum_config['actor_model'] = self.shared_network.actor
                #     worker_curriculum_config['critic_model'] = self.shared_network.critic
                
                worker_args_list.append((
                    self.shared_network.actor.get_weights(),
                    self.shared_network.critic.get_weights(),
                    self.state_dim,
                    self.action_space, # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šä¼ é€’action_space
                    num_steps,
                    self.seed + self.total_steps + i, # æ¯ä¸ªworkeræœ‰ä¸åŒçš„seed
                    self.global_state_dim,
                    self.network_config,
                    worker_curriculum_config
                ))

            # 10-22-10-55 ä¿®å¤ï¼šä½¿ç”¨æ¨¡å—çº§åˆ«åŒ…è£…å‡½æ•°ï¼Œå¹¶è¡Œæ‰§è¡Œé‡‡é›†ä»»åŠ¡ï¼ˆçº¿ç¨‹æ± ï¼‰
            futures = [self.pool.submit(_collect_experience_wrapper, args) for args in worker_args_list]
            results = [f.result() for f in futures]

        except Exception as e:
            print(f"âŒ å¹¶è¡Œå·¥ä½œè¿›ç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0.0, None

        # 10-27-16-30 ä¿®å¤ï¼šæ›´å¥å£®åœ°å¤„ç†workerå¤±è´¥è¿”å›ç©ºç¼“å†²çš„æƒ…å†µ
        if not results or all((not buffers) for (buffers, _, _, _, _) in results):
            print("âš ï¸ æ‰€æœ‰workerå‡è¿”å›ç©ºç¼“å†²ï¼Œè·³è¿‡æœ¬è½®æ›´æ–°ã€‚")
            return 0.0, None

        total_reward = 0.0
        worker_rewards = []  # ğŸ”§ æ–°å¢ï¼šè®°å½•æ¯ä¸ªworkerçš„å¥–åŠ±
        
        # åˆå§‹åŒ–ç”¨äºèšåˆæ‰€æœ‰workeræ•°æ®çš„åˆ—è¡¨
        all_states, all_global_states, all_actions, all_old_probs, all_advantages, all_returns = [], [], [], [], [], []

        for (buffers, ep_reward, last_values, any_terminated, _graduated) in results:
            # æ±‡æ€»å¥–åŠ±
            total_reward += float(ep_reward)
            worker_rewards.append(float(ep_reward))

            # å°†æ¯ä¸ªagentçš„ç¼“å†²åŒºè½¬æ¢ä¸ºGAEå¹¶èšåˆ
            if not buffers:
                continue
            for agent_id, buf in buffers.items():
                if len(buf) == 0:
                    continue
                # ä½¿ç”¨æˆªæ–­æ—¶çš„bootstrapå€¼ï¼ˆå¦‚æœ‰ï¼‰
                next_v = None
                if last_values is not None and agent_id in last_values:
                    next_v = float(last_values[agent_id])
                states, global_states, actions, old_probs, advantages, returns = buf.get_batch(
                    gamma=PPO_NETWORK_CONFIG.get("gamma", 0.99),
                    lam=PPO_NETWORK_CONFIG.get("lambda_gae", 0.95),
                    next_value_if_truncated=next_v,
                    advantage_clip_val=PPO_NETWORK_CONFIG.get("advantage_clip_val")
                )
                all_states.extend(states)
                all_global_states.extend(global_states)
                all_actions.extend(actions)
                all_old_probs.extend(old_probs)
                all_advantages.extend(advantages)
                all_returns.extend(returns)

        if len(all_states) == 0:
            # è¿”å›æ—¶å°†å®Œæˆç»Ÿè®¡ç¼–ç åœ¨Noneæ‰¹æ¬¡æ—è¾¹ï¼ˆé€šè¿‡æ€»å¥–åŠ±çš„infoåœ¨å¤–å±‚æ‰“å°ï¼‰
            self._last_collect_finished_workers = self.num_workers
            self._last_collect_completed_workers = 0
            self._last_collect_worker_rewards = worker_rewards  # ğŸ”§ æ–°å¢ï¼šä¿å­˜workerå¥–åŠ±åˆ—è¡¨
            # 10-23-20-00 æ›´æ–°ï¼šä¿å­˜å›åˆçº§åˆ«çš„ä»»åŠ¡è½®æ¢ä¿¡æ¯ï¼ˆç©ºæ‰¹æ¬¡è¿”å›ï¼‰
            current_mixing_config = self.foundation_multi_task_config if not self.foundation_training_completed else self.generalization_multi_task_config
            mixing_enabled = current_mixing_config.get("enabled", False)
            self._last_collect_mixing_summary = {
                'enabled': bool(mixing_enabled),
                'episode_task': episode_tag,  # æœ¬å›åˆçš„ä»»åŠ¡ç±»å‹
                'all_workers': int(self.num_workers),
                'avg_reward': float(np.mean(worker_rewards)) if worker_rewards else None,
            }
            
            # 10-23-20-15 ä¿å­˜æœ¬å›åˆçš„å®é™…ç¯å¢ƒé…ç½®ï¼ˆç©ºæ‰¹æ¬¡æƒ…å†µï¼‰
            self._last_episode_config = {
                'custom_orders': episode_orders,
                'episode_tag': episode_tag,
                'equipment_failure_enabled': episode_equipment_failure_enabled,
                'emergency_orders_enabled': episode_emergency_orders_enabled,
            }
            
            avg_reward = total_reward / self.num_workers if self.num_workers > 0 else 0.0
            return avg_reward, None

        # 10-25-14-30 ç»Ÿè®¡æˆåŠŸå®Œæˆçš„workersï¼ˆç¼“å†²åŒºéç©ºå³å¯è§†ä¸ºæˆåŠŸï¼‰
        successful_workers = 0
        for (buffers, _, _, _, _) in results:
            if buffers:
                # è‡³å°‘ä¸€ä¸ªagentæœ‰æ•°æ®
                if any(len(buf) > 0 for buf in buffers.values()):
                    successful_workers += 1

        # å°†èšåˆåçš„æ•°æ®åˆ—è¡¨è½¬æ¢ä¸ºNumPyæ•°ç»„ï¼Œå½¢æˆæœ€ç»ˆçš„è®­ç»ƒæ‰¹æ¬¡
        states_array = np.array(all_states, dtype=np.float32)
        global_states_array = np.array(all_global_states, dtype=np.float32)
        returns_array = np.array(all_returns, dtype=np.float32)
        
        # ğŸ”§ æ–°å¢ï¼šè§‚æµ‹å’Œå›æŠ¥å½’ä¸€åŒ–
        if self.normalize_obs and len(states_array) > 0:
            # æ›´æ–°å½’ä¸€åŒ–å™¨ç»Ÿè®¡é‡
            self.obs_normalizer.update(states_array)
            self.global_obs_normalizer.update(global_states_array)
            # å½’ä¸€åŒ–è§‚æµ‹
            states_array = self.obs_normalizer.normalize(states_array)
            global_states_array = self.global_obs_normalizer.normalize(global_states_array)
        
        if self.normalize_rewards and len(returns_array) > 0:
            # å›æŠ¥å½’ä¸€åŒ–ï¼ˆä½¿ç”¨returnsä½œä¸ºç›®æ ‡ï¼‰
            self.reward_normalizer.update(returns_array.reshape(-1, 1))
            returns_array = self.reward_normalizer.normalize(returns_array.reshape(-1, 1)).flatten()
        
        batch = {
            "states": states_array,
            "global_states": global_states_array,
            "actions": np.array(all_actions),
            "old_probs": np.array(all_old_probs, dtype=np.float32),
            "advantages": np.array(all_advantages, dtype=np.float32),
            "returns": returns_array,
        }
        # è®°å½•æœ¬è½®é‡‡é›†å®Œæˆworkerä¸è¾¾æˆworkeræ•°é‡ï¼Œä¾›å¤–å±‚æ—¥å¿—æ‰“å°
        self._last_collect_finished_workers = self.num_workers
        self._last_collect_completed_workers = successful_workers
        self._last_collect_worker_rewards = worker_rewards  # ğŸ”§ æ–°å¢ï¼šä¿å­˜workerå¥–åŠ±åˆ—è¡¨
        # 10-23-20-00 æ›´æ–°ï¼šä¿å­˜å›åˆçº§åˆ«çš„ä»»åŠ¡è½®æ¢ä¿¡æ¯
        current_mixing_config = self.foundation_multi_task_config if not self.foundation_training_completed else self.generalization_multi_task_config
        mixing_enabled = current_mixing_config.get("enabled", False)
        self._last_collect_mixing_summary = {
            'enabled': bool(mixing_enabled),
            'episode_task': episode_tag,  # æœ¬å›åˆçš„ä»»åŠ¡ç±»å‹ï¼ˆBASE_ORDERSæˆ–éšæœºè®¢å•ï¼‰
            'all_workers': int(self.num_workers),
            'avg_reward': float(np.mean(worker_rewards)) if worker_rewards else None,
        }
        
        # 10-23-20-15 ä¿å­˜æœ¬å›åˆçš„å®é™…ç¯å¢ƒé…ç½®ï¼ˆä¾›è¯„ä¼°æ—¶ä½¿ç”¨ï¼Œç¡®ä¿è®­ç»ƒ-è¯„ä¼°ä¸€è‡´æ€§ï¼‰
        self._last_episode_config = {
            'custom_orders': episode_orders,
            'episode_tag': episode_tag,
            'equipment_failure_enabled': episode_equipment_failure_enabled,
            'emergency_orders_enabled': episode_emergency_orders_enabled,
        }
        avg_reward = total_reward / self.num_workers if self.num_workers > 0 else 0.0
        return avg_reward, batch
    
    def update_policy(self, batch: Dict[str, np.ndarray], entropy_coeff: float) -> Dict[str, float]:
        """
        ä¸“å®¶ä¿®å¤ï¼šæ¥æ”¶å·²å¤„ç†å¥½çš„æ•°æ®æ‰¹æ¬¡ï¼Œæ‰§è¡Œæ ‡å‡†çš„PPOæ›´æ–°æµç¨‹
        - ç§»é™¤äº†æ•°æ®èšåˆå’ŒGAEè®¡ç®—é€»è¾‘ï¼Œå› ä¸ºè¿™äº›å·²åœ¨ `collect_and_process_experience` ä¸­å®Œæˆ
        """
        # 1. ä»æ‰¹æ¬¡ä¸­è§£åŒ…æ•°æ®
        all_states = batch["states"]
        all_global_states = batch["global_states"]
        all_actions = batch["actions"]
        all_old_probs = batch["old_probs"]
        all_advantages = batch["advantages"]
        all_returns = batch["returns"]

        total_samples = len(all_states)
        if total_samples == 0:
            return {}

        # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡
        total_actor_loss, total_critic_loss, total_entropy = 0, 0, 0
        total_approx_kl, total_clip_fraction = 0, 0
        update_count = 0

        # 2. æ ‡å‡†PPOæ›´æ–°å¾ªç¯ (Epochs + Mini-batch)
        ppo_epochs = PPO_NETWORK_CONFIG.get("ppo_epochs", 10)
        num_minibatches = PPO_NETWORK_CONFIG.get("num_minibatches", 4)
        
        if total_samples < num_minibatches:
            num_minibatches = 1
            
        batch_size = total_samples // num_minibatches

        for epoch in range(ppo_epochs):
            # 2.1. æ•°æ®éšæœºåŒ– (Shuffle)
            indices = np.arange(total_samples)
            np.random.shuffle(indices)

            shuffled_states = all_states[indices]
            shuffled_global_states = all_global_states[indices]
            shuffled_actions = all_actions[indices]
            shuffled_old_probs = all_old_probs[indices]
            shuffled_advantages = all_advantages[indices]
            shuffled_returns = all_returns[indices]

            # 2.2. Mini-batch è®­ç»ƒ
            for i in range(0, total_samples, batch_size):
                start = i
                end = i + batch_size
                
                if end > total_samples:
                    end = total_samples
                if start == end:
                    continue

                # æå–Mini-batchæ•°æ®
                mini_batch_states = shuffled_states[start:end]
                mini_batch_global_states = shuffled_global_states[start:end]
                mini_batch_actions = shuffled_actions[start:end]
                mini_batch_old_probs = shuffled_old_probs[start:end]
                mini_batch_advantages = shuffled_advantages[start:end]
                mini_batch_returns = shuffled_returns[start:end]

                # 2.3. æ‰§è¡Œç½‘ç»œæ›´æ–°
                loss_info = self.shared_network.update(
                    mini_batch_states,
                    mini_batch_global_states,
                    mini_batch_actions,
                    mini_batch_old_probs,
                    mini_batch_advantages,
                    mini_batch_returns,
                    entropy_coeff=entropy_coeff
                )

                # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
                if loss_info:
                    total_actor_loss += loss_info["actor_loss"]
                    total_critic_loss += loss_info["critic_loss"]
                    total_entropy += loss_info["entropy"]
                    total_approx_kl += loss_info["approx_kl"]
                    total_clip_fraction += loss_info["clip_fraction"]
                    update_count += 1
        
        # è¿”å›å¹³å‡æŸå¤±
        if update_count > 0:
            return {
                "actor_loss": total_actor_loss / update_count,
                "critic_loss": total_critic_loss / update_count,
                "entropy": total_entropy / update_count,
                "approx_kl": total_approx_kl / update_count,
                "clip_fraction": total_clip_fraction / update_count,
            }
        return {}
    
    def _independent_exam_evaluation(self, env, curriculum_config, seed):
        """ğŸ”§ V33 æ–°å¢ï¼šç‹¬ç«‹çš„è€ƒè¯•è¯„ä¼°ï¼Œç¡®ä¿æ¯è½®éƒ½æ˜¯å…¨æ–°çš„ä»¿çœŸ"""
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)
        
        observations, infos = env.reset(seed=seed)
        episode_reward = 0
        step_count = 0
        
        while step_count < self.max_steps_for_eval:
            actions = {}
            
            # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼Œä½†åŸºäºæ–°çš„éšæœºç¯å¢ƒçŠ¶æ€
            for agent in env.agents:
                if agent in observations:
                    state = tf.expand_dims(observations[agent], 0)
                    network = self.shared_network
                    # 10-27-16-30 è¯„ä¼°é˜¶æ®µæ˜ç¡®ç¦ç”¨Dropoutç­‰éšæœºæ€§
                    action_probs = network.actor(state, training=False)
                    if network.is_multidiscrete:
                        # 10-23-16-30 ä¿®å¤ï¼šè¯„ä¼°é˜¶æ®µä½¿ç”¨æ— æ”¾å›è´ªå¿ƒï¼Œé¿å…ç³»ç»Ÿæ€§å†²çª
                        action_prob_list = action_probs if isinstance(action_probs, list) else [action_probs]
                        heads = len(action_prob_list)
                        used = set()
                        selected = []
                        for i in range(heads):
                            p = action_prob_list[i][0].numpy()
                            # ğŸ”§ åº”ç”¨åŠ¨ä½œæ©ç ï¼ˆè‹¥æä¾›ï¼‰
                            mask = infos.get(agent, {}).get('action_mask', None)
                            if mask is not None and len(mask) == p.shape[0]:
                                p = p * mask
                            # å…è®¸å¤šä¸ªå¤´é€‰æ‹© IDLE(0)ï¼Œä»…å±è”½å·²é€‰çš„éé›¶åŠ¨ä½œ
                            if used:
                                for u in list(used):
                                    if u != 0:
                                        p[u] = 0.0
                            idx = int(np.argmax(p)) if p.sum() > 1e-8 else 0
                            selected.append(idx)
                            if idx != 0:
                                used.add(idx)
                        action = np.array(selected, dtype=network.action_space.dtype)
                    else:
                        action = int(tf.argmax(action_probs[0]))
                    actions[agent] = action
            
            observations, rewards, terminations, truncations, infos = env.step(actions)
            episode_reward += sum(rewards.values())
            step_count += 1
            
            if any(terminations.values()) or any(truncations.values()):
                break
        
        # è·å–æœ€ç»ˆç»Ÿè®¡
        final_stats = env.sim.get_final_stats()
        return {
            'mean_reward': episode_reward,
            'mean_makespan': final_stats.get('makespan', 0),
            'mean_utilization': final_stats.get('mean_utilization', 0),
            'mean_completed_parts': final_stats.get('total_parts', 0),
            'mean_tardiness': final_stats.get('total_tardiness', 0)
        }
    
    def quick_kpi_evaluation(self, num_episodes: int = 1, curriculum_config: Dict[str, Any] = None) -> Dict[str, float]:
        """10-23-20-15 ä¿®å¤ç‰ˆï¼šå¿«é€ŸKPIè¯„ä¼°ï¼Œç¡®ä¿è¯„ä¼°ç¯å¢ƒä¸è®­ç»ƒç¯å¢ƒå®Œå…¨ä¸€è‡´"""
        # 10-23-20-15 æ ¸å¿ƒæ”¹è¿›ï¼šä½¿ç”¨ä¸Šä¸€ä¸ªè®­ç»ƒå›åˆçš„å®é™…é…ç½®è¿›è¡Œè¯„ä¼°
        # è¿™ç¡®ä¿äº†è¯„ä¼°KPIèƒ½å¤ŸçœŸå®åæ˜ å½“å‰è®­ç»ƒç¯å¢ƒçš„è¡¨ç°
        eval_config = curriculum_config.copy() if curriculum_config else {}
        
        # 10-23-20-15 ä¼˜å…ˆä½¿ç”¨ä¿å­˜çš„å®é™…ç¯å¢ƒé…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(self, '_last_episode_config') and self._last_episode_config:
            last_config = self._last_episode_config
            eval_config['custom_orders'] = last_config['custom_orders']
            eval_config['equipment_failure_enabled'] = last_config['equipment_failure_enabled']
            eval_config['emergency_orders_enabled'] = last_config['emergency_orders_enabled']
            # ä¿å­˜è®¢å•æ ‡ç­¾ä¾›æ—¥å¿—ä½¿ç”¨
            eval_config['episode_tag'] = last_config['episode_tag']
        
        # è¯„ä¼°æ­¥é•¿å¯¹é½ç¯å¢ƒè¶…æ—¶
        eval_config['MAX_SIM_STEPS'] = self.max_steps_for_eval
        # è®­ç»ƒæœŸè¯„ä¼°å¼ºåˆ¶ç¡®å®šæ€§å€™é€‰ï¼Œä¿è¯å¯å¤ç°
        eval_config['deterministic_candidates'] = True
        
        env = make_parallel_env(eval_config)
        
        total_rewards = []
        makespans = []
        utilizations = []
        completed_parts_list = []
        tardiness_list = []
        
        for episode in range(num_episodes):
            observations, infos = env.reset()
            episode_reward = 0
            step_count = 0
            
            # ä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„æ­¥æ•°é™åˆ¶
            while step_count < self.max_steps_for_eval:
                actions = {}
                
                # 10-23-16-35 ä¿®å¤ï¼šç¡®å®šæ€§è¯„ä¼°é‡‡ç”¨æ— æ”¾å›è´ªå¿ƒï¼Œé¿å…ç³»ç»Ÿæ€§å†²çª
                for agent in env.agents:
                    if agent in observations:
                        state = tf.expand_dims(observations[agent], 0)
                        # 10-27-16-30 è®­ç»ƒæœŸå¿«é€Ÿè¯„ä¼°ï¼šç¦ç”¨Dropout
                        action_probs = self.shared_network.actor(state, training=False)
                        if isinstance(self.action_space, gym.spaces.MultiDiscrete):
                            action_prob_list = action_probs if isinstance(action_probs, list) else [action_probs]
                            heads = len(action_prob_list)
                            used = set()
                            selected = []
                            for i in range(heads):
                                p = action_prob_list[i][0].numpy()
                                # ğŸ”§ åº”ç”¨åŠ¨ä½œæ©ç ï¼ˆè‹¥æä¾›ï¼‰
                                mask = infos.get(agent, {}).get('action_mask', None)
                                if mask is not None and len(mask) == p.shape[0]:
                                    p = p * mask
                                # å…è®¸å¤šä¸ªå¤´é€‰æ‹© IDLE(0)ï¼Œä»…å±è”½å·²é€‰çš„éé›¶åŠ¨ä½œ
                                if used:
                                    for u in list(used):
                                        if u != 0:
                                            p[u] = 0.0
                                idx = int(np.argmax(p)) if p.sum() > 1e-8 else 0
                                selected.append(idx)
                                if idx != 0:
                                    used.add(idx)
                            actions[agent] = np.array(selected, dtype=self.action_space.dtype)
                        else:
                            actions[agent] = int(tf.argmax(action_probs[0]))
                
                observations, rewards, terminations, truncations, infos = env.step(actions)
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if any(terminations.values()) or any(truncations.values()):
                    break
            
            # è·å–æœ€ç»ˆç»Ÿè®¡
            final_stats = env.sim.get_final_stats()
            total_rewards.append(episode_reward)
            makespans.append(final_stats.get('makespan', 0))
            utilizations.append(final_stats.get('mean_utilization', 0))
            completed_parts_list.append(final_stats.get('total_parts', 0))
            tardiness_list.append(final_stats.get('total_tardiness', 0))
        
        # ğŸ”§ V37 æ–°å¢ï¼šæ£€æŸ¥ç¯å¢ƒé‡ç½®ä¿¡å·
        strategy_reset_signal = getattr(env.sim, '_trigger_strategy_reset', False)
        if strategy_reset_signal:
            self._env_strategy_reset_signal = True
        
        env.close()
        
        return {
            'mean_reward': np.mean(total_rewards),
            'mean_makespan': np.mean(makespans),
            'mean_utilization': np.mean(utilizations),
            'mean_completed_parts': np.mean(completed_parts_list),
            'mean_tardiness': np.mean(tardiness_list)
        }
    
    def simple_evaluation(self, num_episodes: int = 5) -> Dict[str, float]:
        """ğŸ”§ ä¿®å¤ç‰ˆï¼šç®€å•è¯„ä¼°ï¼Œè¿”å›æ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡"""
        env, _ = self.create_environment()
        
        total_rewards = []
        total_steps = []
        makespans = []
        completed_parts = []
        utilizations = []
        tardiness_list = []
        
        for episode in range(num_episodes):
            observations, infos = env.reset()
            episode_reward = 0
            step_count = 0
            
            while step_count < self.max_steps_for_eval:
                actions = {}
                
                # 10-23-14-30 ä¿®å¤ï¼šä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥è¯„ä¼°ï¼ˆæ­£ç¡®å¤„ç†MultiDiscreteå¤šå¤´è¾“å‡ºï¼‰
                for agent in env.agents:
                    if agent in observations:
                        state = tf.expand_dims(observations[agent], 0)
                        action_probs = self.shared_network.actor(state, training=False)
                        if isinstance(self.action_space, gym.spaces.MultiDiscrete):
                            # 10-23-14-30 ä¿®å¤ï¼šæ¯ä¸ªå¤´åˆ†åˆ«é€‰æ‹©argmaxï¼Œè€ŒéæŠŠå¤šå¤´è¾“å‡ºå½“å•ä¸ªåˆ†å¸ƒ
                            action_prob_list = action_probs if isinstance(action_probs, list) else [action_probs]
                            actions_list = []
                            for i, p_head in enumerate(action_prob_list):
                                p = p_head[0].numpy()
                                mask = infos.get(agent, {}).get('action_mask', None)
                                if mask is not None and len(mask) == p.shape[0]:
                                    p = p * mask
                                actions_list.append(int(np.argmax(p)) if p.sum() > 1e-8 else 0)
                            actions[agent] = np.array(actions_list, dtype=self.action_space.dtype)
                        else:
                            # å•å¤´ï¼šåº”ç”¨æ©ç åå–argmax
                            p = action_probs[0].numpy()
                            mask = infos.get(agent, {}).get('action_mask', None)
                            if mask is not None and len(mask) == p.shape[0]:
                                p = p * mask
                            actions[agent] = int(np.argmax(p)) if p.sum() > 1e-8 else 0
                
                observations, rewards, terminations, truncations, infos = env.step(actions)
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if any(terminations.values()) or any(truncations.values()):
                    break
            
            # ğŸ”§ ä¿®å¤ï¼šè·å–å®Œæ•´çš„ä¸šåŠ¡æŒ‡æ ‡
            final_stats = env.sim.get_final_stats()
            total_rewards.append(episode_reward)
            total_steps.append(step_count)
            makespans.append(final_stats.get('makespan', 0))
            completed_parts.append(final_stats.get('total_parts', 0))
            utilizations.append(final_stats.get('mean_utilization', 0))
            tardiness_list.append(final_stats.get('total_tardiness', 0))
        
        env.close()
        
        return {
            'mean_reward': np.mean(total_rewards),
            'std_reward': np.std(total_rewards),
            'mean_steps': np.mean(total_steps),
            'mean_makespan': np.mean(makespans),
            'mean_completed_parts': np.mean(completed_parts),
            'mean_utilization': np.mean(utilizations),
            'mean_tardiness': np.mean(tardiness_list)
        }
    
    
    def train(self, max_episodes: int = 1000, steps_per_episode: int = 200, 
              eval_frequency: int = 20, adaptive_mode: bool = True):
        """ è‡ªé€‚åº”è®­ç»ƒä¸»å¾ªç¯ï¼šæ ¹æ®æ€§èƒ½è‡ªåŠ¨è°ƒæ•´è®­ç»ƒç­–ç•¥å’Œè½®æ•°"""
        # è‡ªé€‚åº”æ¨¡å¼ï¼šæœ€å¤§è½®æ•°ä½œä¸ºä¸Šé™ï¼Œå®é™…è½®æ•°æ ¹æ®æ€§èƒ½åŠ¨æ€å†³å®š

        if adaptive_mode:
            self.training_targets["max_episodes"] = max_episodes
        
        # ğŸ”§ V16ï¼šæ˜¾ç¤ºè¯¾ç¨‹å­¦ä¹ é…ç½®
        curriculum_config = self.training_flow_config["foundation_phase"]["curriculum_learning"]
        if curriculum_config.get("enabled", False):
            print(f"ğŸ“š è¯¾ç¨‹å­¦ä¹ å·²å¯ç”¨ï¼Œå…±{len(curriculum_config['stages'])}ä¸ªé˜¶æ®µ:")
            for i, stage in enumerate(curriculum_config["stages"]):
                print(f"   é˜¶æ®µ{i+1}: {stage['name']} - è®¢å• {stage['orders_scale']*100:.0f}%")
        print("=" * 80)
        
        if not validate_config():
            print("âŒ é…ç½®éªŒè¯å¤±è´¥")
            return
        
        # è®­ç»ƒå¼€å§‹æ—¶é—´è®°å½•
        training_start_time = time.time()
        training_start_datetime = datetime.now()
        print(f"ğŸ• è®­ç»ƒå¼€å§‹æ—¶é—´: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        
        # ğŸ”§ V16ï¼šè¯¾ç¨‹å­¦ä¹ ç®¡ç†
        curriculum_config = self.training_flow_config["foundation_phase"]["curriculum_learning"]
        curriculum_enabled = curriculum_config.get("enabled", False)
        current_stage = 0
        stage_episode_count = 0
        
        # ğŸ”§ V8 ä¼˜åŒ–: ä¸å†éœ€è¦åˆ›å»ºä¸»ç¯å¢ƒï¼Œåªåˆ›å»ºç¼“å†²åŒº
        buffers = {
            agent: ExperienceBuffer() 
            for agent in self.agent_ids
        }
        
        best_reward = float('-inf')
        best_makespan = float('inf')
        
        # ğŸ”§ V27 æ ¸å¿ƒä¿®å¤ï¼šä¸ºè¯¾ç¨‹å­¦ä¹ çš„æ¯ä¸ªé˜¶æ®µç‹¬ç«‹è·Ÿè¸ªæœ€ä½³åˆ†æ•°
        stage_best_scores = [float('-inf')] * len(curriculum_config["stages"]) if curriculum_enabled else []
        
        # ğŸ”§ åˆå§‹åŒ–ç”¨äºè¯¾ç¨‹å­¦ä¹ æ¯•ä¸šæ£€æŸ¥çš„æ€§èƒ½æŒ‡æ ‡ï¼Œæ¯•ä¸šæ£€æŸ¥å°†ä½¿ç”¨ä¸Šä¸€ä¸ªå›åˆçš„å‡†ç¡®æ•°æ®
        last_kpi_results = {}
        last_current_score = 0.0
        
        try:
            for episode in range(max_episodes):
                iteration_start_time = time.time()
                
                # --- æ ¸å¿ƒåˆ›æ–°ï¼šåŸºç¡€è®­ç»ƒ + éšæœºé¢†åŸŸå¼ºåŒ– é€»è¾‘ ---
                current_curriculum_config = None
                
                # é¦–å…ˆå¤„ç†è¯¾ç¨‹å­¦ä¹ é€»è¾‘ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if curriculum_enabled and not self.foundation_training_completed:
                    stage_config = curriculum_config["stages"][current_stage]
                    
                    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³è‡ªé€‚åº”æ¯•ä¸šæ¡ä»¶
                    if self.check_curriculum_stage_graduation(last_kpi_results, last_current_score, stage_config):
                        print(f"âœ… é˜¶æ®µ '{stage_config['name']}' æ¯•ä¸šæ ‡å‡†è¾¾æˆï¼")
                        
                        if stage_config.get('is_final_stage', False):
                            print("ğŸ† è¯¾ç¨‹å­¦ä¹ å®Œæˆï¼ç°åœ¨å¼€å§‹åŸºç¡€èƒ½åŠ›è®¤è¯ï¼Œé€šè¿‡åè¿›å…¥æ³›åŒ–å¼ºåŒ–é˜¶æ®µã€‚")
                            # æ ‡è®°è¯¾ç¨‹å­¦ä¹ éƒ¨åˆ†ç»“æŸï¼Œåç»­é€»è¾‘å°†æ¥ç®¡å¹¶å¯åŠ¨åŸºç¡€èƒ½åŠ›è®¤è¯
                            self.foundation_training_completed = True 
                        else:
                            # æ™‹çº§åˆ°ä¸‹ä¸€ä¸ªè¯¾ç¨‹é˜¶æ®µ
                            current_stage += 1
                            stage_episode_count = 0
                            self.curriculum_stage_achievement_count = 0  # ä¸ºæ–°é˜¶æ®µé‡ç½®è®¡æ•°å™¨
                            next_stage_name = curriculum_config["stages"][current_stage]['name']
                            print(f"ğŸš€ è¿›å…¥ä¸‹ä¸€é˜¶æ®µ: '{next_stage_name}'")
                    
                    # è·å–å½“å‰é˜¶æ®µé…ç½® (é˜¶æ®µå¯èƒ½å·²æ›´æ–°)
                    stage = curriculum_config["stages"][current_stage]
                    current_curriculum_config = {
                        'orders_scale': stage.get('orders_scale', 1.0),
                        'time_scale': stage.get('time_scale', 1.0),
                        'stage_name': stage.get('name', f'Stage {current_stage}')
                    }
                    
                    # è¯¦ç»†çš„é˜¶æ®µåˆ‡æ¢å’ŒçŠ¶æ€æ—¥å¿—
                    if stage_episode_count == 0:
                        print(f"ğŸ“š [å›åˆ {episode+1}] ğŸ”„ è¯¾ç¨‹å­¦ä¹ é˜¶æ®µåˆ‡æ¢!")
                        print(f"   æ–°é˜¶æ®µ: {stage['name']}")
                        print(f"   è®¢å•æ¯”ä¾‹: {stage['orders_scale']} (ç›®æ ‡é›¶ä»¶æ•°: {int(get_total_parts_count() * stage['orders_scale'])})")
                        print(f"   æ—¶é—´æ¯”ä¾‹: {stage['time_scale']} (æ—¶é—´é™åˆ¶: {int(SIMULATION_TIME * stage['time_scale'])}åˆ†é’Ÿ)")
                        print(f"ğŸ”§ å½“å‰è¯¾ç¨‹é…ç½®å°†ä¼ é€’ç»™æ‰€æœ‰worker: orders_scale={stage['orders_scale']}, time_scale={stage['time_scale']}")
                        print("-" * 60)
                    
                    # ğŸ”§ V17æ–°å¢ï¼šæ¯10è½®æ˜¾ç¤ºé˜¶æ®µçŠ¶æ€
                    if episode % 10 == 0:
                        print(f"ğŸ“š è¯¾ç¨‹çŠ¶æ€: {stage['name']} (ç¬¬ {stage_episode_count} å›åˆ)")
                        print(f"   å½“å‰éš¾åº¦: {int(get_total_parts_count() * stage['orders_scale'])}é›¶ä»¶, {stage['time_scale']:.1f}xæ—¶é—´")    
                    stage_episode_count += 1
                
                # --- æ ¸å¿ƒè®­ç»ƒé˜¶æ®µåˆ¤æ–­ ---
                
                # æ£€æŸ¥è¯¾ç¨‹å­¦ä¹ æ˜¯å¦å·²å®Œæˆæ‰€æœ‰é˜¶æ®µ
                curriculum_just_completed = False
                if curriculum_enabled and self.foundation_training_completed and not self.generalization_phase_active:
                    # è¿™æ˜¯ä¸€ä¸ªè¿‡æ¸¡çŠ¶æ€ï¼Œè¡¨ç¤ºè¯¾ç¨‹å­¦ä¹ åˆšåˆšå®Œæˆï¼Œä½†è¿˜æœªæ­£å¼è¿›å…¥æ³›åŒ–é˜¶æ®µ
                    # åœ¨è¿™ä¸ªçŠ¶æ€ä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åŸºç¡€èƒ½åŠ›è®¤è¯çš„é…ç½®
                    curriculum_just_completed = True

                if not self.foundation_training_completed or curriculum_just_completed:
                    # 10-23-18-00 æ–°èŒƒå¼ï¼šé˜¶æ®µ1ï¼šåŸºç¡€èƒ½åŠ›è®­ç»ƒï¼ˆéšæœºè®¢å•æ³›åŒ–è®­ç»ƒï¼‰
                    # æ ¸å¿ƒæ”¹å˜ï¼šä¸å†ä½¿ç”¨å›ºå®šBASE_ORDERSï¼Œè€Œæ˜¯é‡‡ç”¨éšæœºè®¢å•è®­ç»ƒ
                    # æ³¨æ„ï¼šå¤šä»»åŠ¡æ··åˆé€»è¾‘ä¼šåœ¨collect_and_process_experienceä¸­åº”ç”¨
                    # è¿™é‡Œåªæä¾›ä¸»é…ç½®æ¡†æ¶ï¼Œå…·ä½“çš„workerçº§åˆ«è®¢å•åˆ†é…åœ¨æ•°æ®é‡‡é›†æ—¶å®Œæˆ
                    if not curriculum_enabled or curriculum_just_completed:
                        foundation_config = {
                            'orders_scale': 1.0,
                            'time_scale': 1.0,
                            'stage_name': 'åŸºç¡€èƒ½åŠ›è®­ç»ƒ-éšæœºè®¢å•',
                            # 10-23-18-00 å…³é”®æ”¹å˜ï¼šä¸å†è®¾ç½®custom_ordersï¼Œè®©å¤šä»»åŠ¡æ··åˆé€»è¾‘å¤„ç†
                            # éšæœºè®¢å•å°†åœ¨collect_and_process_experienceä¸­æŒ‰workeråˆ†é…
                        }
                        current_curriculum_config = foundation_config
                    
                    # åœ¨æ¯ä¸ªå›åˆéƒ½æ·»åŠ å½“å‰å›åˆæ•°ï¼Œä¾›ç¯å¢ƒå†…éƒ¨ä½¿ç”¨
                        if current_curriculum_config:
                            current_curriculum_config['current_episode'] = episode
                
                elif not self.generalization_phase_active:
                    # 10-23-18-00 é˜¶æ®µè½¬æ¢ï¼šåŸºç¡€è®­ç»ƒå®Œæˆï¼Œè¿›å…¥æ³›åŒ–å¼ºåŒ–é˜¶æ®µ
                    self.generalization_phase_active = True
                    print("\n" + "="*80)
                    print(f"ğŸš€ [å›åˆ {episode+1}] åŸºç¡€è®­ç»ƒå·²å®Œæˆï¼Œæ­£å¼è¿›å…¥åŠ¨æ€äº‹ä»¶é²æ£’æ€§è®­ç»ƒé˜¶æ®µ!")
                    print("="*80 + "\n")
                
                if self.generalization_phase_active:
                    # 10-23-18-00 æ–°èŒƒå¼ï¼šé˜¶æ®µ2ï¼šåŠ¨æ€äº‹ä»¶é²æ£’æ€§è®­ç»ƒï¼ˆåŠ¨æ€äº‹ä»¶é²æ£’æ€§è®­ç»ƒï¼‰
                    # æ ¸å¿ƒæ”¹å˜ï¼šå¯ç”¨è®¾å¤‡æ•…éšœã€ç´§æ€¥æ’å•ç­‰åŠ¨æ€äº‹ä»¶
                    # æ³¨æ„ï¼šå¤šä»»åŠ¡æ··åˆé€»è¾‘ä¼šåœ¨collect_and_process_experienceä¸­åº”ç”¨
                    # åŠ¨æ€äº‹ä»¶ï¼ˆè®¾å¤‡æ•…éšœã€ç´§æ€¥æ’å•ï¼‰ä¹Ÿåœ¨é‚£é‡Œå¯ç”¨
                    generalization_config = {
                        'randomize_env': True,  # å¯ç”¨ç¯å¢ƒæ‰°åŠ¨
                        'stage_name': f'æ³›åŒ–å¼ºåŒ–-åŠ¨æ€äº‹ä»¶-R{episode}',
                        'current_episode': episode
                        # 10-23-18-00 å…³é”®æ”¹å˜ï¼šä¸å†åœ¨è¿™é‡Œè®¾ç½®custom_orderså’ŒåŠ¨æ€äº‹ä»¶é…ç½®
                        # è¿™äº›å°†åœ¨collect_and_process_experienceä¸­æŒ‰workeråˆ†é…
                    }
                    
                    current_curriculum_config = generalization_config
                    
                    if episode % 20 == 0:
                        # 10-23-18-00 æ–°èŒƒå¼ï¼šä¿¡æ¯æ˜¾ç¤ºè°ƒæ•´ï¼ˆä¸å†åœ¨è¿™é‡Œç”Ÿæˆrandom_ordersï¼‰
                        # åŠ¨æ€äº‹ä»¶çŠ¶æ€ç”±é…ç½®æ–‡ä»¶æ§åˆ¶
                        generalization_criteria = self.training_flow_config["generalization_phase"]["completion_criteria"]
                        dynamic_events = TRAINING_FLOW_CONFIG["generalization_phase"].get("dynamic_events", {})
                        print(f"ğŸ² æ³›åŒ–å¼ºåŒ–é˜¶æ®µ: åŠ¨æ€äº‹ä»¶è®­ç»ƒ")
                        print(f"   è®¾å¤‡æ•…éšœ: {'âœ“' if dynamic_events.get('equipment_failure_enabled', False) else 'âœ—'}")
                        print(f"   ç´§æ€¥æ’å•: {'âœ“' if dynamic_events.get('emergency_orders_enabled', False) else 'âœ—'}")
                        print(f"   æ³›åŒ–é˜¶æ®µè¿ç»­è¾¾æ ‡: {self.generalization_achievement_count}/{generalization_criteria['target_consistency']} æ¬¡")
                

                collect_start_time = time.time()
                episode_reward, batch = self.collect_and_process_experience(steps_per_episode, current_curriculum_config)
                collect_duration = time.time() - collect_start_time

                # 10-25-14-30 ğŸ”§ ä¿®å¤ï¼šé€’å¢æ€»æ­¥æ•°ç”¨äºå¤šä»»åŠ¡æ··åˆä¸seedå¤šæ ·åŒ–
                self.total_steps += steps_per_episode
                
                # ğŸ”§ V6 å®‰å…¨çš„ç­–ç•¥æ›´æ–°ï¼ˆåŒ…å«å†…å­˜æ£€æŸ¥ï¼‰
                update_start_time = time.time()
                if batch is not None:
                    losses = self.update_policy(batch, entropy_coeff=self.current_entropy_coeff)
                else:
                    # ç©ºæ‰¹æ¬¡é˜²å¾¡ï¼šæä¾›å®‰å…¨çš„é»˜è®¤æŒ‡æ ‡å¹¶è·³è¿‡æ›´æ–°
                    losses = {
                        'actor_loss': 0.0,
                        'critic_loss': 0.0,
                        'entropy': float(self.current_entropy_coeff),
                        'approx_kl': 0.0,
                        'clip_fraction': 0.0,
                    }
                update_duration = time.time() - update_start_time
                
                # è®°å½•ç»Ÿè®¡
                iteration_end_time = time.time()
                iteration_duration = iteration_end_time - iteration_start_time
                self.iteration_times.append(iteration_duration)
                self.episode_rewards.append(episode_reward)

                
                # æå‰è¿›è¡ŒKPIè¯„ä¼°ï¼ˆæŒ‰é¢‘ç‡æ§åˆ¶ï¼‰ï¼Œä»¥ä¾¿æ•´åˆTensorBoardæ—¥å¿—
                if eval_frequency is None:
                    eval_frequency = 1
                if eval_frequency <= 1 or (episode % eval_frequency == 0):
                    kpi_results = self.quick_kpi_evaluation(num_episodes=1, curriculum_config=current_curriculum_config)
                    self.kpi_history.append(kpi_results)
                else:
                    # è‹¥æœ¬å›åˆä¸è¯„ä¼°ï¼Œåˆ™æ²¿ç”¨ä¸Šä¸€æ¬¡å¯ç”¨KPIï¼ˆè‹¥æ— åˆ™åšæœ€å°å ä½ï¼‰
                    if self.kpi_history:
                        kpi_results = self.kpi_history[-1]
                    else:
                        kpi_results = {
                            'mean_reward': 0.0,
                            'mean_makespan': 0.0,
                            'mean_utilization': 0.0,
                            'mean_completed_parts': 0.0,
                            'mean_tardiness': 0.0
                        }
                        self.kpi_history.append(kpi_results)

                # ğŸ”§ æ ¸å¿ƒæ”¹é€ ï¼šè®¡ç®—å½“å‰å›åˆçš„ç»¼åˆè¯„åˆ†
                current_score = calculate_episode_score(kpi_results, config=current_curriculum_config)
                
                # ğŸ”§ BUGä¿®å¤ï¼šä¿å­˜æœ¬å›åˆçš„KPIç»“æœï¼Œä¾›ä¸‹ä¸€å›åˆçš„æ¯•ä¸šæ£€æŸ¥ä½¿ç”¨
                last_kpi_results = kpi_results
                last_current_score = current_score
                
                # --- æ ¸å¿ƒåˆ›æ–°ï¼šæ£€æŸ¥é˜¶æ®µè½¬æ¢å’Œè®­ç»ƒå®Œæˆæ¡ä»¶ ---
                target_parts_for_check = self._get_target_parts(current_curriculum_config)
                
                completion_rate_for_check = (kpi_results.get('mean_completed_parts', 0) / target_parts_for_check) * 100 if target_parts_for_check > 0 else 0
                
                # ğŸ”§ ä¿®å¤ï¼šåªæœ‰åœ¨æœ€ç»ˆé˜¶æ®µæˆ–è¯¾ç¨‹å­¦ä¹ å®Œæˆåæ‰æ£€æŸ¥åŸºç¡€è®­ç»ƒå®Œæˆ
                should_check_foundation_completion = False
                if not self.foundation_training_completed:
                    if curriculum_enabled:
                        # è¯¾ç¨‹å­¦ä¹ æ¨¡å¼ï¼šåªæœ‰åœ¨æœ€ç»ˆé˜¶æ®µæ‰æ£€æŸ¥åŸºç¡€è®­ç»ƒå®Œæˆ
                        if current_stage < len(curriculum_config["stages"]):
                            current_stage_info = curriculum_config["stages"][current_stage]
                            if current_stage_info.get('is_final_stage', False):
                                should_check_foundation_completion = True
                        # æˆ–è€…è¯¾ç¨‹å­¦ä¹ å·²å®Œæˆæ‰€æœ‰é˜¶æ®µ
                        elif current_stage >= len(curriculum_config["stages"]):
                            should_check_foundation_completion = True
                    else:
                        # éè¯¾ç¨‹å­¦ä¹ æ¨¡å¼ï¼šç›´æ¥æ£€æŸ¥
                        should_check_foundation_completion = True
                    
                    if should_check_foundation_completion:
                        # ğŸ”§ BUGä¿®å¤ï¼šä¸è¯¾ç¨‹å­¦ä¹ é€»è¾‘ç»Ÿä¸€ï¼Œä½¿ç”¨ä¸Šä¸€ä¸ªå›åˆçš„KPIç»“æœæ¥åˆ¤æ–­æ˜¯å¦æ¯•ä¸š
                        # 10-27-17-30 ä¿®å¤ï¼šä¼ å…¥current_curriculum_configä»¥ä½¿ç”¨æ­£ç¡®çš„ç›®æ ‡é›¶ä»¶æ•°
                        if self.check_foundation_training_completion(last_kpi_results, last_current_score, current_curriculum_config):
                            self.foundation_training_completed = True
                
                # æ£€æŸ¥æ³›åŒ–è®­ç»ƒæ˜¯å¦å®Œæˆï¼ˆè¿™å°†è§¦å‘æ•´ä¸ªè®­ç»ƒçš„ç»“æŸï¼‰
                training_should_end = False
                if self.generalization_phase_active:
                    if self.check_generalization_training_completion(current_score, completion_rate_for_check):
                        training_should_end = True
                
                # --- ğŸ”§ ä¿®å¤ï¼šè‡ªé€‚åº”ç†µçš„åœæ»è®¡æ•°å™¨ä»…åœ¨å…è®¸ç†µå¢åŠ çš„é˜¶æ®µç´¯ç§¯ ---
                # 1. åˆ¤æ–­æ˜¯å¦å¤„äºå…è®¸ç†µå¢åŠ çš„é˜¶æ®µ
                # è¯¾ç¨‹å­¦ä¹ ä¸‹ï¼šä»…å½“å¤„äºæœ€ç»ˆé˜¶æ®µæˆ–å·²ç»è¿›å…¥æ³›åŒ–é˜¶æ®µæ‰å…è®¸ï¼›
                # éè¯¾ç¨‹å­¦ä¹ ï¼šå…¨ç¨‹å…è®¸ã€‚
                curriculum_is_final_stage = False
                if curriculum_enabled and not self.foundation_training_completed and current_stage < len(curriculum_config["stages"]):
                    curriculum_is_final_stage = bool(curriculum_config["stages"][current_stage].get("is_final_stage", False))

                allow_entropy_increase = (not curriculum_enabled) or curriculum_is_final_stage or self.generalization_phase_active
                
                # 2. åªåœ¨å…è®¸ç†µå¢åŠ çš„é˜¶æ®µæ‰ç´¯ç§¯åœæ»è®¡æ•°
                if allow_entropy_increase:
                    self.epochs_without_improvement += 1
                else:
                    # éç†µå¢åŠ é˜¶æ®µï¼Œé‡ç½®è®¡æ•°å™¨ï¼ˆé¿å…ç´¯ç§¯æ— æ„ä¹‰çš„åœæ»ï¼‰
                    self.epochs_without_improvement = 0
                    self.stagnation_level = 0
                
                # 3. è‡ªé€‚åº”ç†µè°ƒæ•´é€»è¾‘
                adaptive_entropy_enabled = ADAPTIVE_ENTROPY_CONFIG["enabled"]
                start_episode = ADAPTIVE_ENTROPY_CONFIG["start_episode"]
                patience = ADAPTIVE_ENTROPY_CONFIG["patience"]
                boost_factor = ADAPTIVE_ENTROPY_CONFIG["boost_factor"]

                # æ­£ç¡®çš„è§¦å‘ç‚¹ï¼šåœ¨ç¬¬ start_episode + patience å›åˆä¹‹åæ‰å¯èƒ½è§¦å‘
                if adaptive_entropy_enabled and allow_entropy_increase and episode >= (start_episode + patience):
                    # å½“å‰çš„å®Œæˆç‡ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦é™ä½ç†µ
                    target_parts_for_entropy = self._get_target_parts(current_curriculum_config)
                    completion_rate_for_entropy = kpi_results['mean_completed_parts'] / (target_parts_for_entropy + 1e-6)

                    # æ£€æŸ¥æ˜¯å¦åœæ»
                    if self.epochs_without_improvement >= patience:
                        self.stagnation_level += 1
                        boost_multiplier = 1.0 + boost_factor * self.stagnation_level
                        self.current_entropy_coeff = min(
                            self.current_entropy_coeff * boost_multiplier,
                            PPO_NETWORK_CONFIG["entropy_coeff"] * 5 # è®¾ç½®ä¸€ä¸ªç¡¬ä¸Šé™ï¼Œä¾‹å¦‚åŸå§‹çš„5å€
                        )
                        print(f"ğŸ“ˆ åœæ»ç­‰çº§ {self.stagnation_level}! æ€§èƒ½å·²åœæ» {self.epochs_without_improvement} å›åˆã€‚")
                        print(f"   é‡‡å–å¼ºåŠ›æªæ–½: å°†ç†µæå‡è‡³ {self.current_entropy_coeff:.4f} (æå‡å› å­: {boost_multiplier:.2f})")
                        # æ ¸å¿ƒä¿®å¤ï¼šé‡ç½®è®¡æ•°å™¨ï¼Œç»™äºˆæ¨¡å‹é€‚åº”æ–°ç†µå€¼çš„çª—å£æœŸ
                        self.epochs_without_improvement = 0
                    
                    # ğŸ”§ ç¼ºé™·å››ä¿®å¤ï¼šä½¿ç”¨é…ç½®åŒ–çš„ç†µè¡°å‡é€»è¾‘
                    elif completion_rate_for_entropy > ADAPTIVE_ENTROPY_CONFIG["high_completion_threshold"]:
                        self.current_entropy_coeff = max(
                            self.current_entropy_coeff * ADAPTIVE_ENTROPY_CONFIG["high_completion_decay"],
                            ADAPTIVE_ENTROPY_CONFIG["min_entropy"]
                        )
                
                # ç¡®ä¿ç†µä¸ä¼šä½äºè®¾å®šçš„æœ€å°å€¼
                self.current_entropy_coeff = max(self.current_entropy_coeff, ADAPTIVE_ENTROPY_CONFIG["min_entropy"])

                
                # ğŸ”§ V36 ç»Ÿä¸€TensorBoardæ—¥å¿—è®°å½•ï¼Œå¹¶æ ¹æ®è¯¾ç¨‹é˜¶æ®µåŠ¨æ€åˆ‡æ¢run
                if TENSORBOARD_AVAILABLE:
                    try:
                        # æ ¹æ®è¯¾ç¨‹é˜¶æ®µåˆ‡æ¢runï¼Œåœ¨æ‚¬åœæç¤ºä¸­æ˜¾ç¤ºé˜¶æ®µå
                        run_name = "train_default" # Fallback run name
                        if curriculum_enabled and current_curriculum_config:
                            # Get stage name and sanitize it for use as a directory name
                            run_name = current_curriculum_config['stage_name'].replace(" ", "_")
                        
                        if self.train_writer is None or self.current_tensorboard_run_name != run_name:
                            if self.train_writer is not None:
                                try:
                                    self.train_writer.flush()  # ğŸ”§ å…³é”®ä¿®å¤ï¼šå…³é—­å‰å…ˆåˆ·æ–°ç¼“å†²
                                    self.train_writer.close()
                                except Exception as e:
                                    print(f"âš ï¸ å…³é—­æ—§TensorBoard writeræ—¶å‡ºé”™: {e}")
                            
                            logdir = os.path.join(self.tensorboard_dir, run_name)
                            self.train_writer = tf.summary.create_file_writer(logdir)
                            self.current_tensorboard_run_name = run_name
                            print(f"ğŸ“Š TensorBoard runå·²åˆ‡æ¢è‡³: '{run_name}' (æ—¥å¿—ç›®å½•: {logdir})")

                        if self.train_writer:
                            with self.train_writer.as_default():
                                # è®­ç»ƒæ ¸å¿ƒæŒ‡æ ‡
                                tf.summary.scalar('Training/Avg_Episode_Reward', episode_reward, step=episode)
                                tf.summary.scalar('Training/Actor_Loss', losses['actor_loss'], step=episode)
                                tf.summary.scalar('Training/Critic_Loss', losses['critic_loss'], step=episode)
                                tf.summary.scalar('Training/Entropy', losses['entropy'], step=episode)
                                tf.summary.scalar('Training/KL_Divergence', losses['approx_kl'], step=episode)
                                tf.summary.scalar('Training/Clip_Fraction', losses['clip_fraction'], step=episode)
                                # æ€§èƒ½æŒ‡æ ‡
                                tf.summary.scalar('Performance/Iteration_Duration', iteration_duration, step=episode)
                                tf.summary.scalar('Performance/CPU_Collection_Time', collect_duration, step=episode)
                                tf.summary.scalar('Performance/GPU_Update_Time', update_duration, step=episode)
                                # ä¸šåŠ¡KPIæŒ‡æ ‡
                                tf.summary.scalar('KPI/Makespan', kpi_results['mean_makespan'], step=episode)
                                tf.summary.scalar('KPI/Completed_Parts', kpi_results['mean_completed_parts'], step=episode)
                                tf.summary.scalar('KPI/Utilization', kpi_results['mean_utilization'], step=episode)
                                tf.summary.scalar('KPI/Tardiness', kpi_results['mean_tardiness'], step=episode)
                                # è®°å½•ç»¼åˆè¯„åˆ†
                                tf.summary.scalar('KPI/Score', current_score, step=episode)
                                
                            # ğŸ”§ å…³é”®ä¿®å¤ï¼šåœ¨withå—å¤–è°ƒç”¨flushï¼Œç¡®ä¿æ•°æ®ç«‹å³å†™å…¥ç£ç›˜
                            self.train_writer.flush()
                    except Exception as e:
                        print(f"âŒ TensorBoardå†™å…¥å¤±è´¥ (å›åˆ{episode}): {e}")
                        import traceback
                        traceback.print_exc()
                
                # --- æ ¸å¿ƒåˆ›æ–°ï¼šæ–°çš„è®­ç»ƒç»“æŸé€»è¾‘ ---
                if training_should_end:
                    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²é€šè¿‡åŸºç¡€è®­ç»ƒå’Œæ³›åŒ–å¼ºåŒ–ä¸¤ä¸ªé˜¶æ®µçš„è®¤è¯ã€‚")
                    break
                
                # æ£€æŸ¥æœ€å¤§è½®æ•°é™åˆ¶
                if episode >= max_episodes - 1:
                    print(f"\nâ° è¾¾åˆ°æœ€å¤§è®­ç»ƒè½®æ•° {max_episodes}ï¼Œè®­ç»ƒç»“æŸã€‚")
                    break
                
                # ğŸ”§ V36 æ–°å¢ï¼šè®°å½•å½“å‰è¯¾ç¨‹é˜¶æ®µä¿¡æ¯ä¾›å…¶ä»–æ–¹æ³•ä½¿ç”¨
                if current_curriculum_config:
                    self._current_orders_scale = current_curriculum_config.get('orders_scale', 1.0)
                
                # ç®€åŒ–çš„æ€§èƒ½ç›‘æ§ï¼Œç§»é™¤å¤æ‚çš„é‡å¯æœºåˆ¶
                # åŸºç¡€æ€§èƒ½è·Ÿè¸ªï¼ˆç”¨äºè°ƒè¯•å’Œç›‘æ§ï¼‰
                current_performance = kpi_results.get('mean_completed_parts', 0)
                if not hasattr(self, '_performance_history'):
                    self._performance_history = []
                
                self._performance_history.append(current_performance)
                # åªä¿ç•™æœ€è¿‘20è½®çš„å†å²
                if len(self._performance_history) > 20:
                    self._performance_history.pop(0)
                

                # æ­£ç¡®æ›´æ–°æœ€ä½³è®°å½•ï¼ˆåªæœ‰å½“makespan > 0æ—¶æ‰æ›´æ–°ï¼‰
                current_makespan = kpi_results['mean_makespan']
                if current_makespan > 0 and current_makespan < best_makespan:
                    best_makespan = current_makespan
                
                # ------------------- ç»Ÿä¸€æ—¥å¿—è¾“å‡ºå¼€å§‹ -------------------
                
                # å‡†å¤‡KPIæ•°æ®ç”¨äºæ—¥å¿—æ˜¾ç¤º
                makespan = kpi_results['mean_makespan']
                completed_parts = kpi_results['mean_completed_parts']
                utilization = kpi_results['mean_utilization']
                tardiness = kpi_results['mean_tardiness']
                # current_score å·²ç»åœ¨å‰é¢é€šè¿‡ _calculate_score è®¡ç®—è¿‡äº†
                
                if not hasattr(self, 'best_score'):
                    self.best_score = float('-inf')

                model_update_info = ""
                timestamp = datetime.now().strftime("%m%d_%H%M") # è·å–å½“å‰æ—¶é—´æˆ³
                # ğŸ”§ æ ¸å¿ƒæ”¹é€ ï¼šåŒºåˆ†"å…¨å±€æœ€ä½³"å’Œ"æœ€ç»ˆé˜¶æ®µæœ€ä½³"
                # 1. æ›´æ–°å…¨å±€æœ€ä½³åˆ†æ•°ï¼ˆç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼‰
                if current_score > self.best_score:
                    self.best_score = current_score

                # === æ ¸å¿ƒé‡æ„ï¼šæ¨¡å‹ä¿å­˜é€»è¾‘ ===
                
                model_update_info = ""
                
                if curriculum_enabled:
                    # --- å¯ç”¨è¯¾ç¨‹å­¦ä¹ æ—¶çš„ä¿å­˜é€»è¾‘ ---
                    if not self.foundation_training_completed:
                        # 1. ä¿å­˜å½“å‰è¯¾ç¨‹é˜¶æ®µçš„æœ€ä½³æ¨¡å‹
                        if current_score > stage_best_scores[current_stage]:
                            stage_best_scores[current_stage] = current_score
                            stage_name = current_curriculum_config['stage_name'].replace(" ", "_")
                            # ğŸ”§ åªåœ¨éœ€è¦ä¿å­˜æ—¶åˆ›å»ºæ—¶é—´æˆ³ç›®å½•
                            timestamp_dir = os.path.join(self.models_dir, timestamp)
                            os.makedirs(timestamp_dir, exist_ok=True)
                            model_path = self.save_model(f"{timestamp_dir}/{timestamp}_{stage_name}_best")
                            if model_path:
                                stage_display_name = current_curriculum_config['stage_name']
                                model_update_info = f"âœ… {stage_display_name}é˜¶æ®µæœ€ä½³! æ¨¡å‹ä¿å­˜è‡³: {model_path}"
                                # ğŸ”§ ä¿®å¤ï¼šåªåœ¨æœ€ç»ˆé˜¶æ®µé‡ç½®åœæ»è®¡æ•°å™¨
                                if curriculum_is_final_stage:
                                    self.epochs_without_improvement = 0
                                    self.stagnation_level = 0
                    elif self.generalization_phase_active:
                        # 2. æ³›åŒ–å¼ºåŒ–é˜¶æ®µçš„æ¨¡å‹ä¿å­˜
                        if current_score > self.best_score_generalization_phase:
                            self.best_score_generalization_phase = current_score
                            self.best_kpi_generalization_phase = kpi_results.copy()
                            self.best_episode_generalization_phase = episode + 1
                            # ğŸ”§ åªåœ¨éœ€è¦ä¿å­˜æ—¶åˆ›å»ºæ—¶é—´æˆ³ç›®å½•
                            timestamp_dir = os.path.join(self.models_dir, timestamp)
                            os.makedirs(timestamp_dir, exist_ok=True)
                            model_path = self.save_model(f"{timestamp_dir}/{timestamp}general_train_best")
                            if model_path:
                                model_update_info = f"ğŸ† æ³›åŒ–å¼ºåŒ–é˜¶æ®µæœ€ä½³! æ¨¡å‹ä¿å­˜è‡³: {model_path}"
                                # ğŸ”§ ä¿®å¤ï¼šæ³›åŒ–é˜¶æ®µä¿å­˜æœ€ä½³æ¨¡å‹æ—¶é‡ç½®åœæ»è®¡æ•°å™¨
                                self.epochs_without_improvement = 0
                                self.stagnation_level = 0
                else:  # curriculum_enabled is False
                    # --- æœªå¯ç”¨è¯¾ç¨‹å­¦ä¹ æ—¶çš„ä¿å­˜é€»è¾‘ ---
                    if not self.foundation_training_completed:
                        # 1. åŸºç¡€è®­ç»ƒé˜¶æ®µçš„æ¨¡å‹ä¿å­˜
                        if current_score > self.best_score_foundation_phase:
                            self.best_score_foundation_phase = current_score
                            self.best_kpi_foundation_phase = kpi_results.copy()
                            self.best_episode_foundation_phase = episode + 1
                            # ğŸ”§ åªåœ¨éœ€è¦ä¿å­˜æ—¶åˆ›å»ºæ—¶é—´æˆ³ç›®å½•
                            timestamp_dir = os.path.join(self.models_dir, timestamp)
                            os.makedirs(timestamp_dir, exist_ok=True)
                            model_path = self.save_model(f"{timestamp_dir}/{timestamp}base_train_best")
                            if model_path:
                                model_update_info = f"âœ… åŸºç¡€è®­ç»ƒé˜¶æ®µæœ€ä½³! æ¨¡å‹ä¿å­˜è‡³: {model_path}"
                                # ğŸ”§ ä¿®å¤ï¼šéè¯¾ç¨‹å­¦ä¹ æ¨¡å¼ä¸‹ï¼ŒåŸºç¡€é˜¶æ®µä¹Ÿå¯ä»¥é‡ç½®ï¼ˆå› ä¸ºallow_entropy_increase=Trueï¼‰
                                self.epochs_without_improvement = 0
                                self.stagnation_level = 0
                    elif self.generalization_phase_active:
                        # 2. æ³›åŒ–å¼ºåŒ–é˜¶æ®µçš„æ¨¡å‹ä¿å­˜
                        if current_score > self.best_score_generalization_phase:
                            self.best_score_generalization_phase = current_score
                            self.best_kpi_generalization_phase = kpi_results.copy()
                            self.best_episode_generalization_phase = episode + 1
                            # ğŸ”§ åªåœ¨éœ€è¦ä¿å­˜æ—¶åˆ›å»ºæ—¶é—´æˆ³ç›®å½•
                            timestamp_dir = os.path.join(self.models_dir, timestamp)
                            os.makedirs(timestamp_dir, exist_ok=True)
                            model_path = self.save_model(f"{timestamp_dir}/{timestamp}general_train_best")
                            if model_path:
                                model_update_info = f"ğŸ† æ³›åŒ–å¼ºåŒ–é˜¶æ®µæœ€ä½³! æ¨¡å‹ä¿å­˜è‡³: {model_path}"
                                # ğŸ”§ ä¿®å¤ï¼šæ³›åŒ–é˜¶æ®µä¿å­˜æœ€ä½³æ¨¡å‹æ—¶é‡ç½®åœæ»è®¡æ•°å™¨
                                self.epochs_without_improvement = 0
                                self.stagnation_level = 0
                
                # 3. å…¨å±€"åŒè¾¾æ ‡"æœ€ä½³æ¨¡å‹ä¿å­˜ï¼ˆç‹¬ç«‹äºæ‰€æœ‰å…¶ä»–é€»è¾‘ï¼‰
                #    é¦–å…ˆï¼Œè·å–å½“å‰å›åˆçš„æ­£ç¡®ç›®æ ‡é›¶ä»¶æ•°
                target_parts_for_dual_check = self._get_target_parts(current_curriculum_config)
                
                completion_rate_kpi = (kpi_results.get('mean_completed_parts', 0) / target_parts_for_dual_check) * 100 if target_parts_for_dual_check > 0 else 0
                
                # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®è¯¾ç¨‹å­¦ä¹ çŠ¶æ€å†³å®šæ˜¯å¦ä¿å­˜"åŒè¾¾æ ‡"æ¨¡å‹
                save_condition_met = False
                if not curriculum_enabled:
                    # æœªå¯ç”¨è¯¾ç¨‹å­¦ä¹ ï¼šå…¨ç¨‹å…è®¸ä¿å­˜
                    save_condition_met = True
                else:
                    # å¯ç”¨è¯¾ç¨‹å­¦ä¹ ï¼šåªåœ¨æœ€ç»ˆé˜¶æ®µæˆ–æ³›åŒ–é˜¶æ®µå…è®¸ä¿å­˜
                    is_final_curriculum_stage = False
                    if not self.foundation_training_completed and current_stage < len(curriculum_config["stages"]):
                        current_stage_info = curriculum_config["stages"][current_stage]
                        is_final_curriculum_stage = current_stage_info.get('is_final_stage', False)
                    
                    if is_final_curriculum_stage or self.generalization_phase_active or curriculum_just_completed:
                        save_condition_met = True
                
                dual_objective_model_update_info = ""
                if save_condition_met and completion_rate_kpi >= 100 and current_score > self.best_score_dual_objective:
                    self.best_score_dual_objective = current_score
                    self.best_kpi_dual_objective = kpi_results.copy()
                    self.best_episode_dual_objective = episode + 1
                    # ğŸ”§ åªåœ¨éœ€è¦ä¿å­˜æ—¶åˆ›å»ºæ—¶é—´æˆ³ç›®å½•
                    timestamp_dir = os.path.join(self.models_dir, timestamp)
                    os.makedirs(timestamp_dir, exist_ok=True)
                    dual_objective_best_path = self.save_model(f"{timestamp_dir}/{timestamp}Twin_best")
                    if dual_objective_best_path:
                        dual_objective_model_update_info = f" â­å®Œæˆæ‰€æœ‰é›¶ä»¶å¾—åˆ†æœ€ä½³!æ¨¡å‹ä¿å­˜è‡³: {dual_objective_best_path}"
                        
                        # ğŸ”§ ä¿®å¤ï¼šåŒè¾¾æ ‡æ¨¡å‹ä¿å­˜æ—¶é‡ç½®åœæ»è®¡æ•°å™¨ï¼ˆå¦‚æœå¤„äºå…è®¸ç†µå¢åŠ çš„é˜¶æ®µï¼‰
                        if allow_entropy_increase:
                            print(f"ğŸ‰ æ–°çš„åŒè¾¾æ ‡æœ€ä½³æ¨¡å‹! é‡ç½®åœæ»è®¡æ•°ã€‚")
                            self.epochs_without_improvement = 0
                            self.stagnation_level = 0  # åˆ›ä¸‹æ–°é«˜ï¼Œ"è­¦æŠ¥"è§£é™¤
                
                # ------------------- ç»Ÿä¸€æ—¥å¿—è¾“å‡ºå¼€å§‹ -------------------

                 # ç¬¬ä¸€è¡Œï¼šå›åˆä¿¡æ¯å’Œæ€§èƒ½æ•°æ®
                # é‡‡é›†ç»Ÿè®¡ï¼ˆå¹¶è¡Œworkerå®Œæˆä¸è¾¾æˆæƒ…å†µï¼‰
                finished_workers = getattr(self, '_last_collect_finished_workers', self.num_workers)
                completed_workers = getattr(self, '_last_collect_completed_workers', 0)
                worker_rewards = getattr(self, '_last_collect_worker_rewards', [])
                
                # ğŸ”§ æ–°å¢ï¼šæ ¼å¼åŒ–æ¯ä¸ªworkerçš„å¥–åŠ±
                if worker_rewards:
                    worker_rewards_str = ", ".join([f"{r:.0f}" for r in worker_rewards])
                else:
                    worker_rewards_str = "N/A"
                
                line1 = (
                    f"ğŸ”‚ è®­ç»ƒå›åˆ {episode + 1:3d}/{max_episodes} | å¹³å‡å¥–åŠ±: {episode_reward:.1f}"
                    f" (æ¯ä¸ªworkerå¥–åŠ±: {worker_rewards_str}, å®Œæˆå…¨éƒ¨: {completed_workers}/{finished_workers})"
                    f" | ActoræŸå¤±: {losses['actor_loss']:.4f}| â±ï¸æœ¬è½®ç”¨æ™‚: {iteration_duration:.1f}s"
                    f" (CPUé‡‡é›†: {collect_duration:.1f}s, GPUæ›´æ–°: {update_duration:.1f}s)"
                )

                # 10-23-20-00 å¦‚å¼€å¯å¤šä»»åŠ¡æ··åˆï¼Œæ˜¾ç¤ºæœ¬å›åˆçš„ä»»åŠ¡ç±»å‹ï¼ˆå›åˆçº§è½®æ¢ï¼‰
                mixing_summary = getattr(self, '_last_collect_mixing_summary', None)
                if mixing_summary and mixing_summary.get('enabled', False):
                    episode_task = mixing_summary.get('episode_task', 'Unknown')
                    avg_reward = mixing_summary.get('avg_reward')
                    avg_reward_str = f"{avg_reward:.1f}" if avg_reward is not None else "N/A"
                    
                    # 10-23-20-00 æ˜¾ç¤ºæœ¬å›åˆä»»åŠ¡ç±»å‹å’ŒåŠ¨æ€äº‹ä»¶é…ç½®
                    task_display = episode_task
                    if self.generalization_phase_active and episode_task != "BASE_ORDERS":
                        task_display += "+åŠ¨æ€äº‹ä»¶"
                    
                    line1 += (
                        f" | æœ¬å›åˆä»»åŠ¡: [{task_display}]Ã—{self.num_workers}workers(å‡å¥–:{avg_reward_str})"
                    )

                # 10-23-20-15 ç¬¬äºŒè¡Œï¼šKPIæ•°æ®å’Œé˜¶æ®µä¿¡æ¯ï¼ˆåŒ…å«å®é™…è¯„ä¼°ç¯å¢ƒé…ç½®ï¼‰
                target_parts_for_log = self._get_target_parts(current_curriculum_config)
                stage_info_str = ""
                if current_curriculum_config and 'stage_name' in current_curriculum_config:
                    stage_name = current_curriculum_config['stage_name']
                    # ğŸ”§ ä¿®å¤ï¼šæ˜¾ç¤ºä¸¤çº§é˜¶æ®µä¿¡æ¯ï¼ˆè¯¾ç¨‹å­¦ä¹ é˜¶æ®µ + åŸºç¡€è®­ç»ƒé˜¶æ®µï¼‰
                    if curriculum_enabled and not curriculum_just_completed:
                        curriculum_stage_name = curriculum_config["stages"][current_stage]['name']
                        foundation_phase = 'åŸºç¡€è®­ç»ƒ' if not self.foundation_training_completed else 'æ³›åŒ–è®­ç»ƒ'
                        stage_info_str = f"   | è¯¾ç¨‹: '{curriculum_stage_name}' | å¤§é˜¶æ®µ: '{foundation_phase}'"
                    else:
                        stage_info_str = f"   | é˜¶æ®µ: '{stage_name}'"
                
                # 10-23-20-15 æ ¸å¿ƒä¿®å¤ï¼šæ˜¾ç¤ºå®é™…è¯„ä¼°ç¯å¢ƒçš„é…ç½®ï¼ˆç¡®ä¿è®­ç»ƒ-è¯„ä¼°ä¸€è‡´ï¼‰
                if hasattr(self, '_last_episode_config') and self._last_episode_config:
                    eval_task = self._last_episode_config['episode_tag']
                    eval_failure = self._last_episode_config['equipment_failure_enabled']
                    eval_emergency = self._last_episode_config['emergency_orders_enabled']
                    
                    # ç»„åˆè¯„ä¼°ç¯å¢ƒæè¿°
                    eval_env_desc = f"è¯„ä¼°ç¯å¢ƒ:[{eval_task}"
                    if eval_failure or eval_emergency:
                        events = []
                        if eval_failure:
                            events.append("æ•…éšœâœ“")
                        if eval_emergency:
                            events.append("æ’å•âœ“")
                        eval_env_desc += f"+{'+'.join(events)}"
                    eval_env_desc += "]"
                    stage_info_str += f" | {eval_env_desc}"
                
                target_parts_str = f"/{target_parts_for_log}"
                line2 = f"ğŸ“Š æ­¤å›åˆKPIè¯„ä¼° - æ€»å®Œå·¥æ—¶é—´: {makespan:.1f}min  | è®¾å¤‡åˆ©ç”¨ç‡: {utilization:.1%} | è®¢å•å»¶æœŸæ—¶é—´: {tardiness:.1f}min |  å®Œæˆé›¶ä»¶æ•°: {completed_parts:.0f}{target_parts_str}{stage_info_str}"

                # ç¬¬ä¸‰è¡Œï¼šè¯„åˆ†å’Œæ¨¡å‹æ›´æ–°ä¿¡æ¯
                phase_best_str = ""
                if curriculum_enabled:
                    # ğŸ”§ ä¿®å¤ï¼šå¯ç”¨è¯¾ç¨‹å­¦ä¹ æ—¶ï¼Œæ˜¾ç¤ºå½“å‰è¯¾ç¨‹é˜¶æ®µçš„æœ€ä½³åˆ†æ•°
                    if not self.foundation_training_completed:
                        stage_display_name = current_curriculum_config.get('stage_name', 'å½“å‰é˜¶æ®µ')
                        stage_best_str = f" ({stage_display_name}æœ€ä½³: {stage_best_scores[current_stage]:.3f})"
                        line3_score = f"ğŸš¥ å›åˆè¯„åˆ†: {current_score:.3f} (å…¨å±€æœ€ä½³: {self.best_score:.3f}){stage_best_str}"
                    elif self.generalization_phase_active:
                        phase_best_str = f" (æ³›åŒ–é˜¶æ®µæœ€ä½³: {self.best_score_generalization_phase:.3f})"
                        line3_score = f"ğŸš¥ å›åˆè¯„åˆ†: {current_score:.3f} (å…¨å±€æœ€ä½³: {self.best_score:.3f}){phase_best_str}"
                else:
                    # ğŸ”§ ä¿®å¤ï¼šæœªå¯ç”¨è¯¾ç¨‹å­¦ä¹ æ—¶ï¼Œæ˜¾ç¤ºåŸºç¡€è®­ç»ƒé˜¶æ®µçš„æœ€ä½³åˆ†æ•°
                    if not self.foundation_training_completed:
                        phase_best_str = f" (åŸºç¡€é˜¶æ®µæœ€ä½³: {self.best_score_foundation_phase:.3f})"
                    elif self.generalization_phase_active:
                        phase_best_str = f" (æ³›åŒ–é˜¶æ®µæœ€ä½³: {self.best_score_generalization_phase:.3f})"
                    line3_score = f"ğŸš¥ å›åˆè¯„åˆ†: {current_score:.3f} (å…¨å±€æœ€ä½³: {self.best_score:.3f}){phase_best_str}"
                
                # åˆå¹¶æ‰€æœ‰æ¨¡å‹æ›´æ–°ä¿¡æ¯
                combined_model_info = model_update_info + dual_objective_model_update_info
                line3 = f"{line3_score}{combined_model_info}" if combined_model_info else line3_score

                avg_time = np.mean(self.iteration_times)
                remaining_episodes = max_episodes - (episode + 1)
                estimated_remaining = remaining_episodes * avg_time
                progress_percent = ((episode + 1) / max_episodes) * 100
                current_time = datetime.now().strftime('%H:%M:%S')
                finish_str = ""
                if remaining_episodes > 0:
                    finish_time = time.time() + estimated_remaining
                    finish_str = time.strftime('%H:%M:%S', time.localtime(finish_time))
                line4 = f"ğŸ”® å½“å‰è®­ç»ƒè¿›åº¦: {progress_percent:.1f}% | å½“å‰æ—¶é—´ï¼š{current_time} | é¢„è®¡å®Œæˆæ—¶é—´: {finish_str}"

                # æ‰“å°æ—¥å¿—
                print(line1)
                print(line2)
                print(line3)
                print(line4)
                print() # æ¯ä¸ªå›åˆåæ·»åŠ ä¸€ä¸ªç©ºè¡Œ
                
                # ------------------- ç»Ÿä¸€æ—¥å¿—è¾“å‡ºç»“æŸ -------------------
                        
            
            # ğŸ”§ ä¿®å¤ç‰ˆï¼šç®€åŒ–çš„è®­ç»ƒå®Œæˆç»Ÿè®¡
            training_end_time = time.time()
            training_end_datetime = datetime.now()
            total_training_time = training_end_time - training_start_time
            
            print("\n" + "=" * 80)
            print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
            print(f"ğŸ• è®­ç»ƒå¼€å§‹: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ğŸ è®­ç»ƒç»“æŸ: {training_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f}åˆ†é’Ÿ ({total_training_time:.1f}ç§’)")
            
            # è®­ç»ƒæ•ˆç‡ç»Ÿè®¡
            if self.iteration_times:
                avg_iteration_time = np.mean(self.iteration_times)
                print(f"âš¡ å¹³å‡æ¯è½®: {avg_iteration_time:.1f}s | è®­ç»ƒæ•ˆç‡: {len(self.iteration_times)/total_training_time*60:.1f}è½®/åˆ†é’Ÿ")

            # ğŸ”§ Bugä¿®å¤ï¼šè¾“å‡ºæœ€ç»ˆçš„ã€å¯é çš„æœ€ä½³KPI
            print("\n" + "="*40)
            print("ğŸ† æœ€ç»ˆæœ€ä½³KPIè¡¨ç° (åŒé‡æ ‡å‡†æœ€ä½³) ğŸ†")
            print("="*40)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹è¾¾åˆ°äº†åŒé‡æ ‡å‡†ï¼Œå¹¶å®ç°ä¼˜é›…é™çº§
            if self.best_episode_dual_objective != -1:
                best_kpi = self.best_kpi_dual_objective
                best_episode_to_report = self.best_episode_dual_objective
            elif self.best_episode_generalization_phase != -1:
                print("âš ï¸ æœªæ‰¾åˆ°åŒé‡æ ‡å‡†æ¨¡å‹ï¼Œå°†æŠ¥å‘Šã€æ³›åŒ–é˜¶æ®µã€‘çš„æœ€ä½³æ¨¡å‹ã€‚")
                best_kpi = self.best_kpi_generalization_phase
                best_episode_to_report = self.best_episode_generalization_phase
            elif self.best_episode_foundation_phase != -1:
                print("âš ï¸ æœªæ‰¾åˆ°åŒé‡æ ‡å‡†æˆ–æ³›åŒ–é˜¶æ®µæ¨¡å‹ï¼Œå°†æŠ¥å‘Šã€åŸºç¡€è®­ç»ƒé˜¶æ®µã€‘çš„æœ€ä½³æ¨¡å‹ã€‚")
                best_kpi = self.best_kpi_foundation_phase
                best_episode_to_report = self.best_episode_foundation_phase
            else:
                print("âš ï¸ æœªèƒ½è®°å½•ä»»ä½•é˜¶æ®µçš„æœ€ä½³æ¨¡å‹ã€‚")
                # ä½¿ç”¨ä¸€ä¸ªç©ºçš„KPIå­—å…¸æ¥é¿å…é”™è¯¯
                best_kpi = self.best_kpi_dual_objective 
                best_episode_to_report = -1

            target_parts_final = get_total_parts_count() # æœ€ç»ˆè¯„ä¼°æ€»æ˜¯åŸºäºå®Œæ•´ä»»åŠ¡
            completion_rate_final = (best_kpi.get('mean_completed_parts', 0) / target_parts_final) * 100 if target_parts_final > 0 else 0
            
            print(f"   (åœ¨ç¬¬ {best_episode_to_report} å›åˆå–å¾—)") # ğŸ”§ æ–°å¢
            print(f"   å®Œæˆé›¶ä»¶: {best_kpi.get('mean_completed_parts', 0):.1f} / {target_parts_final} ({completion_rate_final:.1f}%)")
            print(f"   æ€»å®Œå·¥æ—¶é—´: {best_kpi.get('mean_makespan', 0):.1f} åˆ†é’Ÿ")
            print(f"   è®¾å¤‡åˆ©ç”¨ç‡: {best_kpi.get('mean_utilization', 0):.1%}")
            print(f"   è®¢å•å»¶æœŸæ—¶é—´: {best_kpi.get('mean_tardiness', 0):.1f} åˆ†é’Ÿ")
            print("="*40)
            
            # --- æ ¸å¿ƒä¿®å¤ï¼šè¾“å‡ºæ¯ä¸ªé˜¶æ®µçš„æœ€ä½³KPI ---
            print("\n" + "="*40)
            print("ğŸ† å„é˜¶æ®µæœ€ä½³KPIè¡¨ç° ğŸ†")
            print("="*40)

            # åŸºç¡€è®­ç»ƒé˜¶æ®µæœ€ä½³
            if self.best_episode_foundation_phase != -1:
                print("\n--- åŸºç¡€è®­ç»ƒé˜¶æ®µ ---")
                best_kpi = self.best_kpi_foundation_phase
                target_parts = get_total_parts_count()
                completion_rate = (best_kpi.get('mean_completed_parts', 0) / target_parts) * 100 if target_parts > 0 else 0
                print(f"   (åœ¨ç¬¬ {self.best_episode_foundation_phase} å›åˆå–å¾—)")
                print(f"   å®Œæˆé›¶ä»¶: {best_kpi.get('mean_completed_parts', 0):.1f} / {target_parts} ({completion_rate:.1f}%)")
                print(f"   æ€»å®Œå·¥æ—¶é—´: {best_kpi.get('mean_makespan', 0):.1f} åˆ†é’Ÿ")
                print(f"   è®¾å¤‡åˆ©ç”¨ç‡: {best_kpi.get('mean_utilization', 0):.1%}")
                print(f"   è®¢å•å»¶æœŸæ—¶é—´: {best_kpi.get('mean_tardiness', 0):.1f} åˆ†é’Ÿ")
                print(f"   ç»¼åˆè¯„åˆ†: {self.best_score_foundation_phase:.3f}")

            # æ³›åŒ–å¼ºåŒ–é˜¶æ®µæœ€ä½³
            if self.best_episode_generalization_phase != -1:
                print("\n--- æ³›åŒ–å¼ºåŒ–é˜¶æ®µ ---")
                best_kpi = self.best_kpi_generalization_phase
                # æ³¨æ„ï¼šæ³›åŒ–é˜¶æ®µçš„ç›®æ ‡é›¶ä»¶æ•°æ˜¯åŠ¨æ€çš„ï¼Œæ­¤å¤„ä»…ä¸ºå‚è€ƒ
                print(f"   (åœ¨ç¬¬ {self.best_episode_generalization_phase} å›åˆå–å¾—)")
                print(f"   å®Œæˆé›¶ä»¶: {best_kpi.get('mean_completed_parts', 0):.1f}")
                print(f"   æ€»å®Œå·¥æ—¶é—´: {best_kpi.get('mean_makespan', 0):.1f} åˆ†é’Ÿ")
                print(f"   è®¾å¤‡åˆ©ç”¨ç‡: {best_kpi.get('mean_utilization', 0):.1%}")
                print(f"   è®¢å•å»¶æœŸæ—¶é—´: {best_kpi.get('mean_tardiness', 0):.1f} åˆ†é’Ÿ")
                print(f"   ç»¼åˆè¯„åˆ†: {self.best_score_generalization_phase:.3f}")
            
            # æ–°å¢ï¼šå¦‚æœå¯ç”¨äº†è¯¾ç¨‹å­¦ä¹ ï¼Œåˆ™å±•ç¤ºæ¯ä¸ªè¯¾ç¨‹é˜¶æ®µçš„æœ€ä½³åˆ†æ•°
            if curriculum_enabled:
                 print("\n--- è¯¾ç¨‹å­¦ä¹ å„é˜¶æ®µæœ€ä½³åˆ†æ•° ---")
                 for i, score in enumerate(stage_best_scores):
                     if score > -np.inf:
                         stage_name = curriculum_config["stages"][i]['name']
                         print(f"   é˜¶æ®µ '{stage_name}': {score:.3f}")
                     else:
                         stage_name = curriculum_config["stages"][i]['name']
                         print(f"   é˜¶æ®µ '{stage_name}': æœªè®°å½•æœ€ä½³åˆ†æ•°")


            # æœ€ç»ˆé»„é‡‘æ ‡å‡†ï¼šåŒè¾¾æ ‡æ¨¡å‹
            print("\n" + "="*40)
            print("â­ æœ€ç»ˆé»„é‡‘æ ‡å‡†æ¨¡å‹ (å®Œæˆæ‰€æœ‰é›¶ä»¶ä¸”å¾—åˆ†æœ€é«˜) â­")
            print("="*40)
            
            if self.best_episode_dual_objective != -1:
                best_kpi = self.best_kpi_dual_objective
                best_episode_to_report = self.best_episode_dual_objective
                
                # åœ¨åŒè¾¾æ ‡çš„æƒ…å†µä¸‹ï¼Œç›®æ ‡é›¶ä»¶æ•°æ˜¯ç¡®å®šçš„
                target_parts_final = get_total_parts_count()
                completion_rate_final = (best_kpi.get('mean_completed_parts', 0) / target_parts_final) * 100 if target_parts_final > 0 else 0
            
                print(f"   (åœ¨ç¬¬ {best_episode_to_report} å›åˆå–å¾—)") 
                print(f"   å®Œæˆé›¶ä»¶: {best_kpi.get('mean_completed_parts', 0):.1f} / {target_parts_final} ({completion_rate_final:.1f}%)")
                print(f"   æ€»å®Œå·¥æ—¶é—´: {best_kpi.get('mean_makespan', 0):.1f} åˆ†é’Ÿ")
                print(f"   è®¾å¤‡åˆ©ç”¨ç‡: {best_kpi.get('mean_utilization', 0):.1%}")
                print(f"   è®¢å•å»¶æœŸæ—¶é—´: {best_kpi.get('mean_tardiness', 0):.1f} åˆ†é’Ÿ")
                print(f"   ç»¼åˆè¯„åˆ†: {self.best_score_dual_objective:.3f}")
            else:
                print("   âš ï¸ æœ¬æ¬¡è®­ç»ƒæœªäº§ç”Ÿæ»¡è¶³'å®Œæˆæ‰€æœ‰é›¶ä»¶'æ¡ä»¶çš„æœ€ä½³æ¨¡å‹ã€‚")

            print("="*40)
            
            return {
                'training_time': total_training_time,
                'kpi_history': self.kpi_history,
                'iteration_times': self.iteration_times,
                'best_kpi': best_kpi
            }
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None
        
        finally:
            # 10-27-16-30 è®­ç»ƒæ”¶å°¾ï¼šä¼˜é›…å…³é—­è¿›ç¨‹æ± ï¼Œé‡Šæ”¾ç³»ç»Ÿèµ„æº
            try:
                if hasattr(self, 'pool') and self.pool:
                    self.pool.shutdown(wait=True)
            except Exception:
                pass
    
    def save_model(self, filepath: str) -> str:
        """
        ä¿å­˜æ¨¡å‹ - TensorFlow 2.15.0 å…¼å®¹ç‰ˆæœ¬
        ä½¿ç”¨å¤šæ ¼å¼å†—ä½™ä¿å­˜ï¼Œç¡®ä¿è·¨ç‰ˆæœ¬å…¼å®¹æ€§
        """
        import json
        import os
        import warnings
        from datetime import datetime
        
        # å±è”½ç‰¹å®šçš„TensorFlowè­¦å‘Šï¼ˆåªå±è”½compile_metricså’ŒHDF5 legacyè­¦å‘Šï¼‰
        warnings.filterwarnings('ignore', message='.*compile_metrics.*')
        warnings.filterwarnings('ignore', message='.*HDF5 file.*legacy.*')
        
        # ç¡®å®šåŸºç¡€è·¯å¾„ï¼ˆç§»é™¤æ‰©å±•åï¼‰
        base_path = filepath.replace('.keras', '').replace('.h5', '')
        
        # è®°å½•ä¿å­˜çŠ¶æ€
        saved_formats = []
        failed_formats = []
        
        try:
            # ç­–ç•¥1ï¼šä¿å­˜ä¸ºH5æ ¼å¼ï¼ˆæœ€ç¨³å®šï¼Œå…¼å®¹æ€§æœ€å¥½ï¼‰
            try:
                actor_h5_path = f"{base_path}_actor.h5"
                self.shared_network.actor.save(actor_h5_path, save_format='h5')
                critic_h5_path = f"{base_path}_critic.h5"
                self.shared_network.critic.save(critic_h5_path, save_format='h5')
                saved_formats.append("H5")
            except Exception as e:
                failed_formats.append(f"H5({str(e)[:30]})")
            
            # ç­–ç•¥2ï¼šä¿å­˜æƒé‡ä¸ºç‹¬ç«‹çš„H5æ–‡ä»¶ï¼ˆä½œä¸ºå¤‡ä»½ï¼‰
            try:
                actor_weights_path = f"{base_path}_actor_weights.h5"
                self.shared_network.actor.save_weights(actor_weights_path)
                critic_weights_path = f"{base_path}_critic_weights.h5"
                self.shared_network.critic.save_weights(critic_weights_path)
                saved_formats.append("Weights")
            except Exception as e:
                failed_formats.append(f"Weights({str(e)[:30]})")
            
            # ç­–ç•¥3ï¼šä¿å­˜å…ƒæ•°æ®ä¸ºJSONï¼ˆå…³é”®ï¼ç”¨äºé‡å»ºæ¨¡å‹ï¼‰
            try:
                is_multidiscrete = self.shared_network.is_multidiscrete
                
                meta = {
                    'state_dim': int(self.state_dim),
                    'action_space': {
                        'type': 'MultiDiscrete' if is_multidiscrete else 'Discrete',
                        'nvec': [int(x) for x in self.action_space.nvec] if is_multidiscrete else None,
                        'n': int(self.action_space.n) if not is_multidiscrete else None
                    },
                    'global_state_dim': int(self.global_state_dim),
                    'network_config': self.shared_network.config,
                    'num_agents': len(self.agent_ids),
                    'tensorflow_version': tf.__version__,
                    'save_timestamp': datetime.now().isoformat()
                }
                
                meta_path = f"{base_path}_meta.json"
                with open(meta_path, 'w', encoding='utf-8') as f:
                    json.dump(meta, f, indent=2, ensure_ascii=False)
                saved_formats.append("Meta")
            except Exception as e:
                failed_formats.append(f"Meta({str(e)[:30]})")
            
            # ç­–ç•¥4ï¼šå°è¯•ä¿å­˜ä¸º.kerasæ ¼å¼ï¼ˆTF 2.15+æ›´ç¨³å®šï¼‰
            try:
                keras_actor_path = f"{base_path}_actor.keras"
                self.shared_network.actor.save(keras_actor_path, save_format='keras')
                keras_critic_path = f"{base_path}_critic.keras"
                self.shared_network.critic.save(keras_critic_path, save_format='keras')
                saved_formats.append("Keras")
            except Exception as e:
                failed_formats.append(f"Keras({str(e)[:30]})")
            
            # ç»Ÿä¸€è¾“å‡ºï¼šç®€æ´ç‰ˆ
            if len(saved_formats) == 4:
                # æ‰€æœ‰æ ¼å¼éƒ½æˆåŠŸ - åªè¾“å‡ºä¸€è¡Œ
                print(f"âœ… 4ç§æ ¼å¼æ¨¡å‹å·²ä¿å­˜è‡³: {base_path}_*")
            else:
                # æœ‰å¤±è´¥çš„æ ¼å¼ - è¾“å‡ºè¯¦ç»†ä¿¡æ¯
                if saved_formats:
                    print(f"âœ… å·²ä¿å­˜ {len(saved_formats)}/4 ç§æ ¼å¼: {', '.join(saved_formats)}")
                if failed_formats:
                    print(f"âš ï¸ ä¿å­˜å¤±è´¥: {', '.join(failed_formats)}")
            
            # è¿”å›H5è·¯å¾„ä½œä¸ºä¸»è¦åŠ è½½ç›®æ ‡
            actor_h5_path = f"{base_path}_actor.h5"
            return actor_h5_path if "H5" in saved_formats else ""
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹ä¿å­˜è¿‡ç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return ""
        finally:
            # æ¢å¤è­¦å‘Šè®¾ç½®
            warnings.filterwarnings('default', message='.*compile_metrics.*')
            warnings.filterwarnings('default', message='.*HDF5 file.*legacy.*')

    def _get_target_parts(self, curriculum_config: Optional[Dict]) -> int:
        """10-23-20-15 ä¿®å¤ç‰ˆï¼šç»Ÿä¸€è·å–å½“å‰å›åˆçš„ç›®æ ‡é›¶ä»¶æ•°ï¼Œä¼˜å…ˆä½¿ç”¨å®é™…è®­ç»ƒé…ç½®"""
        # 10-23-20-15 ä¼˜å…ˆä½¿ç”¨ä¸Šä¸€ä¸ªè®­ç»ƒå›åˆçš„å®é™…è®¢å•é…ç½®
        if hasattr(self, '_last_episode_config') and self._last_episode_config:
            custom_orders = self._last_episode_config.get('custom_orders')
            if custom_orders:
                return get_total_parts_count(custom_orders)
        
        # å¤‡ç”¨é€»è¾‘ï¼šä»curriculum_configè·å–
        if curriculum_config and 'custom_orders' in curriculum_config:
            # æ³›åŒ–é˜¶æ®µæˆ–è‡ªå®šä¹‰è®¢å•
            return get_total_parts_count(curriculum_config['custom_orders'])
        elif curriculum_config and 'orders_scale' in curriculum_config:
            # è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ
            base_parts = get_total_parts_count()
            return int(base_parts * curriculum_config['orders_scale'])
        else:
            # é»˜è®¤æˆ–åŸºç¡€è®­ç»ƒé˜¶æ®µ
            return get_total_parts_count()

    def check_curriculum_stage_graduation(self, kpi_results: Dict[str, float], current_score: float, stage_config: Dict[str, Any]) -> bool:
        """æ£€æŸ¥å½“å‰è¯¾ç¨‹å­¦ä¹ é˜¶æ®µæ˜¯å¦è¾¾åˆ°æ¯•ä¸šæ ‡å‡†"""
        criteria = stage_config.get("graduation_criteria")
        if not criteria:
            return False # å¦‚æœæ²¡æœ‰å®šä¹‰æ ‡å‡†ï¼Œåˆ™æ— æ³•æ¯•ä¸š

        # è·å–å½“å‰é˜¶æ®µçš„ç›®æ ‡é›¶ä»¶æ•°
        target_parts = int(get_total_parts_count() * stage_config.get('orders_scale', 1.0))
        completion_rate_kpi = (kpi_results.get('mean_completed_parts', 0) / target_parts) * 100 if target_parts > 0 else 0
        completion_rate_kpi = float(min(100.0, completion_rate_kpi))
        
        target_score = criteria["target_score"]
        stability_goal = criteria["target_consistency"]
        min_completion_rate = criteria["min_completion_rate"]
        # æ–°å¢ï¼šå¤„ç†å»¶æœŸé˜ˆå€¼
        tardiness_threshold = criteria.get("tardiness_threshold")
        current_tardiness = kpi_results.get('mean_tardiness', float('inf'))

        conditions_met = {
            f"å®Œæˆç‡(>={min_completion_rate}%)": completion_rate_kpi >= min_completion_rate,
            f"åˆ†æ•°(>={target_score})": current_score >= target_score,
        }
        
        if tardiness_threshold is not None:
            conditions_met[f"å»¶æœŸ(<={tardiness_threshold}min)"] = current_tardiness <= tardiness_threshold

        if all(conditions_met.values()):
            self.curriculum_stage_achievement_count += 1
            print(f"[CURRICULUM] é˜¶æ®µ '{stage_config['name']}' è¾¾æ ‡: å®Œæˆç‡ {completion_rate_kpi:.1f}%, åˆ†æ•° {current_score:.3f} (è¿ç»­ç¬¬{self.curriculum_stage_achievement_count}/{stability_goal}æ¬¡)")
        else:
            if self.curriculum_stage_achievement_count > 0:
                reasons = [k for k, v in conditions_met.items() if not v]
                print(f"[CURRICULUM] é˜¶æ®µ '{stage_config['name']}' è¿ç»­è¾¾æ ‡ä¸­æ–­. æœªè¾¾æ ‡é¡¹: {', '.join(reasons)}")
            self.curriculum_stage_achievement_count = 0

        return self.curriculum_stage_achievement_count >= stability_goal

