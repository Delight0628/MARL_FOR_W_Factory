"""
çœŸæ­£çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒè„šæœ¬
ä½¿ç”¨ç®€åŒ–çš„PPOå®ç°ï¼Œé¿å…Rayçš„Windowså…¼å®¹æ€§é—®é¢˜
å®ç°çœŸæ­£çš„ååŒå­¦ä¹ å’Œç­–ç•¥å…±äº«
å¢å¼ºç‰ˆï¼šæ”¯æŒé€’è¿›å¼è®­ç»ƒã€åŠ¨æ€äº‹ä»¶ã€è¯¦ç»†è¯„ä¼°æŒ‡æ ‡
"""

import os
import sys
import time
import json
import random
import numpy as np
import tensorflow as tf
from collections import defaultdict, deque
from typing import Dict, List, Tuple, Any, Optional

# è®¾ç½®TensorFlowæ—¥å¿—çº§åˆ«
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.get_logger().setLevel('ERROR')

# æ·»åŠ ç¯å¢ƒè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from environments.w_factory_env import make_parallel_env
from environments.w_factory_config import *

class PPONetwork:
    """ç®€åŒ–çš„PPOç½‘ç»œå®ç°"""
    
    def __init__(self, state_dim: int, action_dim: int, lr: float = 3e-4):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.lr = lr
        
        # æ„å»ºç½‘ç»œ
        self.actor, self.critic = self._build_networks()
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = tf.keras.optimizers.Adam(lr)
        self.critic_optimizer = tf.keras.optimizers.Adam(lr)
        
    def _build_networks(self):
        """æ„å»ºActor-Criticç½‘ç»œ"""
        # Actorç½‘ç»œ (ç­–ç•¥ç½‘ç»œ)
        actor_input = tf.keras.layers.Input(shape=(self.state_dim,))
        actor_hidden1 = tf.keras.layers.Dense(256, activation='relu')(actor_input)
        actor_hidden2 = tf.keras.layers.Dense(256, activation='relu')(actor_hidden1)
        actor_output = tf.keras.layers.Dense(self.action_dim, activation='softmax')(actor_hidden2)
        actor = tf.keras.Model(inputs=actor_input, outputs=actor_output)
        
        # Criticç½‘ç»œ (ä»·å€¼ç½‘ç»œ)
        critic_input = tf.keras.layers.Input(shape=(self.state_dim,))
        critic_hidden1 = tf.keras.layers.Dense(256, activation='relu')(critic_input)
        critic_hidden2 = tf.keras.layers.Dense(256, activation='relu')(critic_hidden1)
        critic_output = tf.keras.layers.Dense(1)(critic_hidden2)
        critic = tf.keras.Model(inputs=critic_input, outputs=critic_output)
        
        return actor, critic
    
    def get_action_and_value(self, state: np.ndarray) -> Tuple[int, float, float]:
        """è·å–åŠ¨ä½œã€åŠ¨ä½œæ¦‚ç‡å’ŒçŠ¶æ€ä»·å€¼"""
        state = tf.expand_dims(state, 0)
        
        # è·å–åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
        action_probs = self.actor(state)
        action_dist = tf.random.categorical(tf.math.log(action_probs), 1)
        action = int(action_dist[0, 0])
        
        # è·å–åŠ¨ä½œæ¦‚ç‡
        action_prob = float(action_probs[0, action])
        
        # è·å–çŠ¶æ€ä»·å€¼
        value = float(self.critic(state)[0, 0])
        
        return action, action_prob, value
    
    def get_value(self, state: np.ndarray) -> float:
        """è·å–çŠ¶æ€ä»·å€¼"""
        state = tf.expand_dims(state, 0)
        return float(self.critic(state)[0, 0])
    
    def update(self, states: np.ndarray, actions: np.ndarray, 
               old_probs: np.ndarray, advantages: np.ndarray, 
               returns: np.ndarray, clip_ratio: float = 0.2) -> Dict[str, float]:
        """PPOæ›´æ–°"""
        
        # Actoræ›´æ–°
        with tf.GradientTape() as tape:
            action_probs = self.actor(states)
            action_probs_selected = tf.reduce_sum(
                action_probs * tf.one_hot(actions, self.action_dim), axis=1
            )
            
            # è®¡ç®—æ¯”ç‡
            ratio = action_probs_selected / (old_probs + 1e-8)
            
            # PPOè£å‰ªç›®æ ‡
            clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)
            actor_loss = -tf.reduce_mean(
                tf.minimum(ratio * advantages, clipped_ratio * advantages)
            )
            
            # ç†µæ­£åˆ™åŒ–
            entropy = -tf.reduce_sum(action_probs * tf.math.log(action_probs + 1e-8), axis=1)
            actor_loss -= 0.01 * tf.reduce_mean(entropy)
        
        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)
        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))
        
        # Criticæ›´æ–°
        with tf.GradientTape() as tape:
            values = tf.squeeze(self.critic(states))
            critic_loss = tf.reduce_mean(tf.square(returns - values))
        
        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)
        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))
        
        return {
            'actor_loss': float(actor_loss),
            'critic_loss': float(critic_loss),
            'entropy': float(tf.reduce_mean(entropy))
        }

class ExperienceBuffer:
    """ç»éªŒç¼“å†²åŒº"""
    
    def __init__(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.action_probs = []
        self.dones = []
        
    def store(self, state, action, reward, value, action_prob, done):
        """å­˜å‚¨ç»éªŒ"""
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.values.append(value)
        self.action_probs.append(action_prob)
        self.dones.append(done)
    
    def get_batch(self, gamma=0.99, lam=0.95):
        """è·å–æ‰¹æ¬¡æ•°æ®å¹¶è®¡ç®—ä¼˜åŠ¿å‡½æ•°"""
        states = np.array(self.states)
        actions = np.array(self.actions)
        rewards = np.array(self.rewards)
        values = np.array(self.values)
        action_probs = np.array(self.action_probs)
        dones = np.array(self.dones)
        
        # è®¡ç®—GAEä¼˜åŠ¿å‡½æ•°
        advantages = np.zeros_like(rewards)
        last_advantage = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
            advantages[t] = delta + gamma * lam * (1 - dones[t]) * last_advantage
            last_advantage = advantages[t]
        
        # è®¡ç®—å›æŠ¥
        returns = advantages + values
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿å‡½æ•°
        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)
        
        return states, actions, action_probs, advantages, returns
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.values.clear()
        self.action_probs.clear()
        self.dones.clear()

class EnhancedMARLTrainer:
    """å¢å¼ºç‰ˆå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        
        # å…±äº«ç­–ç•¥ç½‘ç»œ (æ‰€æœ‰æ™ºèƒ½ä½“å…±äº«åŒä¸€ä¸ªç½‘ç»œ)
        self.shared_network = PPONetwork(
            state_dim=2,  # [é˜Ÿåˆ—é•¿åº¦, è®¾å¤‡çŠ¶æ€]
            action_dim=2,  # [IDLE, PROCESS]
            lr=self.config.get('lr', 3e-4)
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_lengths = []
        self.training_losses = []
        self.evaluation_history = []
        
        # æ€§èƒ½æŒ‡æ ‡å†å²
        self.makespan_history = []
        self.tardiness_history = []
        self.utilization_history = []
        
    def create_environment(self, enable_dynamic_events: bool = False):
        """åˆ›å»ºç¯å¢ƒï¼ˆæ”¯æŒåŠ¨æ€äº‹ä»¶å¼€å…³ï¼‰"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®enable_dynamic_eventså‚æ•°åˆ›å»ºä¸åŒé…ç½®çš„ç¯å¢ƒ
        env = make_parallel_env()
        
        # ç»éªŒç¼“å†²åŒº (æ¯ä¸ªæ™ºèƒ½ä½“ä¸€ä¸ª)
        buffers = {
            agent: ExperienceBuffer() 
            for agent in env.possible_agents
        }
        
        return env, buffers
    
    def collect_experience(self, env, buffers, num_steps: int = 200) -> Dict[str, float]:
        """æ”¶é›†ç»éªŒ"""
        observations, _ = env.reset()
        episode_rewards = {agent: 0 for agent in env.possible_agents}
        step_count = 0
        
        for step in range(num_steps):
            # è·å–æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œ
            actions = {}
            values = {}
            action_probs = {}
            
            for agent in env.agents:
                if agent in observations:
                    action, action_prob, value = self.shared_network.get_action_and_value(
                        observations[agent]
                    )
                    actions[agent] = action
                    values[agent] = value
                    action_probs[agent] = action_prob
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_observations, rewards, terminations, truncations, _ = env.step(actions)
            
            # å­˜å‚¨ç»éªŒ
            for agent in env.agents:
                if agent in observations and agent in actions:
                    done = terminations.get(agent, False) or truncations.get(agent, False)
                    reward = rewards.get(agent, 0)
                    
                    buffers[agent].store(
                        state=observations[agent],
                        action=actions[agent],
                        reward=reward,
                        value=values[agent],
                        action_prob=action_probs[agent],
                        done=done
                    )
                    
                    episode_rewards[agent] += reward
            
            observations = next_observations
            step_count += 1
            
            # æ£€æŸ¥æ˜¯å¦ç»“æŸ
            if any(terminations.values()) or any(truncations.values()):
                observations, _ = env.reset()
        
        return episode_rewards
    
    def update_policy(self, buffers) -> Dict[str, float]:
        """æ›´æ–°ç­–ç•¥"""
        # åˆå¹¶æ‰€æœ‰æ™ºèƒ½ä½“çš„ç»éªŒ
        all_states = []
        all_actions = []
        all_action_probs = []
        all_advantages = []
        all_returns = []
        
        for agent, buffer in buffers.items():
            if len(buffer.states) > 0:
                states, actions, action_probs, advantages, returns = buffer.get_batch()
                
                all_states.extend(states)
                all_actions.extend(actions)
                all_action_probs.extend(action_probs)
                all_advantages.extend(advantages)
                all_returns.extend(returns)
                
                buffer.clear()
        
        if len(all_states) == 0:
            return {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0}
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        all_states = np.array(all_states)
        all_actions = np.array(all_actions)
        all_action_probs = np.array(all_action_probs)
        all_advantages = np.array(all_advantages)
        all_returns = np.array(all_returns)
        
        # å¤šæ¬¡æ›´æ–°
        losses = {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0}
        num_updates = 5
        
        for _ in range(num_updates):
            batch_losses = self.shared_network.update(
                states=all_states,
                actions=all_actions,
                old_probs=all_action_probs,
                advantages=all_advantages,
                returns=all_returns
            )
            
            for key in losses:
                losses[key] += batch_losses[key] / num_updates
        
        return losses
    
    def static_training(self, num_episodes: int = 50, steps_per_episode: int = 200):
        """é™æ€ç¯å¢ƒè®­ç»ƒï¼ˆç¬¬ä¸€é˜¶æ®µï¼‰"""
        print("=" * 60)
        print("é˜¶æ®µ1: é™æ€ç¯å¢ƒè®­ç»ƒ (åŸºç¡€ååŒè°ƒåº¦å­¦ä¹ )")
        print("=" * 60)
        print(f"ç®—æ³•: PPO (Proximal Policy Optimization)")
        print(f"ç½‘ç»œ: å…±äº«Actor-Criticç½‘ç»œ")
        print(f"è®­ç»ƒå›åˆ: {num_episodes}")
        print(f"æ¯å›åˆæ­¥æ•°: {steps_per_episode}")
        print("ç‰¹ç‚¹: æ— åŠ¨æ€äº‹ä»¶ï¼Œä¸“æ³¨å­¦ä¹ åŸºç¡€è°ƒåº¦é€»è¾‘")
        print("=" * 60)
        
        # åˆ›å»ºé™æ€ç¯å¢ƒ
        env, buffers = self.create_environment(enable_dynamic_events=False)
        
        start_time = time.time()
        
        for episode in range(num_episodes):
            episode_start = time.time()
            
            # æ”¶é›†ç»éªŒ
            episode_rewards = self.collect_experience(env, buffers, steps_per_episode)
            
            # æ›´æ–°ç­–ç•¥
            losses = self.update_policy(buffers)
            
            # è®°å½•ç»Ÿè®¡
            total_reward = sum(episode_rewards.values())
            self.episode_rewards.append(total_reward)
            self.episode_lengths.append(steps_per_episode)
            self.training_losses.append(losses)
            
            episode_time = time.time() - episode_start
            
            # è¾“å‡ºæ—¥å¿—
            if (episode + 1) % 10 == 0:
                recent_rewards = self.episode_rewards[-10:]
                avg_reward = np.mean(recent_rewards)
                
                print(f"å›åˆ {episode + 1:4d}/{num_episodes} | "
                      f"å¥–åŠ±: {total_reward:8.2f} | "
                      f"å¹³å‡å¥–åŠ±: {avg_reward:8.2f} | "
                      f"ActoræŸå¤±: {losses['actor_loss']:.4f} | "
                      f"CriticæŸå¤±: {losses['critic_loss']:.4f} | "
                      f"æ—¶é—´: {episode_time:.2f}s")
        
        training_time = time.time() - start_time
        
        print(f"\nâœ… é™æ€è®­ç»ƒå®Œæˆï¼è®­ç»ƒæ—¶é—´: {training_time/60:.2f} åˆ†é’Ÿ")
        print(f"å¹³å‡å¥–åŠ±: {np.mean(self.episode_rewards):.2f}")
        
        return {
            'phase': 'static',
            'training_time': training_time,
            'episode_rewards': self.episode_rewards.copy(),
            'avg_reward': np.mean(self.episode_rewards)
        }
    
    def dynamic_training(self, num_episodes: int = 30, steps_per_episode: int = 200):
        """åŠ¨æ€ç¯å¢ƒè®­ç»ƒï¼ˆç¬¬äºŒé˜¶æ®µï¼‰- å¾®è°ƒ"""
        print("\n" + "=" * 60)
        print("é˜¶æ®µ2: åŠ¨æ€ç¯å¢ƒå¾®è°ƒ (é²æ£’æ€§å¢å¼º)")
        print("=" * 60)
        print(f"åŸºäºé™æ€è®­ç»ƒç»“æœè¿›è¡Œå¾®è°ƒ")
        print(f"å¾®è°ƒå›åˆ: {num_episodes}")
        print(f"ç‰¹ç‚¹: å¼•å…¥è®¾å¤‡æ•…éšœã€ç´§æ€¥æ’å•ç­‰åŠ¨æ€äº‹ä»¶")
        print("=" * 60)
        
        # åˆ›å»ºåŠ¨æ€ç¯å¢ƒ
        env, buffers = self.create_environment(enable_dynamic_events=True)
        
        # é™ä½å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒ
        original_lr = self.shared_network.lr
        fine_tune_lr = original_lr * 0.1
        self.shared_network.actor_optimizer.learning_rate = fine_tune_lr
        self.shared_network.critic_optimizer.learning_rate = fine_tune_lr
        
        print(f"å¾®è°ƒå­¦ä¹ ç‡: {fine_tune_lr}")
        
        start_time = time.time()
        dynamic_rewards = []
        
        for episode in range(num_episodes):
            episode_start = time.time()
            
            # æ”¶é›†ç»éªŒ
            episode_rewards = self.collect_experience(env, buffers, steps_per_episode)
            
            # æ›´æ–°ç­–ç•¥
            losses = self.update_policy(buffers)
            
            # è®°å½•ç»Ÿè®¡
            total_reward = sum(episode_rewards.values())
            dynamic_rewards.append(total_reward)
            self.episode_rewards.append(total_reward)
            self.training_losses.append(losses)
            
            episode_time = time.time() - episode_start
            
            # è¾“å‡ºæ—¥å¿—
            if (episode + 1) % 5 == 0:
                recent_rewards = dynamic_rewards[-5:]
                avg_reward = np.mean(recent_rewards)
                
                print(f"å¾®è°ƒ {episode + 1:3d}/{num_episodes} | "
                      f"å¥–åŠ±: {total_reward:8.2f} | "
                      f"å¹³å‡å¥–åŠ±: {avg_reward:8.2f} | "
                      f"ActoræŸå¤±: {losses['actor_loss']:.4f} | "
                      f"æ—¶é—´: {episode_time:.2f}s")
        
        training_time = time.time() - start_time
        
        # æ¢å¤åŸå§‹å­¦ä¹ ç‡
        self.shared_network.actor_optimizer.learning_rate = original_lr
        self.shared_network.critic_optimizer.learning_rate = original_lr
        
        print(f"\nâœ… åŠ¨æ€å¾®è°ƒå®Œæˆï¼å¾®è°ƒæ—¶é—´: {training_time/60:.2f} åˆ†é’Ÿ")
        print(f"å¾®è°ƒå¹³å‡å¥–åŠ±: {np.mean(dynamic_rewards):.2f}")
        
        return {
            'phase': 'dynamic',
            'training_time': training_time,
            'episode_rewards': dynamic_rewards,
            'avg_reward': np.mean(dynamic_rewards)
        }
    
    def comprehensive_evaluation(self, num_episodes: int = 20) -> Dict[str, Any]:
        """å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print(f"\n" + "=" * 60)
        print(f"å…¨é¢æ€§èƒ½è¯„ä¼° ({num_episodes} å›åˆ)")
        print("=" * 60)
        print("è¯„ä¼°æŒ‡æ ‡:")
        print("  â€¢ æœ€å¤§å®Œå·¥æ—¶é—´ (Makespan)")
        print("  â€¢ è®¾å¤‡å¹³å‡åˆ©ç”¨ç‡ (Equipment Utilization)")
        print("  â€¢ æœ€å¤§å»¶æœŸæ—¶é—´ (Max Tardiness)")
        print("  â€¢ æ€»å»¶æœŸæ—¶é—´ (Total Tardiness)")
        print("  â€¢ å®Œæˆé›¶ä»¶æ•°é‡")
        print("=" * 60)
        
        # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
        env, _ = self.create_environment(enable_dynamic_events=False)
        
        eval_results = {
            'episode_rewards': [],
            'makespans': [],
            'total_tardiness': [],
            'max_tardiness': [],
            'completed_parts': [],
            'utilizations': [],
            'detailed_stats': []
        }
        
        for episode in range(num_episodes):
            observations, _ = env.reset()
            episode_reward = 0
            step_count = 0
            
            while step_count < 480:  # æœ€å¤§ä»¿çœŸæ—¶é—´
                actions = {}
                for agent in env.agents:
                    if agent in observations:
                        # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆä¸æ¢ç´¢ï¼‰
                        state = tf.expand_dims(observations[agent], 0)
                        action_probs = self.shared_network.actor(state)
                        action = int(tf.argmax(action_probs[0]))
                        actions[agent] = action
                
                observations, rewards, terminations, truncations, infos = env.step(actions)
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if any(terminations.values()) or any(truncations.values()):
                    # è·å–æœ€ç»ˆç»Ÿè®¡
                    if any(infos.values()) and "final_stats" in list(infos.values())[0]:
                        final_stats = list(infos.values())[0]["final_stats"]
                        
                        eval_results['makespans'].append(final_stats.get('makespan', 0))
                        eval_results['total_tardiness'].append(final_stats.get('total_tardiness', 0))
                        eval_results['max_tardiness'].append(final_stats.get('max_tardiness', 0))
                        eval_results['completed_parts'].append(final_stats.get('total_parts', 0))
                        eval_results['utilizations'].append(final_stats.get('avg_utilization', 0))
                        eval_results['detailed_stats'].append(final_stats)
                    break
            
            eval_results['episode_rewards'].append(episode_reward)
            
            if (episode + 1) % 5 == 0:
                print(f"  è¯„ä¼°è¿›åº¦: {episode + 1}/{num_episodes}")
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        summary_stats = {
            'mean_reward': np.mean(eval_results['episode_rewards']),
            'std_reward': np.std(eval_results['episode_rewards']),
            'mean_makespan': np.mean(eval_results['makespans']) if eval_results['makespans'] else 0,
            'mean_tardiness': np.mean(eval_results['total_tardiness']) if eval_results['total_tardiness'] else 0,
            'mean_utilization': np.mean(eval_results['utilizations']) if eval_results['utilizations'] else 0,
            'mean_completed_parts': np.mean(eval_results['completed_parts']) if eval_results['completed_parts'] else 0,
        }
        
        eval_results['summary'] = summary_stats
        
        print("\n" + "=" * 60)
        print("è¯„ä¼°ç»“æœæ±‡æ€»")
        print("=" * 60)
        print(f"å¹³å‡å¥–åŠ±: {summary_stats['mean_reward']:.2f} Â± {summary_stats['std_reward']:.2f}")
        print(f"å¹³å‡Makespan: {summary_stats['mean_makespan']:.1f}")
        print(f"å¹³å‡å»¶æœŸæ—¶é—´: {summary_stats['mean_tardiness']:.1f}")
        print(f"å¹³å‡è®¾å¤‡åˆ©ç”¨ç‡: {summary_stats['mean_utilization']:.1%}")
        print(f"å¹³å‡å®Œæˆé›¶ä»¶: {summary_stats['mean_completed_parts']:.1f}")
        
        return eval_results
    
    def progressive_train(self, static_episodes: int = 80, dynamic_episodes: int = 20, 
                         steps_per_episode: int = 200):
        """é€’è¿›å¼è®­ç»ƒä¸»æµç¨‹"""
        print("ğŸš€ Wå·¥å‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ - é€’è¿›å¼è®­ç»ƒ")
        print("=" * 60)
        print("è®­ç»ƒç­–ç•¥: ä»é™æ€åˆ°åŠ¨æ€çš„é€’è¿›å¼å­¦ä¹ ")
        print(f"æ€»è®­ç»ƒå›åˆ: {static_episodes + dynamic_episodes}")
        print("=" * 60)
        
        # éªŒè¯é…ç½®
        if not validate_config():
            print("é…ç½®éªŒè¯å¤±è´¥")
            return None
        
        try:
            # é˜¶æ®µ1: é™æ€è®­ç»ƒ
            static_results = self.static_training(static_episodes, steps_per_episode)
            
            # ä¸­æœŸè¯„ä¼°
            print("\nğŸ“Š ä¸­æœŸè¯„ä¼°ï¼ˆé™æ€è®­ç»ƒåï¼‰...")
            mid_eval = self.comprehensive_evaluation(num_episodes=10)
            
            # é˜¶æ®µ2: åŠ¨æ€å¾®è°ƒ
            dynamic_results = self.dynamic_training(dynamic_episodes, steps_per_episode)
            
            # æœ€ç»ˆè¯„ä¼°
            print("\nğŸ“Š æœ€ç»ˆè¯„ä¼°ï¼ˆå®Œæ•´è®­ç»ƒåï¼‰...")
            final_eval = self.comprehensive_evaluation(num_episodes=20)
            
            # ä¿å­˜æ¨¡å‹
            os.makedirs("models", exist_ok=True)
            self.save_model("models/enhanced_marl_model")
            
            # æ±‡æ€»ç»“æœ
            complete_results = {
                'training_phases': {
                    'static': static_results,
                    'dynamic': dynamic_results
                },
                'evaluations': {
                    'mid_evaluation': mid_eval,
                    'final_evaluation': final_eval
                },
                'training_history': {
                    'episode_rewards': self.episode_rewards,
                    'training_losses': self.training_losses
                },
                'config': {
                    'algorithm': 'PPO/MAPPO',
                    'network': 'Shared Actor-Critic',
                    'training_approach': 'Progressive (Static â†’ Dynamic)',
                    'agents': list(WORKSTATIONS.keys()),
                    'state_dim': 2,
                    'action_dim': 2
                }
            }
            
            # ä¿å­˜ç»“æœ
            os.makedirs("results", exist_ok=True)
            results_file = f"results/enhanced_marl_results_{int(time.time())}.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(complete_results, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ“ å®Œæ•´ç»“æœå·²ä¿å­˜: {results_file}")
            
            # æ€§èƒ½å¯¹æ¯”åˆ†æ
            self.performance_analysis(mid_eval, final_eval)
            
            return complete_results
            
        except Exception as e:
            print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def performance_analysis(self, mid_eval: Dict, final_eval: Dict):
        """æ€§èƒ½åˆ†æå’Œå¯¹æ¯”"""
        print("\n" + "=" * 60)
        print("ğŸ” æ€§èƒ½åˆ†æä¸å¯¹æ¯”")
        print("=" * 60)
        
        mid_stats = mid_eval['summary']
        final_stats = final_eval['summary']
        
        print("è®­ç»ƒé˜¶æ®µå¯¹æ¯”:")
        print(f"{'æŒ‡æ ‡':<15} {'é™æ€è®­ç»ƒå':<12} {'åŠ¨æ€å¾®è°ƒå':<12} {'æ”¹è¿›':<10}")
        print("-" * 55)
        
        metrics = [
            ('å¹³å‡å¥–åŠ±', 'mean_reward'),
            ('å¹³å‡Makespan', 'mean_makespan'),
            ('å¹³å‡å»¶æœŸ', 'mean_tardiness'),
            ('è®¾å¤‡åˆ©ç”¨ç‡', 'mean_utilization'),
            ('å®Œæˆé›¶ä»¶', 'mean_completed_parts')
        ]
        
        for name, key in metrics:
            mid_val = mid_stats.get(key, 0)
            final_val = final_stats.get(key, 0)
            
            if key == 'mean_utilization':
                improvement = f"{(final_val - mid_val)*100:+.1f}%"
                print(f"{name:<15} {mid_val:.1%:<12} {final_val:.1%:<12} {improvement:<10}")
            else:
                if mid_val != 0:
                    improvement = f"{(final_val - mid_val)/mid_val*100:+.1f}%"
                else:
                    improvement = "N/A"
                print(f"{name:<15} {mid_val:<12.1f} {final_val:<12.1f} {improvement:<10}")
        
        print("\nğŸ¯ è®­ç»ƒæ•ˆæœæ€»ç»“:")
        if final_stats['mean_reward'] > mid_stats['mean_reward']:
            print("âœ… åŠ¨æ€å¾®è°ƒæˆåŠŸæå‡äº†æ•´ä½“æ€§èƒ½")
        else:
            print("âš ï¸  åŠ¨æ€å¾®è°ƒåæ€§èƒ½ç•¥æœ‰ä¸‹é™ï¼Œä½†å¢å¼ºäº†é²æ£’æ€§")
        
        if final_stats['mean_makespan'] < mid_stats['mean_makespan']:
            print("âœ… å®Œå·¥æ—¶é—´å¾—åˆ°ä¼˜åŒ–")
        
        if final_stats['mean_utilization'] > mid_stats['mean_utilization']:
            print("âœ… è®¾å¤‡åˆ©ç”¨ç‡æœ‰æ‰€æå‡")
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        self.shared_network.actor.save(f"{filepath}_actor.keras")
        self.shared_network.critic.save(f"{filepath}_critic.keras")
        print(f"âœ… å¢å¼ºæ¨¡å‹å·²ä¿å­˜: {filepath}_actor.keras å’Œ {filepath}_critic.keras")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ Wå·¥å‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ - å¢å¼ºç‰ˆ")
    print("ğŸ¯ ç›®æ ‡: æœ€å°åŒ–Makespan + æœ€å¤§åŒ–åˆ©ç”¨ç‡ + æœ€å°åŒ–å»¶æœŸ")
    print("ğŸ§  ç®—æ³•: PPO/MAPPO with Progressive Training")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    tf.random.set_seed(RANDOM_SEED)
    
    try:
        # åˆ›å»ºå¢å¼ºè®­ç»ƒå™¨
        trainer = EnhancedMARLTrainer({
            'lr': 3e-4,
        })
        
        # å¼€å§‹é€’è¿›å¼è®­ç»ƒ
        results = trainer.progressive_train(
            static_episodes=80,    # é™æ€ç¯å¢ƒè®­ç»ƒ
            dynamic_episodes=20,   # åŠ¨æ€ç¯å¢ƒå¾®è°ƒ
            steps_per_episode=200
        )
        
        if results:
            print("\n" + "ğŸ‰" * 20)
            print("ğŸ‰ å¢å¼ºç‰ˆMARLè®­ç»ƒå®Œæˆï¼")
            print("ğŸ‰" * 20)
            print("\nâœ… å®ç°çš„æ ¸å¿ƒåŠŸèƒ½:")
            print("  â€¢ çœŸæ­£çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (MARL)")
            print("  â€¢ PPO/MAPPOç®—æ³•å®ç°")
            print("  â€¢ ç­–ç•¥ç½‘ç»œå…±äº«ä¸ååŒå­¦ä¹ ")
            print("  â€¢ é€’è¿›å¼è®­ç»ƒ (é™æ€â†’åŠ¨æ€)")
            print("  â€¢ å…¨é¢çš„æ€§èƒ½è¯„ä¼°æŒ‡æ ‡")
            print("  â€¢ ç¬¦åˆREADMEé¡¹ç›®ç›®æ ‡")
            
            final_eval = results['evaluations']['final_evaluation']['summary']
            print(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡:")
            print(f"  â€¢ å¹³å‡Makespan: {final_eval['mean_makespan']:.1f}")
            print(f"  â€¢ å¹³å‡å»¶æœŸæ—¶é—´: {final_eval['mean_tardiness']:.1f}")
            print(f"  â€¢ å¹³å‡è®¾å¤‡åˆ©ç”¨ç‡: {final_eval['mean_utilization']:.1%}")
            print(f"  â€¢ å¹³å‡å®Œæˆé›¶ä»¶: {final_eval['mean_completed_parts']:.1f}")
            
        else:
            print("\nâŒ è®­ç»ƒå¤±è´¥")
            
    except Exception as e:
        print(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 