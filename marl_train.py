"""
çœŸæ­£çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒè„šæœ¬
ä½¿ç”¨ç®€åŒ–çš„PPOå®ç°ï¼Œé¿å…Rayçš„Windowså…¼å®¹æ€§é—®é¢˜
å®ç°çœŸæ­£çš„ååŒå­¦ä¹ å’Œç­–ç•¥å…±äº«
"""

import os
import sys
import time
import json
import random
import numpy as np
import tensorflow as tf
from collections import defaultdict, deque
from typing import Dict, List, Tuple, Any, Optional

# è®¾ç½®TensorFlowæ—¥å¿—çº§åˆ«
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.get_logger().setLevel('ERROR')

# æ·»åŠ ç¯å¢ƒè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from environments.w_factory_env import make_parallel_env
from environments.w_factory_config import *

class PPONetwork:
    """ç®€åŒ–çš„PPOç½‘ç»œå®ç°"""
    
    def __init__(self, state_dim: int, action_dim: int, lr: float = 3e-4):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.lr = lr
        
        # æ„å»ºç½‘ç»œ
        self.actor, self.critic = self._build_networks()
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = tf.keras.optimizers.Adam(lr)
        self.critic_optimizer = tf.keras.optimizers.Adam(lr)
        
    def _build_networks(self):
        """æ„å»ºActor-Criticç½‘ç»œ"""
        # Actorç½‘ç»œ (ç­–ç•¥ç½‘ç»œ)
        actor_input = tf.keras.layers.Input(shape=(self.state_dim,))
        actor_hidden1 = tf.keras.layers.Dense(256, activation='relu')(actor_input)
        actor_hidden2 = tf.keras.layers.Dense(256, activation='relu')(actor_hidden1)
        actor_output = tf.keras.layers.Dense(self.action_dim, activation='softmax')(actor_hidden2)
        actor = tf.keras.Model(inputs=actor_input, outputs=actor_output)
        
        # Criticç½‘ç»œ (ä»·å€¼ç½‘ç»œ)
        critic_input = tf.keras.layers.Input(shape=(self.state_dim,))
        critic_hidden1 = tf.keras.layers.Dense(256, activation='relu')(critic_input)
        critic_hidden2 = tf.keras.layers.Dense(256, activation='relu')(critic_hidden1)
        critic_output = tf.keras.layers.Dense(1)(critic_hidden2)
        critic = tf.keras.Model(inputs=critic_input, outputs=critic_output)
        
        return actor, critic
    
    def get_action_and_value(self, state: np.ndarray) -> Tuple[int, float, float]:
        """è·å–åŠ¨ä½œã€åŠ¨ä½œæ¦‚ç‡å’ŒçŠ¶æ€ä»·å€¼"""
        state = tf.expand_dims(state, 0)
        
        # è·å–åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
        action_probs = self.actor(state)
        action_dist = tf.random.categorical(tf.math.log(action_probs), 1)
        action = int(action_dist[0, 0])
        
        # è·å–åŠ¨ä½œæ¦‚ç‡
        action_prob = float(action_probs[0, action])
        
        # è·å–çŠ¶æ€ä»·å€¼
        value = float(self.critic(state)[0, 0])
        
        return action, action_prob, value
    
    def get_value(self, state: np.ndarray) -> float:
        """è·å–çŠ¶æ€ä»·å€¼"""
        state = tf.expand_dims(state, 0)
        return float(self.critic(state)[0, 0])
    
    def update(self, states: np.ndarray, actions: np.ndarray, 
               old_probs: np.ndarray, advantages: np.ndarray, 
               returns: np.ndarray, clip_ratio: float = 0.2) -> Dict[str, float]:
        """PPOæ›´æ–°"""
        
        # Actoræ›´æ–°
        with tf.GradientTape() as tape:
            action_probs = self.actor(states)
            action_probs_selected = tf.reduce_sum(
                action_probs * tf.one_hot(actions, self.action_dim), axis=1
            )
            
            # è®¡ç®—æ¯”ç‡
            ratio = action_probs_selected / (old_probs + 1e-8)
            
            # PPOè£å‰ªç›®æ ‡
            clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)
            actor_loss = -tf.reduce_mean(
                tf.minimum(ratio * advantages, clipped_ratio * advantages)
            )
            
            # ç†µæ­£åˆ™åŒ–
            entropy = -tf.reduce_sum(action_probs * tf.math.log(action_probs + 1e-8), axis=1)
            actor_loss -= 0.01 * tf.reduce_mean(entropy)
        
        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)
        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))
        
        # Criticæ›´æ–°
        with tf.GradientTape() as tape:
            values = tf.squeeze(self.critic(states))
            critic_loss = tf.reduce_mean(tf.square(returns - values))
        
        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)
        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))
        
        return {
            'actor_loss': float(actor_loss),
            'critic_loss': float(critic_loss),
            'entropy': float(tf.reduce_mean(entropy))
        }

class ExperienceBuffer:
    """ç»éªŒç¼“å†²åŒº"""
    
    def __init__(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.action_probs = []
        self.dones = []
        
    def store(self, state, action, reward, value, action_prob, done):
        """å­˜å‚¨ç»éªŒ"""
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.values.append(value)
        self.action_probs.append(action_prob)
        self.dones.append(done)
    
    def get_batch(self, gamma=0.99, lam=0.95):
        """è·å–æ‰¹æ¬¡æ•°æ®å¹¶è®¡ç®—ä¼˜åŠ¿å‡½æ•°"""
        states = np.array(self.states)
        actions = np.array(self.actions)
        rewards = np.array(self.rewards)
        values = np.array(self.values)
        action_probs = np.array(self.action_probs)
        dones = np.array(self.dones)
        
        # è®¡ç®—GAEä¼˜åŠ¿å‡½æ•°
        advantages = np.zeros_like(rewards)
        last_advantage = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
            advantages[t] = delta + gamma * lam * (1 - dones[t]) * last_advantage
            last_advantage = advantages[t]
        
        # è®¡ç®—å›æŠ¥
        returns = advantages + values
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿å‡½æ•°
        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)
        
        return states, actions, action_probs, advantages, returns
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.values.clear()
        self.action_probs.clear()
        self.dones.clear()

class MARLTrainer:
    """å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.env = make_parallel_env()
        
        # å…±äº«ç­–ç•¥ç½‘ç»œ (æ‰€æœ‰æ™ºèƒ½ä½“å…±äº«åŒä¸€ä¸ªç½‘ç»œ)
        self.shared_network = PPONetwork(
            state_dim=2,  # [é˜Ÿåˆ—é•¿åº¦, è®¾å¤‡çŠ¶æ€]
            action_dim=2,  # [IDLE, PROCESS]
            lr=self.config.get('lr', 3e-4)
        )
        
        # ç»éªŒç¼“å†²åŒº (æ¯ä¸ªæ™ºèƒ½ä½“ä¸€ä¸ª)
        self.buffers = {
            agent: ExperienceBuffer() 
            for agent in self.env.possible_agents
        }
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_lengths = []
        self.training_losses = []
        
    def collect_experience(self, num_steps: int = 200) -> Dict[str, float]:
        """æ”¶é›†ç»éªŒ"""
        observations, _ = self.env.reset()
        episode_rewards = {agent: 0 for agent in self.env.possible_agents}
        step_count = 0
        
        for step in range(num_steps):
            # è·å–æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œ
            actions = {}
            values = {}
            action_probs = {}
            
            for agent in self.env.agents:
                if agent in observations:
                    action, action_prob, value = self.shared_network.get_action_and_value(
                        observations[agent]
                    )
                    actions[agent] = action
                    values[agent] = value
                    action_probs[agent] = action_prob
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_observations, rewards, terminations, truncations, _ = self.env.step(actions)
            
            # å­˜å‚¨ç»éªŒ
            for agent in self.env.agents:
                if agent in observations and agent in actions:
                    done = terminations.get(agent, False) or truncations.get(agent, False)
                    reward = rewards.get(agent, 0)
                    
                    self.buffers[agent].store(
                        state=observations[agent],
                        action=actions[agent],
                        reward=reward,
                        value=values[agent],
                        action_prob=action_probs[agent],
                        done=done
                    )
                    
                    episode_rewards[agent] += reward
            
            observations = next_observations
            step_count += 1
            
            # æ£€æŸ¥æ˜¯å¦ç»“æŸ
            if any(terminations.values()) or any(truncations.values()):
                observations, _ = self.env.reset()
        
        return episode_rewards
    
    def update_policy(self) -> Dict[str, float]:
        """æ›´æ–°ç­–ç•¥"""
        # åˆå¹¶æ‰€æœ‰æ™ºèƒ½ä½“çš„ç»éªŒ
        all_states = []
        all_actions = []
        all_action_probs = []
        all_advantages = []
        all_returns = []
        
        for agent, buffer in self.buffers.items():
            if len(buffer.states) > 0:
                states, actions, action_probs, advantages, returns = buffer.get_batch()
                
                all_states.extend(states)
                all_actions.extend(actions)
                all_action_probs.extend(action_probs)
                all_advantages.extend(advantages)
                all_returns.extend(returns)
                
                buffer.clear()
        
        if len(all_states) == 0:
            return {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0}
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        all_states = np.array(all_states)
        all_actions = np.array(all_actions)
        all_action_probs = np.array(all_action_probs)
        all_advantages = np.array(all_advantages)
        all_returns = np.array(all_returns)
        
        # å¤šæ¬¡æ›´æ–°
        losses = {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0}
        num_updates = 5
        
        for _ in range(num_updates):
            batch_losses = self.shared_network.update(
                states=all_states,
                actions=all_actions,
                old_probs=all_action_probs,
                advantages=all_advantages,
                returns=all_returns
            )
            
            for key in losses:
                losses[key] += batch_losses[key] / num_updates
        
        return losses
    
    def train(self, num_episodes: int = 100, steps_per_episode: int = 200):
        """è®­ç»ƒä¸»å¾ªç¯"""
        print("=" * 60)
        print("Wå·¥å‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒ (çœŸæ­£çš„MARL)")
        print("=" * 60)
        print(f"ç®—æ³•: PPO (Proximal Policy Optimization)")
        print(f"ç½‘ç»œ: å…±äº«Actor-Criticç½‘ç»œ")
        print(f"æ™ºèƒ½ä½“æ•°é‡: {len(self.env.possible_agents)}")
        print(f"è®­ç»ƒå›åˆ: {num_episodes}")
        print(f"æ¯å›åˆæ­¥æ•°: {steps_per_episode}")
        print("=" * 60)
        
        start_time = time.time()
        
        for episode in range(num_episodes):
            episode_start = time.time()
            
            # æ”¶é›†ç»éªŒ
            episode_rewards = self.collect_experience(steps_per_episode)
            
            # æ›´æ–°ç­–ç•¥
            losses = self.update_policy()
            
            # è®°å½•ç»Ÿè®¡
            total_reward = sum(episode_rewards.values())
            self.episode_rewards.append(total_reward)
            self.episode_lengths.append(steps_per_episode)
            self.training_losses.append(losses)
            
            episode_time = time.time() - episode_start
            
            # è¾“å‡ºæ—¥å¿—
            if (episode + 1) % 10 == 0:
                recent_rewards = self.episode_rewards[-10:]
                avg_reward = np.mean(recent_rewards)
                
                print(f"å›åˆ {episode + 1:4d}/{num_episodes} | "
                      f"å¥–åŠ±: {total_reward:8.2f} | "
                      f"å¹³å‡å¥–åŠ±: {avg_reward:8.2f} | "
                      f"ActoræŸå¤±: {losses['actor_loss']:.4f} | "
                      f"CriticæŸå¤±: {losses['critic_loss']:.4f} | "
                      f"æ—¶é—´: {episode_time:.2f}s")
        
        training_time = time.time() - start_time
        
        print("\n" + "=" * 60)
        print("MARLè®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        print(f"è®­ç»ƒæ—¶é—´: {training_time/60:.2f} åˆ†é’Ÿ")
        print(f"å¹³å‡å¥–åŠ±: {np.mean(self.episode_rewards):.2f}")
        print(f"æœ€ä½³å¥–åŠ±: {max(self.episode_rewards):.2f}")
        
        return {
            'episode_rewards': self.episode_rewards,
            'training_losses': self.training_losses,
            'training_time': training_time
        }
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        self.shared_network.actor.save(f"{filepath}_actor.h5")
        self.shared_network.critic.save(f"{filepath}_critic.h5")
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}_actor.h5 å’Œ {filepath}_critic.h5")
    
    def evaluate(self, num_episodes: int = 10) -> Dict[str, Any]:
        """è¯„ä¼°æ¨¡å‹"""
        print(f"è¯„ä¼°æ¨¡å‹ ({num_episodes} å›åˆ)...")
        
        eval_rewards = []
        eval_stats = []
        
        for episode in range(num_episodes):
            observations, _ = self.env.reset()
            episode_reward = 0
            step_count = 0
            
            while step_count < 480:  # æœ€å¤§ä»¿çœŸæ—¶é—´
                actions = {}
                for agent in self.env.agents:
                    if agent in observations:
                        # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆä¸æ¢ç´¢ï¼‰
                        state = tf.expand_dims(observations[agent], 0)
                        action_probs = self.shared_network.actor(state)
                        action = int(tf.argmax(action_probs[0]))
                        actions[agent] = action
                
                observations, rewards, terminations, truncations, infos = self.env.step(actions)
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if any(terminations.values()) or any(truncations.values()):
                    # è·å–æœ€ç»ˆç»Ÿè®¡
                    if any(infos.values()) and "final_stats" in list(infos.values())[0]:
                        eval_stats.append(list(infos.values())[0]["final_stats"])
                    break
            
            eval_rewards.append(episode_reward)
        
        return {
            'mean_reward': np.mean(eval_rewards),
            'std_reward': np.std(eval_rewards),
            'eval_rewards': eval_rewards,
            'eval_stats': eval_stats
        }

def main():
    """ä¸»å‡½æ•°"""
    print("Wå·¥å‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ - çœŸæ­£çš„MARLè®­ç»ƒ")
    print("=" * 60)
    
    # éªŒè¯é…ç½®
    if not validate_config():
        print("é…ç½®éªŒè¯å¤±è´¥")
        return
    
    # è®¾ç½®éšæœºç§å­
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    tf.random.set_seed(RANDOM_SEED)
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = MARLTrainer({
            'lr': 3e-4,
        })
        
        # å¼€å§‹è®­ç»ƒ
        results = trainer.train(num_episodes=100, steps_per_episode=200)
        
        # ä¿å­˜æ¨¡å‹
        os.makedirs("models", exist_ok=True)
        trainer.save_model("models/marl_model")
        
        # è¯„ä¼°æ¨¡å‹
        eval_results = trainer.evaluate(num_episodes=10)
        
        # ä¿å­˜ç»“æœ
        final_results = {
            'training_results': results,
            'evaluation_results': eval_results,
            'config': {
                'algorithm': 'PPO',
                'network': 'Shared Actor-Critic',
                'agents': trainer.env.possible_agents,
                'state_dim': 2,
                'action_dim': 2
            }
        }
        
        os.makedirs("results", exist_ok=True)
        results_file = f"results/marl_training_results_{int(time.time())}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print("\n" + "=" * 60)
        print("æœ€ç»ˆè¯„ä¼°ç»“æœ")
        print("=" * 60)
        print(f"å¹³å‡å¥–åŠ±: {eval_results['mean_reward']:.2f}")
        print(f"å¥–åŠ±æ ‡å‡†å·®: {eval_results['std_reward']:.2f}")
        
        if eval_results['eval_stats']:
            avg_makespan = np.mean([s.get('makespan', 0) for s in eval_results['eval_stats']])
            avg_tardiness = np.mean([s.get('total_tardiness', 0) for s in eval_results['eval_stats']])
            avg_completed = np.mean([s.get('total_parts', 0) for s in eval_results['eval_stats']])
            
            print(f"å¹³å‡Makespan: {avg_makespan:.1f}")
            print(f"å¹³å‡å»¶æœŸæ—¶é—´: {avg_tardiness:.1f}")
            print(f"å¹³å‡å®Œæˆé›¶ä»¶: {avg_completed:.1f}")
        
        print("\nğŸ‰ çœŸæ­£çš„MARLè®­ç»ƒå®Œæˆï¼")
        
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 