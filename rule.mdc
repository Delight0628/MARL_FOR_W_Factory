---
alwaysApply: true
---
1. 项目核心目标 (Core Objective)
项目名称: 基于多智能体强化学习 (MARL) 的W工厂生产调度系统。
核心使命: 解决W工厂“多品种、小批量”生产模式下的调度难题。
优化指标:
最小化最大完工时间 (Makespan)
最大化设备利用率 (Utilization)
最小化订单延期 (Tardiness)
关键挑战: 模型需要具备鲁棒性，能够应对设备故障和紧急插单等动态事件。
2. 技术架构与关键文件 (Tech Stack & Key Files)
核心技术栈:
训练引擎: XXX(主要使用PPO/MAPPO算法)
多智能体接口: PettingZoo
仿真内核: SimPy (用于离散事件仿真)
关键文件概览:
environments/w_factory_config.py: 项目的唯一真理来源 (Single Source of Truth)。此文件定义了所有工厂参数（设备、产品工艺、OEE）、仿真设置（如QUEUE_CAPACITY）和订单。在进行任何代码修改前，必须先查阅此配置文件。
environments/w_factory_env.py: 项目的心脏。包含了两个核心类：
WFactorySim: 基于SimPy的底层仿真逻辑，模拟真实物理世界。
WFactoryEnv: 遵循PettingZoo API，将仿真环境封装为MARL接口。
main.py: 训练启动脚本。负责配置并运行RLlib训练流程。也是项目的核心执行脚本，它负责：
a.运行基于规则的基准线调度：支持 FIFO（先进先出）和 SPT（最短处理时间）两种传统调度模式，用于对比评估。
b.执行多智能体强化学习（MARL）训练：使用XXX框架，通过 PPO 算法训练生产设备智能体，以实现最小化最大完工时间、最大化设备利用率和最小化订单延期的目标。
README.md: 完整的项目说明文档，是理解项目全貌的最佳起点。
3. MARL建模细节 (MARL Modeling Details)
智能体 (Agent): 每个工作站/设备类型 (WORKSTATIONS in config) 都是一个独立的智能体。所有智能体共享同一套策略网络以提高训练效率。
状态空间 (State Space): 每个智能体的观测是一个精确的2维归一化向量: [归一化队列长度, 设备状态]。
归一化队列长度: len(queue) / QUEUE_CAPACITY，值被裁剪到 [0.0, 1.0]。
设备状态: 0.0 代表完全空闲，1.0 代表至少一台设备在忙。
动作空间 (Action Space): Discrete(2)，离散动作集，含义如下：
0: 空闲 (IDLE) - 保持等待，不处理新任务。
1: 加工 (PROCESS) - 从队列取件并开始加工。
奖励机制 (Reward Function): 采用全局共享奖励。
正奖励: 每完成一个最终成品，所有智能体获得正奖励。
负惩罚: 若订单延期，所有智能体受到巨大负惩罚。
4. 开发与实验流程 (Development & Experiment Workflow)
需求变更: 若要修改工厂布局、产品工艺或订单，请直接修改 environments/w_factory_config.py。
环境验证: 对环境 (w_factory_env.py) 进行任何修改后，必须运行 python test_env.py 确保环境的基本功能未被破坏。
启动训练: 使用 python main.py 启动完整的训练流程。
结果分析: 训练日志和模型检查点保存在 results/ 目录下。使用 tensorboard --logdir=./results 可视化训练过程。同时，利用 `python evaluate.py` 对训练好的模型进行量化评估，并将结果与 `main.py --mode fifo/spt` 运行的基准线进行对比，以全面分析模型性能。

训练范式: 遵循“从静态到动态”的两阶段训练法：
第一阶段 (静态训练): 在无随机事件的环境中训练，得到基准模型。
第二阶段 (动态微调): 在基准模型上，引入设备故障等动态事件进行Fine-tuning，增强模型鲁棒性。