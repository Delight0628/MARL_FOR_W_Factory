"""
çº¯å‡€çš„å¤šæ™ºèƒ½ä½“PPOè®­ç»ƒè„šæœ¬
ä¸“æ³¨äºæ ¸å¿ƒè®­ç»ƒåŠŸèƒ½ï¼Œç§»é™¤å¤æ‚çš„è¯„ä¼°å’Œå¯è§†åŒ–
"""

import os
import sys
import time
import random
import numpy as np
import tensorflow as tf
from typing import Dict, List, Tuple, Any
from datetime import datetime

# è®¾ç½®TensorFlowæ—¥å¿—çº§åˆ«
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.get_logger().setLevel('ERROR')

# æ·»åŠ ç¯å¢ƒè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from environments.w_factory_env import make_parallel_env
from environments.w_factory_config import *

class PPONetwork:
    """PPOç½‘ç»œå®ç°"""
    
    # ğŸ”§ V3 ä¿®å¤: lrå‚æ•°ç°åœ¨å¯ä»¥æ˜¯å­¦ä¹ ç‡è°ƒåº¦å™¨
    def __init__(self, state_dim: int, action_dim: int, lr: Any):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.lr = lr
        
        # æ„å»ºç½‘ç»œ
        self.actor, self.critic = self._build_networks()
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = tf.keras.optimizers.Adam(lr)
        self.critic_optimizer = tf.keras.optimizers.Adam(lr)
        
    def _build_networks(self):
        """æ„å»ºActor-Criticç½‘ç»œ"""
        # Actorç½‘ç»œ - å¢å¼ºç‰ˆ
        actor_input = tf.keras.layers.Input(shape=(self.state_dim,))
        actor_hidden1 = tf.keras.layers.Dense(512, activation='relu')(actor_input)
        actor_hidden2 = tf.keras.layers.Dense(256, activation='relu')(actor_hidden1)
        actor_output = tf.keras.layers.Dense(self.action_dim, activation='softmax')(actor_hidden2)
        actor = tf.keras.Model(inputs=actor_input, outputs=actor_output)
        
        # Criticç½‘ç»œ - å¢å¼ºç‰ˆ
        critic_input = tf.keras.layers.Input(shape=(self.state_dim,))
        critic_hidden1 = tf.keras.layers.Dense(512, activation='relu')(critic_input)
        critic_hidden2 = tf.keras.layers.Dense(256, activation='relu')(critic_hidden1)
        critic_output = tf.keras.layers.Dense(1)(critic_hidden2)
        critic = tf.keras.Model(inputs=critic_input, outputs=critic_output)
        
        return actor, critic
    
    def get_action_and_value(self, state: np.ndarray) -> Tuple[int, float, float]:
        """è·å–åŠ¨ä½œã€åŠ¨ä½œæ¦‚ç‡å’ŒçŠ¶æ€ä»·å€¼"""
        state = tf.expand_dims(state, 0)
        
        action_probs = self.actor(state)
        action_dist = tf.random.categorical(tf.math.log(action_probs + 1e-8), 1)
        action = int(action_dist[0, 0])
        
        action_prob = float(action_probs[0, action])
        value = float(self.critic(state)[0, 0])
        
        return action, action_prob, value
    
    def get_value(self, state: np.ndarray) -> float:
        """è·å–çŠ¶æ€ä»·å€¼"""
        state = tf.expand_dims(state, 0)
        return float(self.critic(state)[0, 0])
    
    def update(self, states: np.ndarray, actions: np.ndarray, 
               old_probs: np.ndarray, advantages: np.ndarray, 
               returns: np.ndarray, clip_ratio: float = 0.15) -> Dict[str, float]:  # ğŸ”§ é™ä½è£å‰ªèŒƒå›´
        """PPOæ›´æ–°"""
        
        # Actoræ›´æ–°
        with tf.GradientTape() as tape:
            action_probs = self.actor(states)
            action_probs_selected = tf.reduce_sum(
                action_probs * tf.one_hot(actions, self.action_dim), axis=1
            )
            
            ratio = action_probs_selected / (old_probs + 1e-8)
            clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)
            actor_loss = -tf.reduce_mean(
                tf.minimum(ratio * advantages, clipped_ratio * advantages)
            )
            
            entropy = -tf.reduce_sum(action_probs * tf.math.log(action_probs + 1e-8), axis=1)
            actor_loss -= 0.01 * tf.reduce_mean(entropy)
        
        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)
        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))
        
        # Criticæ›´æ–°
        with tf.GradientTape() as tape:
            values = tf.squeeze(self.critic(states))
            critic_loss = tf.reduce_mean(tf.square(returns - values))
        
        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)
        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))
        
        return {
            'actor_loss': float(actor_loss),
            'critic_loss': float(critic_loss),
            'entropy': float(tf.reduce_mean(entropy))
        }

class ExperienceBuffer:
    """ç»éªŒç¼“å†²åŒº"""
    
    def __init__(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.action_probs = []
        self.dones = []
        
    def store(self, state, action, reward, value, action_prob, done):
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.values.append(value)
        self.action_probs.append(action_prob)
        self.dones.append(done)
    
    def get_batch(self, gamma=0.99, lam=0.95):
        states = np.array(self.states)
        actions = np.array(self.actions)
        rewards = np.array(self.rewards)
        values = np.array(self.values)
        action_probs = np.array(self.action_probs)
        dones = np.array(self.dones)
        
        advantages = np.zeros_like(rewards)
        last_advantage = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
            advantages[t] = delta + gamma * lam * (1 - dones[t]) * last_advantage
            last_advantage = advantages[t]
        
        returns = advantages + values
        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)
        
        return states, actions, action_probs, advantages, returns
    
    def clear(self):
        self.states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.values.clear()
        self.action_probs.clear()
        self.dones.clear()

class SimplePPOTrainer:
    """ç®€åŒ–çš„PPOè®­ç»ƒå™¨"""
    
    # ğŸ”§ V5 ç³»ç»Ÿèµ„æºä¼˜åŒ–: æ ¹æ®é…ç½®è°ƒæ•´è®­ç»ƒå‚æ•°
    def __init__(self, initial_lr: float, total_train_episodes: int, steps_per_episode: int):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ğŸ”§ V5 æ€§èƒ½ä¼˜åŒ–ï¼šæ£€æµ‹ç³»ç»Ÿèµ„æº
        self.system_info = self._detect_system_resources()
        self._optimize_tensorflow_settings()
        
        # ç¯å¢ƒæ¢æµ‹
        temp_env, _ = self.create_environment()
        self.state_dim = temp_env.observation_space(temp_env.possible_agents[0]).shape[0]
        self.action_dim = temp_env.action_space(temp_env.possible_agents[0]).n
        self.agent_ids = temp_env.possible_agents
        temp_env.close()
        
        print("ğŸ”§ ç¯å¢ƒç©ºé—´æ£€æµ‹:")
        print(f"   è§‚æµ‹ç»´åº¦: {self.state_dim}")
        print(f"   åŠ¨ä½œç»´åº¦: {self.action_dim}")
        print(f"   æ™ºèƒ½ä½“æ•°é‡: {len(self.agent_ids)}")
        
        # ğŸ”§ V5 èµ„æºä¼˜åŒ–ï¼šæ ¹æ®å†…å­˜è°ƒæ•´è®­ç»ƒå‚æ•°
        optimized_episodes, optimized_steps = self._optimize_training_params(
            total_train_episodes, steps_per_episode
        )
        
        # ğŸ”§ V3 ä¿®å¤: åˆ›å»ºå­¦ä¹ ç‡è¡°å‡è°ƒåº¦å™¨
        self.lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
            initial_learning_rate=initial_lr,
            decay_steps=optimized_episodes * optimized_steps,
            end_learning_rate=1e-5,  # è¡°å‡åˆ°è¾ƒä½çš„å€¼
            power=1.0  # çº¿æ€§è¡°å‡
        )

        # å…±äº«ç½‘ç»œ
        self.shared_network = PPONetwork(
            state_dim=self.state_dim,
            action_dim=self.action_dim,
            lr=self.lr_schedule
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.training_losses = []
        self.iteration_times = []  # ğŸ”§ V5 æ–°å¢ï¼šè®°å½•æ¯è½®è®­ç»ƒæ—¶é—´
        self.kpi_history = []      # ğŸ”§ V5 æ–°å¢ï¼šè®°å½•æ¯è½®KPIå†å²
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.models_dir = "models"
        os.makedirs(self.models_dir, exist_ok=True)
    
    def _detect_system_resources(self) -> Dict[str, Any]:
        """ğŸ”§ V5 æ–°å¢ï¼šæ£€æµ‹ç³»ç»Ÿèµ„æº"""
        try:
            import psutil  # type: ignore
            cpu_count = psutil.cpu_count()
            memory_info = psutil.virtual_memory()
            memory_gb = memory_info.total / (1024**3)
            available_gb = memory_info.available / (1024**3)
            
            # æ£€æµ‹GPU
            gpu_available = len(tf.config.list_physical_devices('GPU')) > 0
            gpu_memory = 0
            if gpu_available:
                try:
                    gpus = tf.config.list_physical_devices('GPU')
                    for gpu in gpus:
                        gpu_details = tf.config.experimental.get_device_details(gpu)
                        gpu_memory = gpu_details.get('device_name', 'Unknown')
                except:
                    gpu_available = False
            
            system_info = {
                'cpu_count': cpu_count,
                'memory_gb': memory_gb,
                'available_gb': available_gb,
                'gpu_available': gpu_available,
                'gpu_memory': gpu_memory
            }
            
            print("ğŸ’» ç³»ç»Ÿèµ„æºæ£€æµ‹:")
            print(f"   CPUæ ¸å¿ƒæ•°: {cpu_count}")
            print(f"   æ€»å†…å­˜: {memory_gb:.1f}GB")
            print(f"   å¯ç”¨å†…å­˜: {available_gb:.1f}GB")
            print(f"   GPUå¯ç”¨: {'âœ…' if gpu_available else 'âŒ'}")
            if gpu_available:
                print(f"   GPUä¿¡æ¯: {gpu_memory}")
            
            return system_info
            
        except ImportError:
            # å¦‚æœæ²¡æœ‰psutilï¼Œä½¿ç”¨ä¿å®ˆä¼°è®¡
            print("âš ï¸  æ— æ³•æ£€æµ‹ç³»ç»Ÿèµ„æºï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {
                'cpu_count': 4,
                'memory_gb': 8.0,
                'available_gb': 4.0,
                'gpu_available': False,
                'gpu_memory': None
            }
    
    def _optimize_tensorflow_settings(self):
        """ğŸ”§ V5 æ–°å¢ï¼šä¼˜åŒ–TensorFlowè®¾ç½®"""
        # å†…å­˜å¢é•¿è®¾ç½®
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            try:
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                print("âœ… GPUå†…å­˜å¢é•¿æ¨¡å¼å·²å¯ç”¨")
            except RuntimeError as e:
                print(f"âš ï¸  GPUè®¾ç½®å¤±è´¥: {e}")
        
        # æ ¹æ®å†…å­˜æƒ…å†µè®¾ç½®TensorFlow
        available_gb = self.system_info.get('available_gb', 4.0)
        if available_gb < 6.0:
            # ä½å†…å­˜æ¨¡å¼
            tf.config.threading.set_inter_op_parallelism_threads(2)
            tf.config.threading.set_intra_op_parallelism_threads(2)
            print("ğŸ”§ ä½å†…å­˜æ¨¡å¼: é™åˆ¶TensorFlowå¹¶è¡Œåº¦")
        else:
            # æ­£å¸¸æ¨¡å¼
            cpu_count = self.system_info.get('cpu_count', 4)
            tf.config.threading.set_inter_op_parallelism_threads(min(cpu_count, 4))
            tf.config.threading.set_intra_op_parallelism_threads(min(cpu_count, 8))
            print("ğŸ”§ æ­£å¸¸æ¨¡å¼: TensorFlowå¹¶è¡Œåº¦ä¼˜åŒ–")
    
    def _optimize_training_params(self, num_episodes: int, steps_per_episode: int) -> Tuple[int, int]:
        """ğŸ”§ V5 æ–°å¢ï¼šæ ¹æ®ç³»ç»Ÿèµ„æºä¼˜åŒ–è®­ç»ƒå‚æ•°"""
        available_gb = self.system_info.get('available_gb', 4.0)
        
        # æ ¹æ®å¯ç”¨å†…å­˜è°ƒæ•´è®­ç»ƒè§„æ¨¡
        if available_gb < 4.0:
            # æä½å†…å­˜ï¼šå¤§å¹…é™ä½å‚æ•°
            optimized_episodes = min(num_episodes, 60)
            optimized_steps = min(steps_per_episode, 800)
            print("ğŸš¨ æä½å†…å­˜æ¨¡å¼: è®­ç»ƒè§„æ¨¡å¤§å¹…ç¼©å‡")
        elif available_gb < 6.0:
            # ä½å†…å­˜ï¼šé€‚åº¦é™ä½å‚æ•°
            optimized_episodes = min(num_episodes, 80)
            optimized_steps = min(steps_per_episode, 1000)
            print("âš ï¸  ä½å†…å­˜æ¨¡å¼: è®­ç»ƒè§„æ¨¡é€‚åº¦ç¼©å‡")
        elif available_gb < 8.0:
            # ä¸­ç­‰å†…å­˜ï¼šç•¥å¾®é™ä½å‚æ•°
            optimized_episodes = min(num_episodes, 100)
            optimized_steps = min(steps_per_episode, 1200)
            print("ğŸ”§ ä¸­ç­‰å†…å­˜æ¨¡å¼: è®­ç»ƒè§„æ¨¡ç•¥å¾®è°ƒæ•´")
        else:
            # å……è¶³å†…å­˜ï¼šä½¿ç”¨åŸå§‹å‚æ•°
            optimized_episodes = num_episodes
            optimized_steps = steps_per_episode
            print("âœ… å……è¶³å†…å­˜æ¨¡å¼: ä½¿ç”¨å®Œæ•´è®­ç»ƒè§„æ¨¡")
        
        if optimized_episodes != num_episodes or optimized_steps != steps_per_episode:
            print(f"ğŸ”§ å‚æ•°è°ƒæ•´: {num_episodes}å›åˆÃ—{steps_per_episode}æ­¥ â†’ {optimized_episodes}å›åˆÃ—{optimized_steps}æ­¥")
        
        return optimized_episodes, optimized_steps

    def create_environment(self):
        """åˆ›å»ºç¯å¢ƒ"""
        env = make_parallel_env()
        buffers = {
            agent: ExperienceBuffer() 
            for agent in env.possible_agents
        }
        return env, buffers
    
    def collect_experience(self, env, buffers, num_steps: int = 200) -> float:
        """æ”¶é›†ç»éªŒ"""
        observations, _ = env.reset()
        episode_rewards = {agent: 0 for agent in env.possible_agents}
        step_count = 0
        
        for step in range(num_steps):
            actions = {}
            values = {}
            action_probs = {}
            
            # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“è·å–åŠ¨ä½œ
            for agent in env.agents:
                if agent in observations:
                    action, action_prob, value = self.shared_network.get_action_and_value(
                        observations[agent]
                    )
                    actions[agent] = action
                    values[agent] = value
                    action_probs[agent] = action_prob
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_observations, rewards, terminations, truncations, infos = env.step(actions)
            
            # å­˜å‚¨ç»éªŒ
            for agent in env.agents:
                if agent in observations and agent in actions:
                    done = terminations.get(agent, False) or truncations.get(agent, False)
                    reward = rewards.get(agent, 0)
                    
                    buffers[agent].store(
                        state=observations[agent],
                        action=actions[agent],
                        reward=reward,
                        value=values[agent],
                        action_prob=action_probs[agent],
                        done=done
                    )
                    
                    episode_rewards[agent] += reward
            
            observations = next_observations
            step_count += 1
            
            # æ£€æŸ¥ç»“æŸæ¡ä»¶
            if any(terminations.values()) or any(truncations.values()):
                observations, _ = env.reset()
        
        return sum(episode_rewards.values())
    
    def update_policy(self, buffers) -> Dict[str, float]:
        """æ›´æ–°ç­–ç•¥"""
        all_states = []
        all_actions = []
        all_action_probs = []
        all_advantages = []
        all_returns = []
        
        # åˆå¹¶æ‰€æœ‰æ™ºèƒ½ä½“çš„ç»éªŒ
        for agent, buffer in buffers.items():
            if len(buffer.states) > 0:
                states, actions, action_probs, advantages, returns = buffer.get_batch()
                
                all_states.extend(states)
                all_actions.extend(actions)
                all_action_probs.extend(action_probs)
                all_advantages.extend(advantages)
                all_returns.extend(returns)
                
                buffer.clear()
        
        if len(all_states) == 0:
            return {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0}
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        all_states = np.array(all_states)
        all_actions = np.array(all_actions)
        all_action_probs = np.array(all_action_probs)
        all_advantages = np.array(all_advantages)
        all_returns = np.array(all_returns)
        
        # å¤šæ¬¡æ›´æ–° - ğŸ”§ å¢åŠ è¿­ä»£æ¬¡æ•°æå‡å­¦ä¹ å……åˆ†æ€§
        losses = {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0}
        num_updates = 10  # ä»5å¢åŠ åˆ°10
        
        for _ in range(num_updates):
            batch_losses = self.shared_network.update(
                states=all_states,
                actions=all_actions,
                old_probs=all_action_probs,
                advantages=all_advantages,
                returns=all_returns
            )
            
            for key in losses:
                losses[key] += batch_losses[key] / num_updates
        
        return losses
    
    def quick_kpi_evaluation(self, num_episodes: int = 3) -> Dict[str, float]:
        """ğŸ”§ V5 æ–°å¢ï¼šå¿«é€ŸKPIè¯„ä¼°ï¼ˆç”¨äºæ¯è½®ç›‘æ§ï¼‰"""
        env, _ = self.create_environment()
        
        total_rewards = []
        makespans = []
        utilizations = []
        completed_parts_list = []
        
        for episode in range(num_episodes):
            observations, _ = env.reset()
            episode_reward = 0
            step_count = 0
            
            while step_count < 800:  # å¿«é€Ÿè¯„ä¼°ï¼Œæ­¥æ•°è¾ƒå°‘
                actions = {}
                
                # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥è¯„ä¼°
                for agent in env.agents:
                    if agent in observations:
                        state = tf.expand_dims(observations[agent], 0)
                        action_probs = self.shared_network.actor(state)
                        action = int(tf.argmax(action_probs[0]))
                        actions[agent] = action
                
                observations, rewards, terminations, truncations, infos = env.step(actions)
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if any(terminations.values()) or any(truncations.values()):
                    break
            
            # è·å–æœ€ç»ˆç»Ÿè®¡
            final_stats = env.sim.get_final_stats()
            total_rewards.append(episode_reward)
            makespans.append(final_stats.get('makespan', step_count))
            utilizations.append(final_stats.get('mean_utilization', 0))
            completed_parts_list.append(final_stats.get('total_parts', 0))
        
        env.close()
        
        return {
            'mean_reward': np.mean(total_rewards),
            'mean_makespan': np.mean(makespans),
            'mean_utilization': np.mean(utilizations),
            'mean_completed_parts': np.mean(completed_parts_list)
        }
    
    def simple_evaluation(self, num_episodes: int = 5) -> Dict[str, float]:
        """ç®€å•è¯„ä¼°ï¼ˆä»…ç”¨äºè®­ç»ƒæœŸé—´çš„å¿«é€Ÿæ£€æŸ¥ï¼‰"""
        env, _ = self.create_environment()
        
        total_rewards = []
        total_steps = []
        
        for episode in range(num_episodes):
            observations, _ = env.reset()
            episode_reward = 0
            step_count = 0
            
            while step_count < 1200:  # æå‡æœ€å¤§æ­¥æ•°é™åˆ¶ï¼Œå¢åŠ çœ‹åˆ°æ­£å‘å¥–åŠ±æ¦‚ç‡
                actions = {}
                
                # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥è¯„ä¼°
                for agent in env.agents:
                    if agent in observations:
                        state = tf.expand_dims(observations[agent], 0)
                        action_probs = self.shared_network.actor(state)
                        action = int(tf.argmax(action_probs[0]))  # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åŠ¨ä½œ
                        actions[agent] = action
                
                observations, rewards, terminations, truncations, infos = env.step(actions)
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if any(terminations.values()) or any(truncations.values()):
                    break
            
            total_rewards.append(episode_reward)
            total_steps.append(step_count)
        
        env.close()
        
        return {
            'mean_reward': np.mean(total_rewards),
            'std_reward': np.std(total_rewards),
            'mean_steps': np.mean(total_steps)
        }
    
    def comprehensive_evaluation(self, num_episodes: int = 10) -> Dict[str, Any]:
        """ğŸ”§ V3 ä¿®å¤: å®Œæ•´çš„ä¸šåŠ¡æŒ‡æ ‡è¯„ä¼°, ä¿®å¤KPIç»Ÿè®¡ç¼ºé™·"""
        print(f"\nğŸ“Š å®Œæ•´ä¸šåŠ¡æŒ‡æ ‡è¯„ä¼° ({num_episodes} å›åˆ)")
        print("=" * 60)
        
        env, _ = self.create_environment()
        
        eval_results = {
            'episode_rewards': [],
            'makespans': [],
            'total_tardiness': [],
            'max_tardiness': [],
            'completed_parts': [],
            'utilizations': [],
        }
        
        for episode in range(num_episodes):
            observations, _ = env.reset()
            episode_reward = 0
            step_count = 0
            
            while step_count < 1500:  # è¿›ä¸€æ­¥æå‡æ­¥æ•°ä¸Šé™ï¼Œç¡®ä¿æœ‰å……åˆ†æ—¶é—´å®Œæˆè®¢å•
                actions = {}
                for agent in env.agents:
                    if agent in observations:
                        state = tf.expand_dims(observations[agent], 0)
                        action_probs = self.shared_network.actor(state)
                        action = int(tf.argmax(action_probs[0]))  # ç¡®å®šæ€§ç­–ç•¥
                        actions[agent] = action
                
                observations, rewards, terminations, truncations, infos = env.step(actions)
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if any(terminations.values()) or any(truncations.values()):
                    break # ä»¿çœŸè‡ªç„¶ç»“æŸï¼Œé€€å‡ºå¾ªç¯
            
            # --- ğŸ”§ V3 å…³é”®ä¿®å¤ ---
            # æ— è®ºå¾ªç¯å¦‚ä½•ç»“æŸ (è‡ªç„¶å®Œæˆæˆ–è¶…æ—¶)ï¼Œéƒ½ç›´æ¥ä»ç¯å¢ƒä¸­è·å–æœ€ç»ˆç»Ÿè®¡æ•°æ®
            # è¿™æ˜¯è·å–çœŸå®KPIçš„å”¯ä¸€å¯é æ–¹æ³•
            final_stats = env.sim.get_final_stats()
            
            eval_results['episode_rewards'].append(episode_reward)
            eval_results['makespans'].append(final_stats.get('makespan', 0))
            eval_results['total_tardiness'].append(final_stats.get('total_tardiness', 0))
            eval_results['max_tardiness'].append(final_stats.get('max_tardiness', 0))
            eval_results['completed_parts'].append(final_stats.get('total_parts', 0))
            eval_results['utilizations'].append(final_stats.get('mean_utilization', 0))
            
            print(f"    âœ… å›åˆ{episode+1}: Makespan={final_stats.get('makespan', 0):.1f}, å®Œæˆ={final_stats.get('total_parts', 0)}, åˆ©ç”¨ç‡={final_stats.get('mean_utilization', 0):.1%}")

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        summary_stats = {
            'mean_reward': np.mean(eval_results['episode_rewards']),
            'std_reward': np.std(eval_results['episode_rewards']),
            'mean_makespan': np.mean(eval_results['makespans']) if eval_results['makespans'] else 0,
            'mean_tardiness': np.mean(eval_results['total_tardiness']) if eval_results['total_tardiness'] else 0,
            'mean_utilization': np.mean(eval_results['utilizations']) if eval_results['utilizations'] else 0,
            'mean_completed_parts': np.mean(eval_results['completed_parts']) if eval_results['completed_parts'] else 0,
        }
        
        eval_results['summary'] = summary_stats
        
        print(f"\nğŸ“Š ä¸šåŠ¡æŒ‡æ ‡æ±‡æ€»:")
        print(f"  å¹³å‡å¥–åŠ±: {summary_stats['mean_reward']:.2f} Â± {summary_stats['std_reward']:.2f}")
        print(f"  å¹³å‡Makespan: {summary_stats['mean_makespan']:.1f} åˆ†é’Ÿ")
        print(f"  å¹³å‡å»¶æœŸæ—¶é—´: {summary_stats['mean_tardiness']:.1f} åˆ†é’Ÿ")
        print(f"  å¹³å‡è®¾å¤‡åˆ©ç”¨ç‡: {summary_stats['mean_utilization']:.1%}")
        print(f"  å¹³å‡å®Œæˆé›¶ä»¶æ•°: {summary_stats['mean_completed_parts']:.1f}")
        
        env.close()
        return eval_results
    
    def train(self, num_episodes: int = 100, steps_per_episode: int = 200, 
              eval_frequency: int = 20):
        """ğŸ”§ V5 å¢å¼ºç‰ˆè®­ç»ƒä¸»å¾ªç¯ - è¯¦ç»†æ—¥å¿—å’ŒKPIç›‘æ§"""
        # ğŸ”§ V5 åº”ç”¨ç³»ç»Ÿä¼˜åŒ–çš„å‚æ•°
        optimized_episodes, optimized_steps = self._optimize_training_params(num_episodes, steps_per_episode)
        
        print(f"ğŸš€ å¼€å§‹PPOè®­ç»ƒ (V5 ç³»ç»Ÿä¼˜åŒ–ç‰ˆ)")
        print(f"ğŸ“Š è®­ç»ƒå‚æ•°: {optimized_episodes}å›åˆ, æ¯å›åˆ{optimized_steps}æ­¥")
        print(f"ğŸ’» ç³»ç»Ÿé…ç½®: {self.system_info['memory_gb']:.1f}GBå†…å­˜, GPU={'âœ…' if self.system_info['gpu_available'] else 'âŒ'}")
        print("=" * 80)
        
        if not validate_config():
            print("âŒ é…ç½®éªŒè¯å¤±è´¥")
            return
        
        # è®­ç»ƒå¼€å§‹æ—¶é—´è®°å½•
        training_start_time = time.time()
        training_start_datetime = datetime.now()
        print(f"ğŸ• è®­ç»ƒå¼€å§‹æ—¶é—´: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        
        # åˆ›å»ºç¯å¢ƒ
        env, buffers = self.create_environment()
        
        best_reward = float('-inf')
        best_makespan = float('inf')
        
        try:
            for episode in range(optimized_episodes):
                iteration_start_time = time.time()
                
                # æ”¶é›†ç»éªŒ
                episode_reward = self.collect_experience(env, buffers, optimized_steps)
                
                # æ›´æ–°ç­–ç•¥
                losses = self.update_policy(buffers)
                
                # è®°å½•ç»Ÿè®¡
                iteration_end_time = time.time()
                iteration_duration = iteration_end_time - iteration_start_time
                self.iteration_times.append(iteration_duration)
                self.episode_rewards.append(episode_reward)
                self.training_losses.append(losses)
                
                # ğŸ”§ V5 æ ¸å¿ƒï¼šæ¯è½®è¿›è¡Œå¿«é€ŸKPIè¯„ä¼°
                if (episode + 1) % 5 == 0 or episode == 0:  # æ¯5è½®æˆ–ç¬¬ä¸€è½®è¯„ä¼°KPI
                    kpi_results = self.quick_kpi_evaluation(num_episodes=2)
                    self.kpi_history.append(kpi_results)
                    
                    # æ›´æ–°æœ€ä½³è®°å½•
                    current_makespan = kpi_results['mean_makespan']
                    if current_makespan < best_makespan:
                        best_makespan = current_makespan
                    
                    print(f"\nğŸ“Š å›åˆ {episode + 1:3d}/{optimized_episodes} | "
                          f"å¥–åŠ±: {episode_reward:8.2f} | "
                          f"ActoræŸå¤±: {losses['actor_loss']:7.4f}")
                    print(f"   â±ï¸  ç”¨æ—¶: {iteration_duration:.1f}s | "
                          f"KPI - Makespan: {current_makespan:.1f}min | "
                          f"åˆ©ç”¨ç‡: {kpi_results['mean_utilization']:.1%} | "
                          f"å®Œæˆ: {kpi_results['mean_completed_parts']:.0f}/33")
                    
                    # ğŸ”§ V5 æ—¶é—´é¢„æµ‹ï¼ˆå‚è€ƒWSLè„šæœ¬ï¼‰
                    if len(self.iteration_times) > 1:
                        avg_time = np.mean(self.iteration_times)
                        remaining_episodes = optimized_episodes - (episode + 1)
                        estimated_remaining = remaining_episodes * avg_time
                        
                        if remaining_episodes > 0:
                            finish_time = time.time() + estimated_remaining
                            finish_str = time.strftime('%H:%M:%S', time.localtime(finish_time))
                            print(f"   ğŸ”® é¢„è®¡å‰©ä½™: {estimated_remaining/60:.1f}min | "
                                  f"å®Œæˆæ—¶é—´: {finish_str}")
                else:
                    # ç®€åŒ–è¾“å‡º
                    if (episode + 1) % 10 == 0:
                        recent_rewards = self.episode_rewards[-10:]
                        avg_reward = np.mean(recent_rewards)
                        
                        print(f"å›åˆ {episode + 1:3d}/{optimized_episodes} | "
                              f"å¥–åŠ±: {episode_reward:8.2f} | "
                              f"å¹³å‡: {avg_reward:8.2f} | "
                              f"ActoræŸå¤±: {losses['actor_loss']:7.4f} | "
                              f"ç”¨æ—¶: {iteration_duration:.1f}s")
                
                # å®šæœŸè¯¦ç»†è¯„ä¼°å’Œæ¨¡å‹ä¿å­˜
                if (episode + 1) % eval_frequency == 0:
                    print(f"\nğŸ” ç¬¬{episode + 1}å›åˆè¯¦ç»†è¯„ä¼°...")
                    eval_results = self.simple_evaluation()
                    print(f"   è¯„ä¼°å¥–åŠ±: {eval_results['mean_reward']:.2f} Â± {eval_results['std_reward']:.2f}")
                    print(f"   å¹³å‡æ­¥æ•°: {eval_results['mean_steps']:.1f}")
                    
                    # ğŸ”§ V3.1 ä¿®å¤: æ­£ç¡®è·å–å’Œæ‰“å°å½“å‰å­¦ä¹ ç‡çš„å€¼
                    optimizer_step = self.shared_network.actor_optimizer.iterations
                    current_lr_value = self.shared_network.actor_optimizer.learning_rate(optimizer_step)
                    print(f"   å½“å‰å­¦ä¹ ç‡: {current_lr_value.numpy():.6f}")
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if eval_results['mean_reward'] > best_reward:
                        best_reward = eval_results['mean_reward']
                        self.save_model(f"{self.models_dir}/best_ppo_model_{self.timestamp}")
                        print(f"   âœ… æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (å¥–åŠ±: {best_reward:.2f})")
                    print()
            
            # ğŸ”§ V5 è®­ç»ƒå®Œæˆç»Ÿè®¡ï¼ˆå‚è€ƒWSLè„šæœ¬ï¼‰
            training_end_time = time.time()
            training_end_datetime = datetime.now()
            total_training_time = training_end_time - training_start_time
            
            print("\n" + "=" * 80)
            print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
            print(f"ğŸ• è®­ç»ƒå¼€å§‹: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ğŸ è®­ç»ƒç»“æŸ: {training_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f}åˆ†é’Ÿ ({total_training_time:.1f}ç§’)")
            print(f"ğŸ“ˆ æœ€ä½³è¯„ä¼°å¥–åŠ±: {best_reward:.2f}")
            print(f"ğŸ¯ æœ€ä½³Makespan: {best_makespan:.1f}åˆ†é’Ÿ")
            
            # è®­ç»ƒæ•ˆç‡ç»Ÿè®¡
            if self.iteration_times:
                avg_iteration_time = np.mean(self.iteration_times)
                min_iteration_time = np.min(self.iteration_times)
                max_iteration_time = np.max(self.iteration_times)
                print(f"âš¡ å¹³å‡æ¯è½®: {avg_iteration_time:.1f}s | "
                      f"æœ€å¿«: {min_iteration_time:.1f}s | "
                      f"æœ€æ…¢: {max_iteration_time:.1f}s")
            
            # KPIè¶‹åŠ¿åˆ†æ
            if self.kpi_history:
                initial_makespan = self.kpi_history[0]['mean_makespan']
                final_makespan = self.kpi_history[-1]['mean_makespan']
                makespan_improvement = (initial_makespan - final_makespan) / initial_makespan * 100
                
                initial_utilization = self.kpi_history[0]['mean_utilization']
                final_utilization = self.kpi_history[-1]['mean_utilization']
                utilization_improvement = (final_utilization - initial_utilization) * 100
                
                print(f"ğŸ“Š KPIæ”¹è¿›:")
                print(f"   Makespan: {initial_makespan:.1f}â†’{final_makespan:.1f}min "
                      f"({'æ”¹è¿›' if makespan_improvement > 0 else 'é€€åŒ–'}{abs(makespan_improvement):.1f}%)")
                print(f"   åˆ©ç”¨ç‡: {initial_utilization:.1%}â†’{final_utilization:.1%} "
                      f"({'æå‡' if utilization_improvement > 0 else 'é™ä½'}{abs(utilization_improvement):.1f}%)")
            
            # ğŸ”§ æœ€ç»ˆå®Œæ•´è¯„ä¼°ï¼ˆåŒ…å«çœŸå®ä¸šåŠ¡æŒ‡æ ‡ï¼‰
            print("\nğŸ“Š æœ€ç»ˆå®Œæ•´è¯„ä¼°...")
            final_eval = self.comprehensive_evaluation(num_episodes=10)
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            self.save_model(f"{self.models_dir}/final_ppo_model_{self.timestamp}")
            
            return {
                'training_time': total_training_time,
                'best_reward': best_reward,
                'best_makespan': best_makespan,
                'final_eval': final_eval,
                'episode_rewards': self.episode_rewards,
                'kpi_history': self.kpi_history,
                'iteration_times': self.iteration_times
            }
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None
        
        finally:
            env.close()
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        try:
            self.shared_network.actor.save(f"{filepath}_actor.keras")
            self.shared_network.critic.save(f"{filepath}_critic.keras")
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {filepath}_actor.keras")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ Wå·¥å‚è®¢å•æ€ç»´é©å‘½PPOè®­ç»ƒç³»ç»Ÿ V5")
    print("ğŸ¯ å¥–åŠ±é©å‘½ï¼šä»é›¶ä»¶æ€ç»´åˆ°è®¢å•æ€ç»´çš„æ ¹æœ¬æ€§è½¬å˜")
    print("ğŸ”§ V5æ–°ç‰¹æ€§: ç³»ç»Ÿèµ„æºä¼˜åŒ– + GPUåŠ é€Ÿ + è¯¦ç»†è®­ç»ƒæ—¥å¿— + å®æ—¶KPIç›‘æ§")
    print("ğŸ”§ é©å‘½é¡¹: è®¢å•å¥–åŠ±5000 vs é›¶ä»¶å¥–åŠ±1 (5000:1å‹å€’æ€§ä¼˜åŠ¿) + ä¸¥å‰é—å¼ƒæƒ©ç½š")
    print("=" * 80)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    tf.random.set_seed(RANDOM_SEED)
    
    try:
        # ğŸ”§ V5 ç³»ç»Ÿä¼˜åŒ–: æ ¹æ®ç¡¬ä»¶é…ç½®åŠ¨æ€è°ƒæ•´è®­ç»ƒå‚æ•°
        num_episodes = 120
        steps_per_episode = 1200
        
        trainer = SimplePPOTrainer(
            initial_lr=1e-4,
            total_train_episodes=num_episodes,
            steps_per_episode=steps_per_episode
        )
        
        # å¼€å§‹è®­ç»ƒï¼ˆç³»ç»Ÿä¼šè‡ªåŠ¨æ ¹æ®èµ„æºè°ƒæ•´å‚æ•°ï¼‰
        results = trainer.train(
            num_episodes=num_episodes,
            steps_per_episode=steps_per_episode,
            eval_frequency=20       # è¯„ä¼°é¢‘ç‡
        )
        
        if results:
            print("\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
            
            # ğŸ”§ V5 å¢å¼ºç‰ˆç»“æœåˆ†æ
            final_summary = results['final_eval']['summary']
            print(f"\nğŸ“Š æœ€ç»ˆä¸šåŠ¡è¡¨ç°:")
            print(f"  å¥–åŠ±: {final_summary['mean_reward']:.2f} Â± {final_summary['std_reward']:.2f}")
            print(f"  Makespan: {final_summary['mean_makespan']:.1f} åˆ†é’Ÿ")
            print(f"  å»¶æœŸæ—¶é—´: {final_summary['mean_tardiness']:.1f} åˆ†é’Ÿ")
            print(f"  è®¾å¤‡åˆ©ç”¨ç‡: {final_summary['mean_utilization']:.1%}")
            print(f"  å®Œæˆé›¶ä»¶æ•°: {final_summary['mean_completed_parts']:.1f}")
            print(f"  æœ€ä½³è®­ç»ƒMakespan: {results['best_makespan']:.1f} åˆ†é’Ÿ")
            
            # ğŸ”§ V5 è®­ç»ƒæ•ˆç‡åˆ†æ
            training_time_min = results['training_time'] / 60
            if 'iteration_times' in results and results['iteration_times']:
                total_iterations = len(results['iteration_times'])
                avg_per_iteration = results['training_time'] / total_iterations
                print(f"\nâš¡ è®­ç»ƒæ•ˆç‡åˆ†æ:")
                print(f"  æ€»è®­ç»ƒæ—¶é•¿: {training_time_min:.1f}åˆ†é’Ÿ")
                print(f"  å¹³å‡æ¯è½®æ—¶é—´: {avg_per_iteration:.1f}ç§’")
                print(f"  è®­ç»ƒæ€»è½®æ•°: {total_iterations}è½®")
                print(f"  è®­ç»ƒæ•ˆç‡: {total_iterations/training_time_min:.1f}è½®/åˆ†é’Ÿ")
            
            # ğŸ”§ V5 KPIè¶‹åŠ¿åˆ†æ
            if 'kpi_history' in results and results['kpi_history']:
                kpi_history = results['kpi_history']
                print(f"\nğŸ“ˆ KPIè®­ç»ƒè¶‹åŠ¿:")
                print(f"  åˆå§‹Makespan: {kpi_history[0]['mean_makespan']:.1f}min")
                print(f"  æœ€ç»ˆMakespan: {kpi_history[-1]['mean_makespan']:.1f}min")
                print(f"  åˆå§‹åˆ©ç”¨ç‡: {kpi_history[0]['mean_utilization']:.1%}")
                print(f"  æœ€ç»ˆåˆ©ç”¨ç‡: {kpi_history[-1]['mean_utilization']:.1%}")
                print(f"  KPIç›‘æ§ç‚¹æ•°: {len(kpi_history)}ä¸ª")
            
            # ç¨³å®šæ€§åˆ†æ
            rewards_history = results['episode_rewards']
            if len(rewards_history) >= 20:
                early_avg = np.mean(rewards_history[:20])
                late_avg = np.mean(rewards_history[-20:])
                stability = abs(late_avg - early_avg) / (abs(early_avg) + 1e-8) * 100
                print(f"\nğŸ” å­¦ä¹ ç¨³å®šæ€§åˆ†æ:")
                print(f"  å‰20å›åˆå¹³å‡å¥–åŠ±: {early_avg:.2f}")
                print(f"  å20å›åˆå¹³å‡å¥–åŠ±: {late_avg:.2f}")
                print(f"  æ³¢åŠ¨å¹…åº¦: {stability:.1f}%")
                if stability < 10:
                    print("  âœ… å­¦ä¹ è¿‡ç¨‹è¾ƒä¸ºç¨³å®š")
                else:
                    print("  âš ï¸ å­¦ä¹ è¿‡ç¨‹å­˜åœ¨è¾ƒå¤§æ³¢åŠ¨ï¼Œå»ºè®®è¿›ä¸€æ­¥è°ƒæ•´è¶…å‚æ•°")
        else:
            print("\nâŒ è®­ç»ƒå¤±è´¥")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
