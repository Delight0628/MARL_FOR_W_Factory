"""
çº¯å‡€çš„å¤šæ™ºèƒ½ä½“PPOè®­ç»ƒè„šæœ¬
ä¸“æ³¨äºæ ¸å¿ƒè®­ç»ƒåŠŸèƒ½ï¼Œç§»é™¤å¤æ‚çš„è¯„ä¼°å’Œå¯è§†åŒ–
"""

import os
# ğŸ”§ V10.2 ç»ˆææ—¥å¿—æ¸…ç†: åœ¨æ‰€æœ‰åº“å¯¼å…¥å‰ï¼Œå¼ºåˆ¶è®¾ç½®æ—¥å¿—çº§åˆ«
# è¿™èƒ½æœ€æœ‰æ•ˆåœ°å±è”½æ‰CUDAå’ŒcuBLASåœ¨å­è¿›ç¨‹ä¸­çš„åˆå§‹åŒ–é”™è¯¯ä¿¡æ¯
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import sys
import time
import random
import numpy as np
import tensorflow as tf
from typing import Dict, List, Tuple, Any
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

# ğŸ”§ V12 æ–°å¢ï¼šTensorBoardæ”¯æŒ
try:
    from tensorflow.python.summary.writer.writer import FileWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False

# V10.1ä¸­è®¾ç½®çš„æ—¥å¿—çº§åˆ«ç°åœ¨ç”±æ–‡ä»¶é¡¶éƒ¨çš„ç¯å¢ƒå˜é‡æ¥ç®¡ï¼Œæ•…ç§»é™¤
# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
# tf.get_logger().setLevel('ERROR')

# æ·»åŠ ç¯å¢ƒè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from environments.w_factory_env import make_parallel_env
from environments.w_factory_config import *

class ExperienceBuffer:
    """ç»éªŒç¼“å†²åŒº"""
    
    def __init__(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.action_probs = []
        self.dones = []
        
    def store(self, state, action, reward, value, action_prob, done):
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.values.append(value)
        self.action_probs.append(action_prob)
        self.dones.append(done)
    
    def get_batch(self, gamma=0.99, lam=0.95):
        states = np.array(self.states)
        actions = np.array(self.actions)
        rewards = np.array(self.rewards)
        values = np.array(self.values)
        action_probs = np.array(self.action_probs)
        dones = np.array(self.dones)
        
        advantages = np.zeros_like(rewards)
        last_advantage = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
            advantages[t] = delta + gamma * lam * (1 - dones[t]) * last_advantage
            last_advantage = advantages[t]
        
        returns = advantages + values
        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)
        
        return states, actions, action_probs, advantages, returns
    
    def clear(self):
        self.states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.values.clear()
        self.action_probs.clear()
        self.dones.clear()

class PPONetwork:
    """PPOç½‘ç»œå®ç°"""
    
    # ğŸ”§ V3 ä¿®å¤: lrå‚æ•°ç°åœ¨å¯ä»¥æ˜¯å­¦ä¹ ç‡è°ƒåº¦å™¨
    def __init__(self, state_dim: int, action_dim: int, lr: Any):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.lr = lr
        
        # æ„å»ºç½‘ç»œ
        self.actor, self.critic = self._build_networks()
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = tf.keras.optimizers.Adam(lr)
        self.critic_optimizer = tf.keras.optimizers.Adam(lr)
        
    def _build_networks(self):
        """ğŸ”§ V6 æ„å»ºActor-Criticç½‘ç»œ - å†…å­˜å‹å¥½ç‰ˆæœ¬"""
        # ğŸ”§ V6 æ ¹æ®ç³»ç»Ÿèµ„æºåŠ¨æ€è°ƒæ•´ç½‘ç»œå¤§å°
        available_gb = getattr(self, 'system_info', {}).get('available_gb', 8.0)
        
        if available_gb < 5.0:
            # ä½å†…å­˜ï¼šå°ç½‘ç»œ
            hidden_sizes = [128, 64]
            
        elif available_gb < 8.0:
            # ä¸­ç­‰å†…å­˜ï¼šä¸­å‹ç½‘ç»œ
            hidden_sizes = [256, 128]
        else:
            # å……è¶³å†…å­˜ï¼šå¤§å‹ç½‘ç»œ - ğŸš€ V12 æ¨¡å‹å®¹é‡æå‡
            hidden_sizes = [1024, 512]
        
        # Actorç½‘ç»œ
        actor_input = tf.keras.layers.Input(shape=(self.state_dim,))
        actor_x = tf.keras.layers.Dense(hidden_sizes[0], activation='relu')(actor_input)
        actor_x = tf.keras.layers.Dropout(0.1)(actor_x)  # ğŸ”§ V6 æ·»åŠ dropouté˜²è¿‡æ‹Ÿåˆ
        actor_x = tf.keras.layers.Dense(hidden_sizes[1], activation='relu')(actor_x)
        actor_output = tf.keras.layers.Dense(self.action_dim, activation='softmax')(actor_x)
        actor = tf.keras.Model(inputs=actor_input, outputs=actor_output)
        
        # Criticç½‘ç»œ
        critic_input = tf.keras.layers.Input(shape=(self.state_dim,))
        critic_x = tf.keras.layers.Dense(hidden_sizes[0], activation='relu')(critic_input)
        critic_x = tf.keras.layers.Dropout(0.1)(critic_x)  # ğŸ”§ V6 æ·»åŠ dropouté˜²è¿‡æ‹Ÿåˆ
        critic_x = tf.keras.layers.Dense(hidden_sizes[1], activation='relu')(critic_x)
        critic_output = tf.keras.layers.Dense(1)(critic_x)
        critic = tf.keras.Model(inputs=critic_input, outputs=critic_output)
        
        return actor, critic
    
    def get_action_and_value(self, state: np.ndarray) -> Tuple[int, float, float]:
        """è·å–åŠ¨ä½œã€åŠ¨ä½œæ¦‚ç‡å’ŒçŠ¶æ€ä»·å€¼"""
        state = tf.expand_dims(state, 0)
        
        action_probs = self.actor(state)
        action_dist = tf.random.categorical(tf.math.log(action_probs + 1e-8), 1)
        action = int(action_dist[0, 0])
        
        action_prob = float(action_probs[0, action])
        value = float(self.critic(state)[0, 0])
        
        return action, action_prob, value
    
    def get_value(self, state: np.ndarray) -> float:
        """è·å–çŠ¶æ€ä»·å€¼"""
        state = tf.expand_dims(state, 0)
        return float(self.critic(state)[0, 0])
    
    def update(self, states: np.ndarray, actions: np.ndarray, 
               old_probs: np.ndarray, advantages: np.ndarray, 
               returns: np.ndarray, clip_ratio: float = 0.15) -> Dict[str, float]:  # ğŸ”§ é™ä½è£å‰ªèŒƒå›´
        """PPOæ›´æ–°"""
        
        # Actoræ›´æ–°
        with tf.GradientTape() as tape:
            action_probs = self.actor(states)
            action_probs_selected = tf.reduce_sum(
                action_probs * tf.one_hot(actions, self.action_dim), axis=1
            )
            
            ratio = action_probs_selected / (old_probs + 1e-8)
            clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)
            actor_loss = -tf.reduce_mean(
                tf.minimum(ratio * advantages, clipped_ratio * advantages)
            )
            
            entropy = -tf.reduce_sum(action_probs * tf.math.log(action_probs + 1e-8), axis=1)
            actor_loss -= 0.01 * tf.reduce_mean(entropy)
        
        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)
        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))
        
        # Criticæ›´æ–°
        with tf.GradientTape() as tape:
            values = tf.squeeze(self.critic(states))
            critic_loss = tf.reduce_mean(tf.square(returns - values))
        
        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)
        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))
        
        return {
            'actor_loss': float(actor_loss),
            'critic_loss': float(critic_loss),
            'entropy': float(tf.reduce_mean(entropy))
        }

# ğŸ”§ V8 æ–°å¢: å¤šè¿›ç¨‹å¹¶è¡Œå·¥ä½œå‡½æ•°
def run_simulation_worker(network_weights: Dict[str, List[np.ndarray]],
                          state_dim: int, action_dim: int, num_steps: int, seed: int) -> Tuple[Dict[str, ExperienceBuffer], float]:
    """
    Worker process for collecting experience in parallel.
    Each worker creates its own environment and network.
    """
    # 1. è®¾ç½®è¿›ç¨‹ç‰¹å®šçš„éšæœºç§å­
    os.environ['CUDA_VISIBLE_DEVICES'] = ''  # ğŸ”§ V10.2 ä¿®æ­£: å¿…é¡»ä¿ç•™ï¼Œç¡®ä¿å­è¿›ç¨‹ä¸è®¿é—®GPU
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

    # 2. åˆ›å»ºæœ¬åœ°ç¯å¢ƒå’Œç½‘ç»œ
    env = make_parallel_env()
    # å­¦ä¹ ç‡æ˜¯å ä½ç¬¦ï¼Œå› ä¸ºå·¥ä½œè¿›ç¨‹ä¸è¿›è¡Œè®­ç»ƒ
    local_network = PPONetwork(state_dim, action_dim, lr=1e-4)
    local_network.actor.set_weights(network_weights['actor'])
    local_network.critic.set_weights(network_weights['critic'])

    buffers = {agent: ExperienceBuffer() for agent in env.possible_agents}
    
    # 3. ğŸ”§ ä¿®å¤ï¼šæ”¶é›†ç»éªŒï¼Œä½¿ç”¨ä¸è¯„ä¼°ä¸€è‡´çš„episodeé•¿åº¦é™åˆ¶
    observations, _ = env.reset()
    episode_rewards = {agent: 0 for agent in env.possible_agents}
    step_count = 0
    collected_steps = 0

    while collected_steps < num_steps:
        actions = {}
        values = {}
        action_probs = {}

        for agent in env.agents:
            if agent in observations:
                action, action_prob, value = local_network.get_action_and_value(observations[agent])
                actions[agent] = action
                values[agent] = value
                action_probs[agent] = action_prob

        next_observations, rewards, terminations, truncations, _ = env.step(actions)
        step_count += 1
        collected_steps += 1

        for agent in env.agents:
            if agent in observations and agent in actions:
                done = terminations.get(agent, False) or truncations.get(agent, False)
                reward = rewards.get(agent, 0)
                buffers[agent].store(
                    observations[agent], actions[agent], reward,
                    values[agent], action_probs[agent], done
                )
                episode_rewards[agent] += reward

        observations = next_observations

        # ğŸ”§ ä¿®å¤ï¼šä¸è¯„ä¼°ä¸€è‡´çš„ç»ˆæ­¢æ¡ä»¶
        if any(terminations.values()) or any(truncations.values()) or step_count >= 1500:
            observations, _ = env.reset()
            step_count = 0  # é‡ç½®episodeæ­¥æ•°è®¡æ•°å™¨

    env.close()

    total_reward = sum(episode_rewards.values())
    return buffers, total_reward

class SimplePPOTrainer:
    """ç®€åŒ–çš„PPOè®­ç»ƒå™¨"""
    
    # ğŸ”§ V5 ç³»ç»Ÿèµ„æºä¼˜åŒ–: æ ¹æ®é…ç½®è°ƒæ•´è®­ç»ƒå‚æ•°
    def __init__(self, initial_lr: float, total_train_episodes: int, steps_per_episode: int):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ğŸ”§ V5 æ€§èƒ½ä¼˜åŒ–ï¼šæ£€æµ‹ç³»ç»Ÿèµ„æº
        self.system_info = self._detect_system_resources()
        self._optimize_tensorflow_settings()
        
        # ğŸ”§ V9 CPUå¹¶è¡Œä¼˜åŒ–: æ™ºèƒ½è°ƒèŠ‚è¿›ç¨‹æ•°ï¼Œé˜²æ­¢å†…å­˜çˆ†ç‚¸
        cpu_cores = self.system_info.get('cpu_count', 4)
        # ä¿ç•™æ ¸å¿ƒç»™ä¸»è¿›ç¨‹å’Œç³»ç»Ÿï¼Œä½¿ç”¨æ ¸å¿ƒæ•°çš„ä¸€åŠä½œä¸ºå·¥ä½œè¿›ç¨‹æ•°ï¼Œå…¼é¡¾æ€§èƒ½ä¸ç¨³å®š
        self.num_workers = min(max(1, cpu_cores // 2), 32)
        print(f"ğŸ”§ V9 CPUå¹¶è¡Œä¼˜åŒ–: å°†ä½¿ç”¨ {self.num_workers} ä¸ªå¹¶è¡Œç¯å¢ƒè¿›è¡Œæ•°æ®é‡‡é›† (æ™ºèƒ½è°ƒèŠ‚)")
        
        # ç¯å¢ƒæ¢æµ‹
        temp_env, _ = self.create_environment()
        self.state_dim = temp_env.observation_space(temp_env.possible_agents[0]).shape[0]
        self.action_dim = temp_env.action_space(temp_env.possible_agents[0]).n
        self.agent_ids = temp_env.possible_agents
        temp_env.close()
        
        print("ğŸ”§ ç¯å¢ƒç©ºé—´æ£€æµ‹:")
        print(f"   è§‚æµ‹ç»´åº¦: {self.state_dim}")
        print(f"   åŠ¨ä½œç»´åº¦: {self.action_dim}")
        print(f"   æ™ºèƒ½ä½“æ•°é‡: {len(self.agent_ids)}")
        
        # ğŸ”§ V5 èµ„æºä¼˜åŒ–ï¼šæ ¹æ®å†…å­˜è°ƒæ•´è®­ç»ƒå‚æ•°
        optimized_episodes, optimized_steps = self._optimize_training_params(
            total_train_episodes, steps_per_episode
        )
        
        # ğŸ”§ V3 ä¿®å¤: åˆ›å»ºå­¦ä¹ ç‡è¡°å‡è°ƒåº¦å™¨
        self.lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
            initial_learning_rate=initial_lr,
            decay_steps=optimized_episodes * optimized_steps,
            end_learning_rate=1e-5,  # è¡°å‡åˆ°è¾ƒä½çš„å€¼
            power=1.0  # çº¿æ€§è¡°å‡
        )

        # å…±äº«ç½‘ç»œ
        self.shared_network = PPONetwork(
            state_dim=self.state_dim,
            action_dim=self.action_dim,
            lr=self.lr_schedule
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.training_losses = []
        self.iteration_times = []  # ğŸ”§ V5 æ–°å¢ï¼šè®°å½•æ¯è½®è®­ç»ƒæ—¶é—´
        self.kpi_history = []      # ğŸ”§ V5 æ–°å¢ï¼šè®°å½•æ¯è½®KPIå†å²
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.models_dir = "è‡ªå®šä¹‰ppo/ppo_models"
        os.makedirs(self.models_dir, exist_ok=True)
        
        # ğŸ”§ V12 æ–°å¢ï¼šTensorBoardæ”¯æŒ
        self.tensorboard_dir = f"è‡ªå®šä¹‰ppo/tensorboard_logs/{self.timestamp}"
        os.makedirs(self.tensorboard_dir, exist_ok=True)
        if TENSORBOARD_AVAILABLE:
            self.train_writer = tf.summary.create_file_writer(f"{self.tensorboard_dir}/train")
            print(f"ğŸ“Š TensorBoardæ—¥å¿—å·²å¯ç”¨: {self.tensorboard_dir}")
            print(f"    ä½¿ç”¨å‘½ä»¤: tensorboard --logdir={self.tensorboard_dir}")
        else:
            self.train_writer = None
            print("âš ï¸  TensorBoardä¸å¯ç”¨")
    
    def _detect_system_resources(self) -> Dict[str, Any]:
        """ğŸ”§ V5 æ–°å¢ï¼šæ£€æµ‹ç³»ç»Ÿèµ„æº"""
        try:
            import psutil  # type: ignore
            cpu_count = psutil.cpu_count()
            memory_info = psutil.virtual_memory()
            memory_gb = memory_info.total / (1024**3)
            available_gb = memory_info.available / (1024**3)
            
            # æ£€æµ‹GPU
            gpu_available = len(tf.config.list_physical_devices('GPU')) > 0
            gpu_memory = 0
            if gpu_available:
                try:
                    gpus = tf.config.list_physical_devices('GPU')
                    for gpu in gpus:
                        gpu_details = tf.config.experimental.get_device_details(gpu)
                        gpu_memory = gpu_details.get('device_name', 'Unknown')
                except:
                    gpu_available = False
            
            system_info = {
                'cpu_count': cpu_count,
                'memory_gb': memory_gb,
                'available_gb': available_gb,
                'gpu_available': gpu_available,
                'gpu_memory': gpu_memory
            }
            
            print("ğŸ’» ç³»ç»Ÿèµ„æºæ£€æµ‹:")
            print(f"   CPUæ ¸å¿ƒæ•°: {cpu_count}")
            print(f"   æ€»å†…å­˜: {memory_gb:.1f}GB")
            print(f"   å¯ç”¨å†…å­˜: {available_gb:.1f}GB")
            print(f"   GPUå¯ç”¨: {'âœ…' if gpu_available else 'âŒ'}")
            if gpu_available:
                print(f"   GPUä¿¡æ¯: {gpu_memory}")
            
            return system_info
            
        except ImportError:
            # å¦‚æœæ²¡æœ‰psutilï¼Œä½¿ç”¨ä¿å®ˆä¼°è®¡
            print("âš ï¸  æ— æ³•æ£€æµ‹ç³»ç»Ÿèµ„æºï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {
                'cpu_count': 4,
                'memory_gb': 8.0,
                'available_gb': 4.0,
                'gpu_available': False,
                'gpu_memory': None
            }
    
    def _optimize_tensorflow_settings(self):
        """ğŸ”§ V7 å¢å¼ºç‰ˆï¼šä¼˜åŒ–TensorFlowè®¾ç½®ï¼Œå……åˆ†åˆ©ç”¨48æ ¸CPU"""
        # å†…å­˜å¢é•¿è®¾ç½®
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            try:
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                print("âœ… GPUå†…å­˜å¢é•¿æ¨¡å¼å·²å¯ç”¨")
            except RuntimeError as e:
                print(f"âš ï¸  GPUè®¾ç½®å¤±è´¥: {e}")
        
        # ğŸ”§ V7 CPUä¼˜åŒ–ï¼šå……åˆ†åˆ©ç”¨48æ ¸CPU
        cpu_count = self.system_info.get('cpu_count', 4)
        available_gb = self.system_info.get('available_gb', 4.0)
        
        if available_gb < 6.0:
            # ä½å†…å­˜æ¨¡å¼ï¼šä¿å®ˆä½¿ç”¨CPU
            tf.config.threading.set_inter_op_parallelism_threads(min(cpu_count // 4, 12))
            tf.config.threading.set_intra_op_parallelism_threads(min(cpu_count // 2, 24))
            print(f"ğŸ”§ ä½å†…å­˜æ¨¡å¼: TensorFlowä½¿ç”¨{min(cpu_count // 4, 12)}ä¸ªinterçº¿ç¨‹, {min(cpu_count // 2, 24)}ä¸ªintraçº¿ç¨‹")
        else:
            # ğŸ”§ V7 é«˜æ€§èƒ½æ¨¡å¼ï¼šæ¿€è¿›ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
            inter_threads = min(cpu_count // 2, 24)  # æœ€å¤š24ä¸ªinterçº¿ç¨‹
            intra_threads = min(cpu_count, 48)       # æœ€å¤š48ä¸ªintraçº¿ç¨‹
            tf.config.threading.set_inter_op_parallelism_threads(inter_threads)
            tf.config.threading.set_intra_op_parallelism_threads(intra_threads)
            print(f"ğŸ”§ V7é«˜æ€§èƒ½æ¨¡å¼: TensorFlowä½¿ç”¨{inter_threads}ä¸ªinterçº¿ç¨‹, {intra_threads}ä¸ªintraçº¿ç¨‹")
            print(f"ğŸš€ CPUä¼˜åŒ–: å……åˆ†åˆ©ç”¨{cpu_count}æ ¸å¿ƒå¤„ç†å™¨")
    
    def _optimize_training_params(self, num_episodes: int, steps_per_episode: int) -> Tuple[int, int]:
        """ğŸ”§ V6 å¼ºåŒ–ç‰ˆï¼šæ ¹æ®ç³»ç»Ÿèµ„æºä¼˜åŒ–è®­ç»ƒå‚æ•°ï¼Œé˜²æ­¢å¡æ­»"""
        available_gb = self.system_info.get('available_gb', 4.0)
        total_gb = self.system_info.get('memory_gb', 8.0)
        
        # ğŸ”§ V6 æ›´ä¿å®ˆçš„å†…å­˜ç­–ç•¥ï¼ŒåŸºäºå®é™…å¯ç”¨å†…å­˜è€Œéæ€»å†…å­˜
        if available_gb < 3.0:
            # å±é™©å†…å­˜ï¼šæåº¦ä¿å®ˆ
            optimized_episodes = min(num_episodes, 40)
            optimized_steps = min(steps_per_episode, 600)
            print("ğŸš¨ å±é™©å†…å­˜æ¨¡å¼: è®­ç»ƒè§„æ¨¡æåº¦ç¼©å‡ï¼ˆé˜²å¡æ­»ï¼‰")
        elif available_gb < 5.0:
            # ä½å†…å­˜ï¼šå¤§å¹…é™ä½å‚æ•°
            optimized_episodes = min(num_episodes, 60)
            optimized_steps = min(steps_per_episode, 800)
            print("âš ï¸  ä½å†…å­˜æ¨¡å¼: è®­ç»ƒè§„æ¨¡å¤§å¹…ç¼©å‡")
        elif available_gb < 7.0:
            # ä¸­ç­‰å†…å­˜ï¼šé€‚åº¦é™ä½å‚æ•° - ğŸ”§ V6 æ›´ä¿å®ˆ
            optimized_episodes = min(num_episodes, 80)
            optimized_steps = min(steps_per_episode, 1000)
            print("ğŸ”§ ä¸­ç­‰å†…å­˜æ¨¡å¼: è®­ç»ƒè§„æ¨¡é€‚åº¦ç¼©å‡")
        elif available_gb < 10.0:
            # è¾ƒå¥½å†…å­˜ï¼šç•¥å¾®é™ä½å‚æ•° - ğŸ”§ V6 æ–°å¢å±‚çº§
            optimized_episodes = min(num_episodes, 90)
            optimized_steps = min(steps_per_episode, 1100)
            print("ğŸ’š è¾ƒå¥½å†…å­˜æ¨¡å¼: è®­ç»ƒè§„æ¨¡ç•¥å¾®è°ƒæ•´")
        else:
            # å……è¶³å†…å­˜: æ€§èƒ½å®Œå…¨é‡Šæ”¾ - ğŸš€ V11 æé™æ€§èƒ½æ¨¡å¼
            optimized_episodes = num_episodes
            optimized_steps = steps_per_episode
            print("âœ… å……è¶³å†…å­˜æ¨¡å¼: æ€§èƒ½å®Œå…¨é‡Šæ”¾ï¼Œä½¿ç”¨å®Œæ•´è®­ç»ƒè§„æ¨¡ï¼")
        
        # ğŸ”§ V6 æ–°å¢ï¼šå†…å­˜ä½¿ç”¨ç‡è­¦å‘Š
        memory_usage_percent = ((total_gb - available_gb) / total_gb) * 100
        if memory_usage_percent > 90:
            print(f"âš ï¸  å½“å‰å†…å­˜ä½¿ç”¨ç‡: {memory_usage_percent:.1f}% - å»ºè®®å…³é—­å…¶ä»–ç¨‹åº")
        
        if optimized_episodes != num_episodes or optimized_steps != steps_per_episode:
            print(f"ğŸ”§ å‚æ•°è°ƒæ•´: {num_episodes}å›åˆÃ—{steps_per_episode}æ­¥ â†’ {optimized_episodes}å›åˆÃ—{optimized_steps}æ­¥")
            print(f"ğŸ’¡ èŠ‚çœå†…å­˜: é¢„è®¡å‡å°‘{((num_episodes*steps_per_episode) - (optimized_episodes*optimized_steps))/(num_episodes*steps_per_episode)*100:.1f}%çš„å†…å­˜ä½¿ç”¨")
        
        return optimized_episodes, optimized_steps
    
    def _check_memory_usage(self) -> bool:
        """ğŸ”§ V6 æ–°å¢ï¼šæ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¿…è¦æ—¶è§¦å‘åƒåœ¾å›æ”¶"""
        try:
            import psutil  # type: ignore
            import gc
            
            memory_info = psutil.virtual_memory()
            available_gb = memory_info.available / (1024**3)
            usage_percent = memory_info.percent
            
            # å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜æ—¶è§¦å‘åƒåœ¾å›æ”¶
            if usage_percent > 95:
                print(f"ğŸ§¹ å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({usage_percent:.1f}%)ï¼Œæ‰§è¡Œåƒåœ¾å›æ”¶...")
                gc.collect()
                tf.keras.backend.clear_session()  # æ¸…ç†TensorFlowä¼šè¯
                return False  # å»ºè®®æš‚åœè®­ç»ƒ
            elif usage_percent > 90:
                print(f"âš ï¸  å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ ({usage_percent:.1f}%)ï¼Œå»ºè®®æ³¨æ„")
                gc.collect()
                return True
            
            return True
        except ImportError:
            return True  # æ— æ³•æ£€æµ‹æ—¶å‡è®¾æ­£å¸¸
    
    def _safe_model_update(self, buffers) -> Dict[str, float]:
        """ğŸ”§ V6 æ–°å¢ï¼šå®‰å…¨çš„æ¨¡å‹æ›´æ–°ï¼ŒåŒ…å«å†…å­˜æ£€æŸ¥"""
        # æ›´æ–°å‰æ£€æŸ¥å†…å­˜
        if not self._check_memory_usage():
            print("âš ï¸  å†…å­˜ä¸è¶³ï¼Œè·³è¿‡æœ¬è½®æ¨¡å‹æ›´æ–°")
            return {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0}
        
        # æ‰§è¡Œæ­£å¸¸çš„ç­–ç•¥æ›´æ–°
        return self.update_policy(buffers)

    def create_environment(self):
        """åˆ›å»ºç¯å¢ƒ"""
        env = make_parallel_env()
        buffers = {
            agent: ExperienceBuffer() 
            for agent in env.possible_agents
        }
        return env, buffers
    
    def collect_experience_parallel(self, buffers, num_steps: int) -> float:
        """ğŸ”§ V8 æ–°å¢ï¼šä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œæ”¶é›†ç»éªŒ"""
        for buffer in buffers.values():
            buffer.clear()

        network_weights = {
            'actor': self.shared_network.actor.get_weights(),
            'critic': self.shared_network.critic.get_weights()
        }
        steps_per_worker = num_steps // self.num_workers
        
        total_reward = 0

        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            futures = []
            for i in range(self.num_workers):
                seed = random.randint(0, 1_000_000)
                future = executor.submit(
                    run_simulation_worker,
                    network_weights,
                    self.state_dim,
                    self.action_dim,
                    steps_per_worker,
                    seed
                )
                futures.append(future)

            for future in as_completed(futures):
                try:
                    worker_buffers, worker_reward = future.result()
                    total_reward += worker_reward
                    
                    for agent_id, worker_buffer in worker_buffers.items():
                        buffers[agent_id].states.extend(worker_buffer.states)
                        buffers[agent_id].actions.extend(worker_buffer.actions)
                        buffers[agent_id].rewards.extend(worker_buffer.rewards)
                        buffers[agent_id].values.extend(worker_buffer.values)
                        buffers[agent_id].action_probs.extend(worker_buffer.action_probs)
                        buffers[agent_id].dones.extend(worker_buffer.dones)
                except Exception as e:
                    print(f"âŒ ä¸€ä¸ªå¹¶è¡Œå·¥ä½œè¿›ç¨‹å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()

        return total_reward
    
    def update_policy(self, buffers) -> Dict[str, float]:
        """æ›´æ–°ç­–ç•¥"""
        all_states = []
        all_actions = []
        all_action_probs = []
        all_advantages = []
        all_returns = []
        
        # åˆå¹¶æ‰€æœ‰æ™ºèƒ½ä½“çš„ç»éªŒ
        for agent, buffer in buffers.items():
            if len(buffer.states) > 0:
                states, actions, action_probs, advantages, returns = buffer.get_batch()
                
                all_states.extend(states)
                all_actions.extend(actions)
                all_action_probs.extend(action_probs)
                all_advantages.extend(advantages)
                all_returns.extend(returns)
                
                buffer.clear()
        
        if len(all_states) == 0:
            return {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0}
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        all_states = np.array(all_states)
        all_actions = np.array(all_actions)
        all_action_probs = np.array(all_action_probs)
        all_advantages = np.array(all_advantages)
        all_returns = np.array(all_returns)
        
        # å¤šæ¬¡æ›´æ–° - ğŸ”§ å¢åŠ è¿­ä»£æ¬¡æ•°æå‡å­¦ä¹ å……åˆ†æ€§
        losses = {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0}
        num_updates = 10  # ä»5å¢åŠ åˆ°10
        
        for _ in range(num_updates):
            batch_losses = self.shared_network.update(
                states=all_states,
                actions=all_actions,
                old_probs=all_action_probs,
                advantages=all_advantages,
                returns=all_returns
            )
            
            for key in losses:
                losses[key] += batch_losses[key] / num_updates
        
        return losses
    
    def quick_kpi_evaluation(self, num_episodes: int = 3) -> Dict[str, float]:
        """ğŸ”§ ä¿®å¤ç‰ˆï¼šå¿«é€ŸKPIè¯„ä¼°ï¼ˆç”¨äºæ¯è½®ç›‘æ§ï¼‰"""
        env, _ = self.create_environment()
        
        total_rewards = []
        makespans = []
        utilizations = []
        completed_parts_list = []
        tardiness_list = []
        
        for episode in range(num_episodes):
            observations, _ = env.reset()
            episode_reward = 0
            step_count = 0
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„æ­¥æ•°é™åˆ¶
            while step_count < 1200:
                actions = {}
                
                # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥è¯„ä¼°
                for agent in env.agents:
                    if agent in observations:
                        state = tf.expand_dims(observations[agent], 0)
                        action_probs = self.shared_network.actor(state)
                        action = int(tf.argmax(action_probs[0]))
                        actions[agent] = action
                
                observations, rewards, terminations, truncations, infos = env.step(actions)
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if any(terminations.values()) or any(truncations.values()):
                    break
            
            # è·å–æœ€ç»ˆç»Ÿè®¡
            final_stats = env.sim.get_final_stats()
            total_rewards.append(episode_reward)
            makespans.append(final_stats.get('makespan', 0))
            utilizations.append(final_stats.get('mean_utilization', 0))
            completed_parts_list.append(final_stats.get('total_parts', 0))
            tardiness_list.append(final_stats.get('total_tardiness', 0))
        
        env.close()
        
        return {
            'mean_reward': np.mean(total_rewards),
            'mean_makespan': np.mean(makespans),
            'mean_utilization': np.mean(utilizations),
            'mean_completed_parts': np.mean(completed_parts_list),
            'mean_tardiness': np.mean(tardiness_list)
        }
    
    def simple_evaluation(self, num_episodes: int = 5) -> Dict[str, float]:
        """ğŸ”§ ä¿®å¤ç‰ˆï¼šç®€å•è¯„ä¼°ï¼Œè¿”å›æ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡"""
        env, _ = self.create_environment()
        
        total_rewards = []
        total_steps = []
        makespans = []
        completed_parts = []
        utilizations = []
        tardiness_list = []
        
        for episode in range(num_episodes):
            observations, _ = env.reset()
            episode_reward = 0
            step_count = 0
            
            while step_count < 1200:
                actions = {}
                
                # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥è¯„ä¼°
                for agent in env.agents:
                    if agent in observations:
                        state = tf.expand_dims(observations[agent], 0)
                        action_probs = self.shared_network.actor(state)
                        action = int(tf.argmax(action_probs[0]))
                        actions[agent] = action
                
                observations, rewards, terminations, truncations, infos = env.step(actions)
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if any(terminations.values()) or any(truncations.values()):
                    break
            
            # ğŸ”§ ä¿®å¤ï¼šè·å–å®Œæ•´çš„ä¸šåŠ¡æŒ‡æ ‡
            final_stats = env.sim.get_final_stats()
            total_rewards.append(episode_reward)
            total_steps.append(step_count)
            makespans.append(final_stats.get('makespan', 0))
            completed_parts.append(final_stats.get('total_parts', 0))
            utilizations.append(final_stats.get('mean_utilization', 0))
            tardiness_list.append(final_stats.get('total_tardiness', 0))
        
        env.close()
        
        return {
            'mean_reward': np.mean(total_rewards),
            'std_reward': np.std(total_rewards),
            'mean_steps': np.mean(total_steps),
            'mean_makespan': np.mean(makespans),
            'mean_completed_parts': np.mean(completed_parts),
            'mean_utilization': np.mean(utilizations),
            'mean_tardiness': np.mean(tardiness_list)
        }
    
    
    def train(self, num_episodes: int = 100, steps_per_episode: int = 200, 
              eval_frequency: int = 20):
        """ğŸ”§ V5 å¢å¼ºç‰ˆè®­ç»ƒä¸»å¾ªç¯ - è¯¦ç»†æ—¥å¿—å’ŒKPIç›‘æ§"""
        # ğŸ”§ V5 åº”ç”¨ç³»ç»Ÿä¼˜åŒ–çš„å‚æ•°
        optimized_episodes, optimized_steps = self._optimize_training_params(num_episodes, steps_per_episode)
        
        print(f"ğŸš€ å¼€å§‹PPOè®­ç»ƒ (V5 ç³»ç»Ÿä¼˜åŒ–ç‰ˆ)")
        print(f"ğŸ“Š è®­ç»ƒå‚æ•°: {optimized_episodes}å›åˆ, æ¯å›åˆ{optimized_steps}æ­¥")
        print(f"ğŸ’» ç³»ç»Ÿé…ç½®: {self.system_info['memory_gb']:.1f}GBå†…å­˜, GPU={'âœ…' if self.system_info['gpu_available'] else 'âŒ'}")
        print("=" * 80)
        
        if not validate_config():
            print("âŒ é…ç½®éªŒè¯å¤±è´¥")
            return
        
        # è®­ç»ƒå¼€å§‹æ—¶é—´è®°å½•
        training_start_time = time.time()
        training_start_datetime = datetime.now()
        print(f"ğŸ• è®­ç»ƒå¼€å§‹æ—¶é—´: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        
        # ğŸ”§ V8 ä¼˜åŒ–: ä¸å†éœ€è¦åˆ›å»ºä¸»ç¯å¢ƒï¼Œåªåˆ›å»ºç¼“å†²åŒº
        buffers = {
            agent: ExperienceBuffer() 
            for agent in self.agent_ids
        }
        
        best_reward = float('-inf')
        best_makespan = float('inf')
        
        try:
            for episode in range(optimized_episodes):
                iteration_start_time = time.time()
                
                # æ”¶é›†ç»éªŒ - ğŸ”§ V8 æ”¹ä¸ºå¹¶è¡Œæ”¶é›†
                collect_start_time = time.time()
                episode_reward = self.collect_experience_parallel(buffers, optimized_steps)
                collect_duration = time.time() - collect_start_time
                
                # ğŸ”§ V6 å®‰å…¨çš„ç­–ç•¥æ›´æ–°ï¼ˆåŒ…å«å†…å­˜æ£€æŸ¥ï¼‰
                update_start_time = time.time()
                losses = self._safe_model_update(buffers)
                update_duration = time.time() - update_start_time
                
                # è®°å½•ç»Ÿè®¡
                iteration_end_time = time.time()
                iteration_duration = iteration_end_time - iteration_start_time
                self.iteration_times.append(iteration_duration)
                self.episode_rewards.append(episode_reward)
                self.training_losses.append(losses)
                
                # ğŸ”§ V12 TensorBoardæ—¥å¿—è®°å½•
                if self.train_writer is not None:
                    with self.train_writer.as_default():
                        tf.summary.scalar('Training/Episode_Reward', episode_reward, step=episode)
                        tf.summary.scalar('Training/Actor_Loss', losses['actor_loss'], step=episode)
                        tf.summary.scalar('Training/Critic_Loss', losses['critic_loss'], step=episode)
                        tf.summary.scalar('Training/Entropy', losses['entropy'], step=episode)
                        tf.summary.scalar('Performance/Iteration_Duration', iteration_duration, step=episode)
                        tf.summary.scalar('Performance/CPU_Collection_Time', collect_duration, step=episode)
                        tf.summary.scalar('Performance/GPU_Update_Time', update_duration, step=episode)
                        self.train_writer.flush()
                
                # ğŸ”§ V11 Checkpoint ä¼˜åŒ–: ç§»é™¤å®šæœŸä¿å­˜ï¼Œåªä¿ç•™æœ€ä½³æ¨¡å‹ä¿å­˜
                
                # ğŸ”§ V12 åˆå¹¶ä¿®å¤ï¼šæ¯è½®éƒ½è¿›è¡ŒKPIè¯„ä¼°å’Œæ˜¾ç¤º
                kpi_results = self.quick_kpi_evaluation(num_episodes=2)
                self.kpi_history.append(kpi_results)
                
                # ğŸ”§ V12 TensorBoard KPIè®°å½•
                if self.train_writer is not None:
                    with self.train_writer.as_default():
                        tf.summary.scalar('KPI/Makespan', kpi_results['mean_makespan'], step=episode)
                        tf.summary.scalar('KPI/Completed_Parts', kpi_results['mean_completed_parts'], step=episode)
                        tf.summary.scalar('KPI/Utilization', kpi_results['mean_utilization'], step=episode)
                        tf.summary.scalar('KPI/Tardiness', kpi_results['mean_tardiness'], step=episode)
                        self.train_writer.flush()
                
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®æ›´æ–°æœ€ä½³è®°å½•ï¼ˆåªæœ‰å½“makespan > 0æ—¶æ‰æ›´æ–°ï¼‰
                current_makespan = kpi_results['mean_makespan']
                if current_makespan > 0 and current_makespan < best_makespan:
                    best_makespan = current_makespan
                
                # ğŸ”§ V12 ç»Ÿä¸€æ˜¾ç¤ºæ ¼å¼ï¼šæ¯è½®éƒ½æ˜¾ç¤ºå®Œæ•´ä¿¡æ¯
                print(f"\nğŸ”‚ å›åˆ {episode + 1:3d}/{optimized_episodes} | "
                      f"å¥–åŠ±: {episode_reward:.1f} | "
                      f"ActoræŸå¤±: {losses['actor_loss']:7.4f}| "
                      f"â±ï¸  æœ¬è½®ç”¨æ—¶: {iteration_duration:.1f}s (CPUé‡‡é›†: {collect_duration:.1f}s, GPUæ›´æ–°: {update_duration:.1f}s)")
                print(f"ğŸ“Š KPI - æ€»å®Œå·¥æ—¶é—´: {current_makespan:.1f}min | "
                      f"å®Œæˆ: {kpi_results['mean_completed_parts']:.0f}/33 | "
                      f"è®¾å¤‡åˆ©ç”¨ç‡: {kpi_results['mean_utilization']:.1%} | "
                      f"å»¶æœŸæ—¶é—´: {kpi_results['mean_tardiness']:.1f}min")
                

                if len(self.iteration_times) > 1: #and (episode + 1) % 10 == 0:
                    avg_time = np.mean(self.iteration_times)
                    remaining_episodes = optimized_episodes - (episode + 1)
                    estimated_remaining = remaining_episodes * avg_time
                    progress_percent = ((episode + 1) / optimized_episodes) * 100
                    
                    if remaining_episodes > 0:
                        finish_time = time.time() + estimated_remaining
                        finish_str = time.strftime('%H:%M:%S', time.localtime(finish_time))
                    recent_rewards = self.episode_rewards[-10:]
                    avg_reward = np.mean(recent_rewards)
                    print(f"ğŸ”® å½“å‰è®­ç»ƒè¿›åº¦: {progress_percent:.1f}% | é¢„è®¡å‰©ä½™æ—¶é—´: {estimated_remaining/60:.1f}min | "
                          f"å®Œæˆæ—¶é—´: {finish_str}\n")
                    #print(f"=========================================================================\n"
                    #      f"ğŸ”® å½“å‰è®­ç»ƒè¿›åº¦: {progress_percent:.1f}% | é¢„è®¡å‰©ä½™æ—¶é—´: {estimated_remaining/60:.1f}min | "
                    #      f"å®Œæˆæ—¶é—´: {finish_str} | è¿‘10è½®å¹³å‡å¥–åŠ±: {avg_reward:.1f}\n"
                    #      f"=========================================================================")
                
                # ğŸ”§ V12 æœ€ä½³æ¨¡å‹æ£€æŸ¥ï¼ˆæ¯è½®éƒ½æ£€æŸ¥ï¼‰
                current_kpi = kpi_results
                if current_kpi:
                    # ğŸ”§ V12 ç»¼åˆè¯„åˆ†æ ‡å‡†ï¼šMakespanæœ€å° + åˆ©ç”¨ç‡æœ€å¤§ + å»¶æœŸæœ€çŸ­
                    # å½’ä¸€åŒ–å„é¡¹æŒ‡æ ‡åˆ°0-1èŒƒå›´ï¼Œç„¶ååŠ æƒæ±‚å’Œ
                    makespan_score = max(0, 1 - current_kpi['mean_makespan'] / 600)  # 600åˆ†é’Ÿä¸ºåŸºå‡†
                    utilization_score = current_kpi['mean_utilization']  # åˆ©ç”¨ç‡æœ¬èº«å°±æ˜¯0-1
                    tardiness_score = max(0, 1 - current_kpi['mean_tardiness'] / 1000)  # 1000åˆ†é’Ÿä¸ºåŸºå‡†
                    completion_score = current_kpi['mean_completed_parts'] / 33  # å®Œæˆç‡0-1
                    
                    # ç»¼åˆè¯„åˆ†ï¼šæƒé‡å¯è°ƒæ•´
                    current_score = (
                        makespan_score * 0.3 +      # Makespanæƒé‡30%
                        utilization_score * 0.2 +   # åˆ©ç”¨ç‡æƒé‡20%
                        tardiness_score * 0.2 +     # å»¶æœŸæƒé‡20%
                        completion_score * 0.3      # å®Œæˆç‡æƒé‡30%
                    )
                    
                    if not hasattr(self, 'best_score'):
                        self.best_score = float('-inf')
                        self.best_kpi = None
                    
                    if current_score > self.best_score:
                        self.best_score = current_score
                        self.best_kpi = current_kpi.copy()
                        self.save_model(f"{self.models_dir}/best_ppo_model_{self.timestamp}")
                        print(f"âœ… æœ€ä½³æ¨¡å‹å·²æ›´æ–°ï¼ç»¼åˆè¯„åˆ†: {current_score:.3f}")
                        print(f"   ğŸ“Š æŒ‡æ ‡è¯¦æƒ… - æ€»å®Œå·¥æ—¶é—´: {current_kpi['mean_makespan']:.1f}min | "
                              f"è®¾å¤‡åˆ©ç”¨ç‡: {current_kpi['mean_utilization']:.1%} | "
                              f"å»¶æœŸæ—¶é—´: {current_kpi['mean_tardiness']:.1f}min | "
                              f"å®Œæˆç‡: {current_kpi['mean_completed_parts']:.0f}/33")
            
            # ğŸ”§ ä¿®å¤ç‰ˆï¼šç®€åŒ–çš„è®­ç»ƒå®Œæˆç»Ÿè®¡
            training_end_time = time.time()
            training_end_datetime = datetime.now()
            total_training_time = training_end_time - training_start_time
            
            print("\n" + "=" * 80)
            print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
            print(f"ğŸ• è®­ç»ƒå¼€å§‹: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ğŸ è®­ç»ƒç»“æŸ: {training_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f}åˆ†é’Ÿ ({total_training_time:.1f}ç§’)")
            
            # è®­ç»ƒæ•ˆç‡ç»Ÿè®¡
            if self.iteration_times:
                avg_iteration_time = np.mean(self.iteration_times)
                print(f"âš¡ å¹³å‡æ¯è½®: {avg_iteration_time:.1f}s | è®­ç»ƒæ•ˆç‡: {len(self.iteration_times)/total_training_time*60:.1f}è½®/åˆ†é’Ÿ")
            
            # ğŸ”§ ä¿®å¤ï¼šæœ€ç»ˆè¯„ä¼°ï¼ˆä½¿ç”¨å¤šå›åˆè·å–ç¨³å®šç»“æœï¼‰
            print(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½è¯„ä¼° (10ä¸ªè¯„ä¼°episode):")
            final_eval = self.simple_evaluation(num_episodes=10)
            
            print(f"   å¹³å‡å¥–åŠ±: {final_eval['mean_reward']:.1f} Â± {final_eval['std_reward']:.1f}")
            print(f"   å¹³å‡æ€»å®Œå·¥æ—¶é—´: {final_eval['mean_makespan']:.1f} åˆ†é’Ÿ")
            print(f"   å¹³å‡å®Œæˆé›¶ä»¶: {final_eval['mean_completed_parts']:.1f}/33 ({final_eval['mean_completed_parts']/33*100:.1f}%)")
            print(f"   å¹³å‡è®¾å¤‡åˆ©ç”¨ç‡: {final_eval['mean_utilization']:.1%}")
            print(f"   å¹³å‡å»¶æœŸæ—¶é—´: {final_eval['mean_tardiness']:.1f} åˆ†é’Ÿ")
            
            # KPIæ”¹è¿›è¶‹åŠ¿ï¼ˆå¦‚æœæœ‰å†å²æ•°æ®ï¼‰
            if len(self.kpi_history) >= 2:
                initial = self.kpi_history[0]
                final_kpi = self.kpi_history[-1]
                
                print(f"\nğŸ“ˆ è®­ç»ƒæ”¹è¿›è¶‹åŠ¿:")
                if initial['mean_makespan'] > 0 and final_kpi['mean_makespan'] > 0:
                    makespan_change = ((initial['mean_makespan'] - final_kpi['mean_makespan']) / initial['mean_makespan']) * 100
                    print(f"   æ€»å®Œå·¥æ—¶é—´: {initial['mean_makespan']:.1f}â†’{final_kpi['mean_makespan']:.1f}min ({makespan_change:+.1f}%)")
                
                util_change = (final_kpi['mean_utilization'] - initial['mean_utilization']) * 100
                print(f"   è®¾å¤‡åˆ©ç”¨ç‡: {initial['mean_utilization']:.1%}â†’{final_kpi['mean_utilization']:.1%} ({util_change:+.1f}%)")
                
                parts_change = final_kpi['mean_completed_parts'] - initial['mean_completed_parts']
                print(f"   å®Œæˆé›¶ä»¶æ•°: {initial['mean_completed_parts']:.1f}â†’{final_kpi['mean_completed_parts']:.1f} ({parts_change:+.1f})")
                
                tardiness_change = final_kpi['mean_tardiness'] - initial['mean_tardiness']
                print(f"   å»¶æœŸæ—¶é—´: {initial['mean_tardiness']:.1f}â†’{final_kpi['mean_tardiness']:.1f}min ({tardiness_change:+.1f})")
                
                # ğŸ”§ V12 æ–°å¢ï¼šæ˜¾ç¤ºæœ€ä½³æ¨¡å‹ä¿¡æ¯
                if hasattr(self, 'best_kpi') and self.best_kpi:
                    print(f"\nğŸ† è®­ç»ƒæœŸé—´æœ€ä½³æ¨¡å‹ (ç¬¬{self.kpi_history.index(self.best_kpi)+1}è½®):")
                    print(f"   ç»¼åˆè¯„åˆ†: {self.best_score:.3f}")
                    print(f"   æ€»å®Œå·¥æ—¶é—´: {self.best_kpi['mean_makespan']:.1f}min")
                    print(f"   è®¾å¤‡åˆ©ç”¨ç‡: {self.best_kpi['mean_utilization']:.1%}")
                    print(f"   å»¶æœŸæ—¶é—´: {self.best_kpi['mean_tardiness']:.1f}min")
                    print(f"   å®Œæˆç‡: {self.best_kpi['mean_completed_parts']:.0f}/33 ({self.best_kpi['mean_completed_parts']/33*100:.1f}%)")
            
            # ğŸ”§ ä¿®å¤ï¼šä¸ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼Œåªä¿ç•™æœ€ä½³æ¨¡å‹
            # self.save_model(f"{self.models_dir}/final_ppo_model_{self.timestamp}")  # å·²ç¦ç”¨
            
            return {
                'training_time': total_training_time,
                'final_eval': final_eval,
                'kpi_history': self.kpi_history,
                'iteration_times': self.iteration_times
            }
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None
        
        finally:
            # ğŸ”§ V8 ä¼˜åŒ–: ä¸»å¾ªç¯ä¸­æ²¡æœ‰envéœ€è¦å…³é—­
            pass
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        try:
            self.shared_network.actor.save(f"{filepath}_actor.keras")
            self.shared_network.critic.save(f"{filepath}_critic.keras")
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {filepath}_actor.keras")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ Wå·¥å‚è®¢å•æ€ç»´é©å‘½PPOè®­ç»ƒç³»ç»Ÿ V12 (æ€§èƒ½æé™ç‰ˆ)")
    print("ğŸ¯ V12 æ ¸å¿ƒå‡çº§: æå‡ç¥ç»ç½‘ç»œå®¹é‡ï¼Œå……åˆ†åˆ©ç”¨RTX 3080 Tiç®—åŠ›")
    print("ğŸš€ V10æ€§èƒ½é©å‘½: é‡‡ç”¨å®‰å…¨çš„Spawnæ¨¡å¼å®ç°ç¨³å®šçš„CPUå¹¶è¡ŒåŠ é€Ÿ")
    print("ğŸ”§ æ ¸å¿ƒä¼˜åŒ–: å½»åº•è§£å†³BrokenProcessPoolé”™è¯¯ï¼Œç¡®ä¿é•¿æ—¶é—´ç¨³å®šè®­ç»ƒ")
    print("ğŸ’¾ å®‰å…¨ç‰¹æ€§: è‡ªåŠ¨å†…å­˜ç›‘æ§ + åƒåœ¾å›æ”¶ + æ£€æŸ¥ç‚¹ä¿å­˜ + åŠ¨æ€ç½‘ç»œè°ƒæ•´")
    print("=" * 80)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    tf.random.set_seed(RANDOM_SEED)
    
    try:
        # ğŸ”§ V12 æ€§èƒ½æé™ç‰ˆï¼šå¢åŠ è®­ç»ƒè½®æ•°å’Œæ­¥æ•°
        num_episodes = 40  # å¢åŠ è®­ç»ƒè½®æ•°ï¼Œç»™æ™ºèƒ½ä½“æ›´å¤šå­¦ä¹ æœºä¼š
        steps_per_episode = 2048  # ä¿æŒè¾ƒé•¿çš„episodeé•¿åº¦  
        
        trainer = SimplePPOTrainer(
            initial_lr=1e-4,
            total_train_episodes=num_episodes,
            steps_per_episode=steps_per_episode
        )
        
        # å¼€å§‹è®­ç»ƒï¼ˆç³»ç»Ÿä¼šè‡ªåŠ¨æ ¹æ®èµ„æºè°ƒæ•´å‚æ•°ï¼‰
        results = trainer.train(
            num_episodes=num_episodes,
            steps_per_episode=steps_per_episode,
            eval_frequency=20       # è¯„ä¼°é¢‘ç‡
        )
        
        if results:
            print("\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        else:
            print("\nâŒ è®­ç»ƒå¤±è´¥")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # ğŸ”§ V10 å…³é”®ä¿®å¤: è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•ä¸º'spawn'ï¼Œé¿å…TensorFlowçš„forkä¸å®‰å…¨é—®é¢˜
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    main()
