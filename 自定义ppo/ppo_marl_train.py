"""
çº¯å‡€çš„å¤šæ™ºèƒ½ä½“PPOè®­ç»ƒè„šæœ¬
ä¸“æ³¨äºæ ¸å¿ƒè®­ç»ƒåŠŸèƒ½ï¼Œç§»é™¤å¤æ‚çš„è¯„ä¼°å’Œå¯è§†åŒ–
"""

import os
# ğŸ”§ V10.2 ç»ˆææ—¥å¿—æ¸…ç†: åœ¨æ‰€æœ‰åº“å¯¼å…¥å‰ï¼Œå¼ºåˆ¶è®¾ç½®æ—¥å¿—çº§åˆ«
# è¿™èƒ½æœ€æœ‰æ•ˆåœ°å±è”½æ‰CUDAå’ŒcuBLASåœ¨å­è¿›ç¨‹ä¸­çš„åˆå§‹åŒ–é”™è¯¯ä¿¡æ¯
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'


import sys
import time
import random
import numpy as np
import tensorflow as tf
from typing import Dict, List, Tuple, Any
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

# ğŸ”§ V12 æ–°å¢ï¼šTensorBoardæ”¯æŒ
try:
    from tensorflow.python.summary.writer.writer import FileWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False

# æ·»åŠ ç¯å¢ƒè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from environments.w_factory_env import make_parallel_env, WFactoryEnv
from environments.w_factory_config import *
# ğŸ”§ V38 æ–°å¢ï¼šå¯¼å…¥ä»»åŠ¡å¯è¡Œæ€§åˆ†æå‡½æ•°
from environments.w_factory_config import validate_config, get_total_parts_count

class ExperienceBuffer:
    """ğŸ”§ MAPPOç»éªŒç¼“å†²åŒº - æ”¯æŒå…¨å±€çŠ¶æ€"""
    
    def __init__(self):
        self.states = []
        self.global_states = []  # ğŸ”§ æ–°å¢ï¼šå­˜å‚¨å…¨å±€çŠ¶æ€
        self.actions = []
        self.rewards = []
        self.values = []
        self.action_probs = []
        self.dones = []
        self.truncateds = []
        
    def store(self, state, global_state, action, reward, value, action_prob, done, truncated=False):
        self.states.append(state)
        self.global_states.append(global_state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.values.append(value)
        self.action_probs.append(action_prob)
        self.dones.append(done)
        self.truncateds.append(truncated)
    
    def get_batch(self, gamma=0.99, lam=0.95, next_value_if_truncated=None):
        """ğŸ”§ MAPPOæ”¹è¿›ï¼šæ­£ç¡®å¤„ç†è½¨è¿¹æˆªæ–­"""
        states = np.array(self.states)
        global_states = np.array(self.global_states) # ğŸ”§ æ–°å¢
        actions = np.array(self.actions)
        rewards = np.array(self.rewards)
        values = np.array(self.values)
        action_probs = np.array(self.action_probs)
        dones = np.array(self.dones)
        truncateds = np.array(self.truncateds)
        
        advantages = np.zeros_like(rewards)
        last_advantage = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ­£ç¡®å¤„ç†æœ€åä¸€æ­¥
                if truncateds[t] and next_value_if_truncated is not None:
                    # æˆªæ–­ï¼šä½¿ç”¨criticé¢„æµ‹çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ä»·å€¼
                    next_value = next_value_if_truncated
                elif dones[t]:
                    # çœŸæ­£ç»ˆæ­¢ï¼šä»·å€¼ä¸º0
                    next_value = 0
                else:
                    # ğŸ”§ ä¿®å¤ï¼šæ—¢ä¸æˆªæ–­ä¹Ÿä¸ç»ˆæ­¢ï¼ˆæ­£å¸¸trajectoryç»“æŸï¼‰
                    # ä½¿ç”¨bootstrapä»·å€¼ï¼ˆå¦‚æœæä¾›ï¼‰
                    next_value = next_value_if_truncated if next_value_if_truncated is not None else 0
            else:
                next_value = values[t + 1]
            
            # GAEè®¡ç®—
            delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
            advantages[t] = delta + gamma * lam * (1 - dones[t]) * last_advantage
            last_advantage = advantages[t]
        
        returns = advantages + values
        
        # ğŸ”§ ä¿®å¤ï¼šæ›´ç¨³å¥çš„ä¼˜åŠ¿æ ‡å‡†åŒ–
        if len(advantages) > 1:
            adv_mean = np.mean(advantages)
            adv_std = np.std(advantages)
            # é¿å…æ ‡å‡†å·®è¿‡å°å¯¼è‡´çš„æ•°å€¼ä¸ç¨³å®š
            if adv_std > 1e-8:
                advantages = (advantages - adv_mean) / (adv_std + 1e-8)
            else:
                advantages = advantages - adv_mean
        
        # ğŸ”§ æ–°å¢ï¼šä¼˜åŠ¿è£å‰ªï¼Œé˜²æ­¢æç«¯å€¼ï¼ˆä½†ä¿ç•™è¶³å¤Ÿçš„åŠ¨æ€èŒƒå›´ï¼‰
        advantages = np.clip(advantages, -5, 5)
        
        return states, global_states, actions, action_probs, advantages, returns
    
    def clear(self):
        self.states.clear()
        self.global_states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.values.clear()
        self.action_probs.clear()
        self.dones.clear()
        self.truncateds.clear()

class PPONetwork:
    """ğŸ”§ MAPPOç½‘ç»œå®ç° - åŒ…å«é›†ä¸­å¼Critic"""
    
    # ğŸ”§ V3 ä¿®å¤: lrå‚æ•°ç°åœ¨å¯ä»¥æ˜¯å­¦ä¹ ç‡è°ƒåº¦å™¨
    def __init__(self, state_dim: int, action_dim: int, lr: Any, global_state_dim: int):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.global_state_dim = global_state_dim # ğŸ”§ æ–°å¢
        self.lr = lr
        
        # æ„å»ºç½‘ç»œ
        self.actor, self.critic = self._build_networks()
        
        # ä¼˜åŒ–å™¨ - ğŸ”§ ä¿®å¤ï¼šå¤„ç†lrä¸ºNoneçš„æƒ…å†µï¼ˆworkerä¸éœ€è¦ä¼˜åŒ–å™¨ï¼‰
        if lr is not None:
            self.actor_optimizer = tf.keras.optimizers.Adam(lr)
            self.critic_optimizer = tf.keras.optimizers.Adam(lr)
        else:
            self.actor_optimizer = None
            self.critic_optimizer = None
        
    def _build_networks(self):
        """ğŸ”§ MAPPOä¼˜åŒ–ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶å‚æ•°æ„å»ºç½‘ç»œ"""
        # å¯¼å…¥é…ç½®
        from environments.w_factory_config import PPO_NETWORK_CONFIG
        hidden_sizes = PPO_NETWORK_CONFIG["hidden_sizes"]
        dropout_rate = PPO_NETWORK_CONFIG["dropout_rate"]
        
        # Actorç½‘ç»œ (å»ä¸­å¿ƒåŒ–) - ä½¿ç”¨å±€éƒ¨è§‚æµ‹
        state_input = tf.keras.layers.Input(shape=(self.state_dim,))
        # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ æ­£ç¡®çš„æƒé‡åˆå§‹åŒ–
        actor_x = tf.keras.layers.Dense(
            hidden_sizes[0], 
            activation='relu',
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=np.sqrt(2)),
            bias_initializer=tf.keras.initializers.Constant(0.0)
        )(state_input)
        actor_x = tf.keras.layers.Dropout(dropout_rate)(actor_x)
        actor_x = tf.keras.layers.Dense(
            hidden_sizes[1], 
            activation='relu',
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=np.sqrt(2)),
            bias_initializer=tf.keras.initializers.Constant(0.0)
        )(actor_x)
        # ğŸ”§ ç­–ç•¥è¾“å‡ºå±‚ä½¿ç”¨è¾ƒå°çš„åˆå§‹åŒ–å€¼
        action_probs = tf.keras.layers.Dense(
            self.action_dim, 
            activation='softmax',
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=0.01),
            bias_initializer=tf.keras.initializers.Constant(0.0)
        )(actor_x)
        actor = tf.keras.Model(inputs=state_input, outputs=action_probs)

        # Criticç½‘ç»œ (ä¸­å¿ƒåŒ–) - ğŸ”§ ä¿®å¤ï¼šç½‘ç»œå¤§å°åº”è¯¥ä¸Actorå¹³è¡¡
        # å…¨å±€çŠ¶æ€æœ¬èº«å·²ç»åŒ…å«äº†æ›´å¤šä¿¡æ¯ï¼Œä¸éœ€è¦è¿‡åº¦å¢å¤§ç½‘ç»œ
        global_state_input = tf.keras.layers.Input(shape=(self.global_state_dim,))
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æƒé‡åˆå§‹åŒ–
        critic_x = tf.keras.layers.Dense(
            hidden_sizes[0],
            activation='relu',
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=np.sqrt(2)),
            bias_initializer=tf.keras.initializers.Constant(0.0)
        )(global_state_input)
        critic_x = tf.keras.layers.Dropout(dropout_rate)(critic_x)
        critic_x = tf.keras.layers.Dense(
            hidden_sizes[1],
            activation='relu',
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=np.sqrt(2)),
            bias_initializer=tf.keras.initializers.Constant(0.0)
        )(critic_x)
        critic_x = tf.keras.layers.Dropout(dropout_rate)(critic_x)
        # ğŸ”§ Valueè¾“å‡ºå±‚ä½¿ç”¨æ ‡å‡†åˆå§‹åŒ–
        value_output = tf.keras.layers.Dense(
            1,
            activation=None,
            kernel_initializer=tf.keras.initializers.Orthogonal(gain=1.0),
            bias_initializer=tf.keras.initializers.Constant(0.0)
        )(critic_x)
        critic = tf.keras.Model(inputs=global_state_input, outputs=value_output)
        
        return actor, critic
    
    def get_action_and_value(self, state: np.ndarray, global_state: np.ndarray) -> Tuple[int, float, float]:
        """è·å–åŠ¨ä½œã€ä»·å€¼å’ŒåŠ¨ä½œæ¦‚ç‡"""
        state_tensor = tf.expand_dims(tf.convert_to_tensor(state), 0)
        probs = self.actor(state_tensor)
        # ğŸ”§ ä¿®å¤ï¼šæ•°å€¼ç¨³å®šæ€§
        probs = tf.clip_by_value(probs, 1e-8, 1.0)
        action = tf.random.categorical(tf.math.log(probs + 1e-8), 1)[0, 0].numpy()
        action_prob = probs[0, action].numpy()

        # ğŸ”§ Criticä½¿ç”¨å…¨å±€çŠ¶æ€
        value = self.critic(tf.expand_dims(tf.convert_to_tensor(global_state), 0))[0, 0].numpy()
        
        return action, float(value), float(action_prob)
    
    def get_value(self, global_state: np.ndarray) -> float:
        """è·å–çŠ¶æ€ä»·å€¼ï¼ˆä»…ä½¿ç”¨å…¨å±€çŠ¶æ€ï¼‰"""
        global_state = tf.expand_dims(global_state, 0)
        return float(self.critic(global_state)[0, 0])
    
    def update(self, states: np.ndarray, global_states: np.ndarray, actions: np.ndarray, 
               old_probs: np.ndarray, advantages: np.ndarray, 
               returns: np.ndarray, clip_ratio: float = None, entropy_coeff: float = None) -> Dict[str, float]:
        """ğŸ”§ MAPPOæ›´æ–°ï¼šCriticä½¿ç”¨å…¨å±€çŠ¶æ€"""
        # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥ä¼˜åŒ–å™¨æ˜¯å¦å­˜åœ¨
        if self.actor_optimizer is None or self.critic_optimizer is None:
            raise ValueError("Optimizers not initialized. Cannot update network.")
            
        # ğŸ”§ V32 ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„PPOå‚æ•°
        if clip_ratio is None:
            clip_ratio = PPO_NETWORK_CONFIG["clip_ratio"]
        # å…³é”®ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„åŠ¨æ€ç†µç³»æ•°
        current_entropy_coeff = entropy_coeff if entropy_coeff is not None else PPO_NETWORK_CONFIG["entropy_coeff"]
        
        # Actoræ›´æ–°
        with tf.GradientTape() as tape:
            probs = self.actor(states)
            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤
            probs = tf.clip_by_value(probs, 1e-8, 1.0)
            dist = tf.compat.v1.distributions.Categorical(probs=probs)
            
            new_probs = dist.prob(actions)
            # ğŸ”§ ä¿®å¤ï¼šé˜²æ­¢é™¤é›¶å’Œæ•°å€¼çˆ†ç‚¸
            ratio = new_probs / (old_probs + 1e-8)
            ratio = tf.clip_by_value(ratio, 0.01, 100.0)  # é˜²æ­¢æç«¯ratio
            
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—KLæ•£åº¦
            old_log_probs = tf.math.log(old_probs + 1e-8)
            new_log_probs = tf.math.log(new_probs + 1e-8)
            approx_kl = tf.reduce_mean(old_probs * (old_log_probs - new_log_probs))
            
            # è®¡ç®—è£å‰ªæ¯”ä¾‹ (ç”¨äºç›‘æ§)
            clipped_mask = tf.greater(tf.abs(ratio - 1.0), clip_ratio)
            clip_fraction = tf.reduce_mean(tf.cast(clipped_mask, tf.float32))

            clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)
            actor_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))
            
            entropy = tf.reduce_mean(dist.entropy())
            actor_loss -= current_entropy_coeff * entropy
            
        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)
        # ğŸ”§ æ–°å¢ï¼šæ¢¯åº¦è£å‰ªä»¥æé«˜è®­ç»ƒç¨³å®šæ€§
        actor_grads, _ = tf.clip_by_global_norm(actor_grads, 1.0)  # å¢åŠ åˆ°1.0ï¼Œå…è®¸æ›´å¤§æ¢¯åº¦
        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))
        
        # Criticæ›´æ–° (ä½¿ç”¨å…¨å±€çŠ¶æ€)
        with tf.GradientTape() as tape:
            values = self.critic(global_states)
            critic_loss = tf.reduce_mean(tf.square(returns - values))
        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)
        # ğŸ”§ æ–°å¢ï¼šæ¢¯åº¦è£å‰ª
        critic_grads, _ = tf.clip_by_global_norm(critic_grads, 1.0)  # ä¸actorä¿æŒä¸€è‡´
        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))
        
        return {
            "actor_loss": actor_loss.numpy(),
            "critic_loss": critic_loss.numpy(),
            "entropy": entropy.numpy(),
            "approx_kl": approx_kl.numpy(),
            "clip_fraction": clip_fraction.numpy()
        }

# ğŸ”§ V8 æ–°å¢: å¤šè¿›ç¨‹å¹¶è¡Œå·¥ä½œå‡½æ•°
def run_simulation_worker(network_weights: Dict[str, List[np.ndarray]],
                          state_dim: int, action_dim: int, num_steps: int, seed: int, 
                          global_state_dim: int, curriculum_config: Dict[str, Any] = None) -> Tuple[Dict[str, ExperienceBuffer], float]:
    """å¹¶è¡Œä»¿çœŸå·¥ä½œè¿›ç¨‹ - ğŸ”§ MAPPOæ”¹é€ ï¼šæ”¶é›†å…¨å±€çŠ¶æ€"""
    
    # ğŸ”§ ç»ˆæä¿®å¤ï¼šå°†tfå¯¼å…¥ç§»è‡³é¡¶éƒ¨ï¼Œè§£å†³UnboundLocalError
    import tensorflow as tf
    import numpy as np
    import random
    
    # 1. åˆå§‹åŒ–
    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # ç¦ç”¨GPU
    tf.config.threading.set_inter_op_parallelism_threads(1)
    tf.config.threading.set_intra_op_parallelism_threads(1)
    
    tf.random.set_seed(seed)
    env = make_parallel_env(curriculum_config)
    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨åŠ¨æ€å­¦ä¹ ç‡è€Œéå›ºå®šå€¼
    # æ³¨æ„ï¼šworkerä¸éœ€è¦å­¦ä¹ ç‡ï¼Œåªåšæ¨ç†
    network = PPONetwork(state_dim, action_dim, None, global_state_dim) # Workerä¸éœ€è¦ä¼˜åŒ–å™¨
    network.actor.set_weights(network_weights['actor'])
    network.critic.set_weights(network_weights['critic']) # ğŸ”§ Criticæƒé‡ä¹Ÿéœ€è¦åŒæ­¥
    
    buffers = {agent: ExperienceBuffer() for agent in env.agents}
    
    observations, infos = env.reset(seed=seed)
    global_state = infos[env.agents[0]]['global_state']
    
    total_reward_collected = 0.0
    collected_steps = 0
    step_count = 0
    
    while collected_steps < num_steps:
        actions = {}
        values = {}
        action_probs = {}
        
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰æ™ºèƒ½ä½“ä½¿ç”¨åŒä¸€ä¸ªå…¨å±€çŠ¶æ€
        current_global_state = global_state.copy() if global_state is not None else np.zeros(global_state_dim)

        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ™ºèƒ½ä½“åŠ¨ä½œçš„åŒæ­¥æ€§
        for agent in env.agents:  # ä½¿ç”¨env.agentsç¡®ä¿é¡ºåºä¸€è‡´
            if agent in observations:
                obs = observations[agent]
                action, value, action_prob = network.get_action_and_value(obs, current_global_state)
                actions[agent] = action
                values[agent] = value
                action_probs[agent] = action_prob
            
        next_observations, rewards, terminations, truncations, infos = env.step(actions)
        step_count += 1
        collected_steps += 1
        global_state = infos[env.agents[0]]['global_state']
        
        
        total_reward_collected += sum(rewards.values())

        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰æ™ºèƒ½ä½“çš„æ•°æ®ä¸€è‡´æ€§
        for agent in env.agents:
            if agent in observations and agent in actions:
                terminated = terminations.get(agent, False)
                truncated = truncations.get(agent, False)
                reward = rewards.get(agent, 0)
                # ğŸ”§ é‡è¦ï¼šå­˜å‚¨æ—¶ä½¿ç”¨ç›¸åŒçš„å…¨å±€çŠ¶æ€
                buffers[agent].store(
                    observations[agent], 
                    current_global_state.copy(),  # ä½¿ç”¨å‰¯æœ¬é¿å…å¼•ç”¨é—®é¢˜
                    actions[agent], 
                    reward,
                    values[agent], 
                    action_probs[agent], 
                    terminated,
                    truncated
                )

        observations = next_observations

        # ğŸ”§ ä¿®å¤ï¼šä¸è¯„ä¼°ä¸€è‡´çš„ç»ˆæ­¢æ¡ä»¶
        if any(terminations.values()) or any(truncations.values()) or step_count >= 1500:
            
            # ğŸ”§ MAPPOå…³é”®ä¿®å¤ï¼šæ­£ç¡®å¤„ç†æˆªæ–­æ—¶çš„bootstrapä»·å€¼
            # æ³¨æ„ï¼šè¿™é‡Œæš‚æ—¶ä¸å¤„ç†ï¼Œè®©bufferè‡ªå·±åœ¨get_batchæ—¶å¤„ç†
            pass
            
        
            # total_reward += sum(episode_rewards.values())
            # é‡ç½®
            observations, infos = env.reset(seed=seed)
            global_state = infos[env.agents[0]]['global_state']
            step_count = 0  # é‡ç½®episodeæ­¥æ•°è®¡æ•°å™¨

    env.close()
    return buffers, total_reward_collected

class SimplePPOTrainer:
    """ğŸ”§ V31 è‡ªé€‚åº”PPOè®­ç»ƒå™¨ï¼šæ ¹æ®è®­ç»ƒçŠ¶æ€è‡ªåŠ¨è°ƒæ•´è®­ç»ƒç­–ç•¥"""
    
    # ğŸ”§ V31 æ–°å¢ï¼šæ”¯æŒè‡ªé€‚åº”è®­ç»ƒç›®æ ‡å’ŒåŠ¨æ€è½®æ•°è°ƒæ•´
    def __init__(self, initial_lr: float, total_train_episodes: int, steps_per_episode: int, training_targets: dict = None):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        
        # ğŸ”§ V32 ä½¿ç”¨é…ç½®æ–‡ä»¶çš„ç³»ç»Ÿèµ„æºé…ç½®
        self.num_workers = SYSTEM_CONFIG["num_parallel_workers"]
        print(f"ğŸ”§ ä½¿ç”¨ {self.num_workers} ä¸ªå¹¶è¡Œç¯å¢ƒè¿›è¡Œæ•°æ®é‡‡é›†")
        
        # ğŸ”§ V32 ä½¿ç”¨é…ç½®æ–‡ä»¶çš„TensorFlowçº¿ç¨‹é…ç½®
        tf.config.threading.set_inter_op_parallelism_threads(SYSTEM_CONFIG["tf_inter_op_threads"])
        tf.config.threading.set_intra_op_parallelism_threads(SYSTEM_CONFIG["tf_intra_op_threads"])
        print(f"ğŸ”§ TensorFlowå°†ä½¿ç”¨ {SYSTEM_CONFIG['tf_inter_op_threads']}ä¸ªinterçº¿ç¨‹, {SYSTEM_CONFIG['tf_intra_op_threads']}ä¸ªintraçº¿ç¨‹")
        
        # ç¯å¢ƒæ¢æµ‹
        # ä¹‹å‰çš„ä»£ç ä¾èµ–åŠ¨æ€é…ç½®ï¼Œç°åœ¨æˆ‘ä»¬ç›´æ¥åˆ›å»º
        temp_env = make_parallel_env()
        self.state_dim = temp_env.observation_space(temp_env.possible_agents[0]).shape[0]
        self.action_dim = temp_env.action_space(temp_env.possible_agents[0]).n
        self.global_state_dim = temp_env.global_state_space.shape[0]
        self.agent_ids = temp_env.possible_agents
        temp_env.close()
        
        print("ğŸ”§ ç¯å¢ƒç©ºé—´æ£€æµ‹:")
        print(f"   è§‚æµ‹ç»´åº¦: {self.state_dim}")
        print(f"   åŠ¨ä½œç»´åº¦: {self.action_dim}")
        print(f"   æ™ºèƒ½ä½“æ•°é‡: {len(self.agent_ids)}")
        
        # ğŸ”§ V26 ç»ˆæä¿®å¤ï¼šç§»é™¤åŠ¨æ€å‚æ•°è°ƒæ•´
        optimized_episodes = total_train_episodes
        optimized_steps = steps_per_episode
        
        # ğŸ”§ V32 ä½¿ç”¨é…ç½®æ–‡ä»¶çš„å­¦ä¹ ç‡è°ƒåº¦é…ç½®
        self.lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
            initial_learning_rate=LEARNING_RATE_CONFIG["initial_lr"],
            decay_steps=optimized_episodes * optimized_steps,
            end_learning_rate=LEARNING_RATE_CONFIG["end_lr"],
            power=LEARNING_RATE_CONFIG["decay_power"]
        )

        # å…±äº«ç½‘ç»œ
        self.shared_network = PPONetwork(
            state_dim=self.state_dim,
            action_dim=self.action_dim,
            lr=self.lr_schedule,
            global_state_dim=self.global_state_dim
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.training_losses = []
        self.iteration_times = []  # ğŸ”§ V5 æ–°å¢ï¼šè®°å½•æ¯è½®è®­ç»ƒæ—¶é—´
        self.kpi_history = []      # ğŸ”§ V5 æ–°å¢ï¼šè®°å½•æ¯è½®KPIå†å²
        self.initial_lr = initial_lr  # ğŸ”§ V19 ä¿®å¤: ä¿å­˜åˆå§‹å­¦ä¹ ç‡
        self.start_time = time.time()
        self.start_time_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ğŸ”§ æ ¸å¿ƒæ”¹é€ ï¼šæ–°å¢"æœ€ç»ˆé˜¶æ®µ"æœ€ä½³KPIè·Ÿè¸ªå™¨
        self.final_stage_best_kpi = {
            'mean_completed_parts': -1.0,
            'mean_makespan': float('inf'),
            'mean_utilization': 0.0,
            'mean_tardiness': float('inf')
        }
        self.final_stage_best_score = -1.0
        self.final_stage_best_episode = -1 # ğŸ”§ æ–°å¢ï¼šè®°å½•æœ€ä½³KPIçš„å›åˆæ•°
        
        # ğŸ”§ æ ¸å¿ƒæ”¹é€ ï¼šæ–°å¢"åŒè¾¾æ ‡"æœ€ä½³KPIè·Ÿè¸ªå™¨
        self.best_kpi_dual_objective = self.final_stage_best_kpi.copy()
        self.best_score_dual_objective = -1.0
        self.best_episode_dual_objective = -1

        # ğŸ”§ V32 ç»Ÿä¸€ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è‡ªé€‚åº”è®­ç»ƒé…ç½®
        self.training_targets = training_targets or ADAPTIVE_TRAINING_CONFIG.copy()
        
        # ğŸ”§ V31 æ–°å¢ï¼šè‡ªé€‚åº”è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.adaptive_state = {
            "target_achieved_count": 0,          # è¿ç»­è¾¾åˆ°ç›®æ ‡çš„æ¬¡æ•°
            "best_performance": 0.0,             # å†å²æœ€ä½³æ€§èƒ½
            "last_improvement_episode": 0,       # ä¸Šæ¬¡æ”¹è¿›çš„è½®æ•°
            "performance_history": [],           # æ€§èƒ½å†å²è®°å½•
            "training_phase": "exploration",     # å½“å‰è®­ç»ƒé˜¶æ®µï¼šexploration, exploitation, fine_tuning
            "stagnation_counter": 0,             # åœæ»è®¡æ•°å™¨
            "last_stagnation_performance": -1.0, # ä¸Šä¸€æ¬¡åœæ»æ—¶çš„æ€§èƒ½
        }
        # ğŸ”§ V34 åˆå§‹åŒ–åŠ¨æ€è®­ç»ƒå‚æ•°
        self.current_entropy_coeff = PPO_NETWORK_CONFIG["entropy_coeff"] # åˆå§‹åŒ–åŠ¨æ€ç†µç³»æ•°
        self.current_learning_rate = LEARNING_RATE_CONFIG["initial_lr"] # ğŸ”§ V34 ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å­¦ä¹ ç‡é…ç½®
        
        # ğŸ”§ æ–°å¢ï¼šç†µç³»æ•°é€€ç«è®¡åˆ’ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        self.entropy_decay_rate = 0.999  # æ›´æ…¢çš„è¡°å‡ç‡
        self.min_entropy_coeff = 0.02    # æ›´é«˜çš„æœ€å°ç†µç³»æ•°ï¼Œä¿æŒåŸºæœ¬æ¢ç´¢
        
        
        # ğŸ”§ V40 æ–°å¢ï¼šå›åˆäº‹ä»¶æ—¥å¿—è®°å½•å™¨
        self.episode_events = []
        
        # åˆ›å»ºä¿å­˜ç›®å½• (V31æ–°å¢ï¼šä»¥è®­ç»ƒå¼€å§‹æ—¶é—´åˆ›å»ºä¸“ç”¨æ–‡ä»¶å¤¹)
        self.base_models_dir = "è‡ªå®šä¹‰ppo/ppo_models"
        self.models_dir = f"{self.base_models_dir}/{self.start_time_str}"
        os.makedirs(self.models_dir, exist_ok=True)
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜ç›®å½•: {self.models_dir}")
        
        # ğŸ”§ V12 æ–°å¢ï¼šTensorBoardæ”¯æŒ
        self.tensorboard_dir = f"è‡ªå®šä¹‰ppo/tensorboard_logs/{self.timestamp}"
        os.makedirs(self.tensorboard_dir, exist_ok=True)
        if TENSORBOARD_AVAILABLE:
            self.train_writer = None
            self.current_tensorboard_run_name = None
            print(f"ğŸ“Š TensorBoardå‘½ä»¤: tensorboard --logdir=\"{self.tensorboard_dir}\"")
        else:
            self.train_writer = None
            print("âš ï¸  TensorBoardä¸å¯ç”¨")
    
    def should_continue_training(self, episode: int, current_score: float, completion_rate: float) -> tuple:
        """ğŸ”§ æ ¸å¿ƒæ”¹é€ ï¼šè¯„ä¼°æ˜¯å¦åº”è¯¥ç»§ç»­è®­ç»ƒï¼ŒåŸºäº"ç»¼åˆè¯„åˆ†"""
        targets = self.training_targets
        state = self.adaptive_state
        
        # åŸºæœ¬é™åˆ¶æ£€æŸ¥ (ç§»é™¤min_episodesæ£€æŸ¥)
        if episode >= targets["max_episodes"]:
            return False, f"å·²è¾¾åˆ°æœ€å¤§è®­ç»ƒè½®æ•°({targets['max_episodes']})", 0
        
        # æ ¸å¿ƒé€»è¾‘ï¼šå¿…é¡»åŒæ—¶æ»¡è¶³100%å®Œæˆç‡å’Œç›®æ ‡åˆ†æ•°
        target_score = targets["target_score"]
        if completion_rate >= 100 and current_score >= target_score:
            state["target_achieved_count"] += 1
            print(f"ğŸ¯ åŒé‡ç›®æ ‡è¾¾æˆ: å®Œæˆç‡ {completion_rate:.1f}% & åˆ†æ•° {current_score:.3f} (è¿ç»­ç¬¬{state['target_achieved_count']}æ¬¡)")
            
            if state["target_achieved_count"] >= targets["target_consistency"]:
                return False, f"è¿ç»­{targets['target_consistency']}æ¬¡è¾¾åˆ°åŒé‡ç›®æ ‡", 0
        else:
            # ä»»ä½•ä¸€ä¸ªä¸æ»¡è¶³ï¼Œè®¡æ•°å™¨å°±é‡ç½®
            state["target_achieved_count"] = 0

        # æ—©åœé€»è¾‘ (ä¿æŒä¸å˜ï¼ŒåŸºäºåˆ†æ•°)
        state["performance_history"].append(current_score)
        if len(state["performance_history"]) > targets["performance_window"]:
            state["performance_history"].pop(0)

        if current_score > state["best_performance"]:
            state["best_performance"] = current_score
            state["last_improvement_episode"] = episode
        
        improvement_gap = episode - state["last_improvement_episode"]
        if improvement_gap >= targets["early_stop_patience"]:
            if len(state["performance_history"]) >= targets["performance_window"]:
                recent_avg_score = sum(state["performance_history"]) / len(state["performance_history"])
                if recent_avg_score < target_score * 0.8:
                    return False, f"è¿ç»­{improvement_gap}è½®æ— æ”¹è¿›ï¼Œä¸”å¹³å‡åˆ†æ•°ä½äº{target_score*0.8:.3f}", 0
        
        return True, f"å½“å‰åˆ†æ•° {current_score:.3f}, å®Œæˆç‡ {completion_rate:.1f}%", 0
    
    def create_environment(self, curriculum_stage=None):
        """åˆ›å»ºç¯å¢ƒï¼ˆæ”¯æŒè¯¾ç¨‹å­¦ä¹ ï¼‰"""
        config = {}
        
        # ğŸ”§ V16ï¼šå®ç°è¯¾ç¨‹å­¦ä¹ çš„ç¯å¢ƒé…ç½®
        if curriculum_stage is not None and CURRICULUM_CONFIG.get("enabled", False):
            stage = CURRICULUM_CONFIG["stages"][curriculum_stage] if curriculum_stage < len(CURRICULUM_CONFIG["stages"]) else CURRICULUM_CONFIG["stages"][-1]
            config['curriculum_stage'] = stage
            config['orders_scale'] = stage.get('orders_scale', 1.0)
            config['time_scale'] = stage.get('time_scale', 1.0)
            print(f"ğŸ“š è¯¾ç¨‹å­¦ä¹ é˜¶æ®µ {curriculum_stage+1}: {stage['name']} (è®¢å•æ¯”ä¾‹: {stage['orders_scale']}, æ—¶é—´å€æ•°: {stage['time_scale']})")
        
        env = make_parallel_env(config)
        buffers = {
            agent: ExperienceBuffer() 
            for agent in env.possible_agents
        }
        return env, buffers
    
    def collect_experience_parallel(self, buffers, num_steps: int, curriculum_config: Dict[str, Any] = None) -> float:
        """ğŸ”§ V17ä¿®å¤ï¼šä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œæ”¶é›†ç»éªŒï¼Œæ”¯æŒè¯¾ç¨‹å­¦ä¹ """
        for buffer in buffers.values():
            buffer.clear()

        network_weights = {
            'actor': self.shared_network.actor.get_weights(),
            'critic': self.shared_network.critic.get_weights()
        }
        steps_per_worker = num_steps // self.num_workers
        
        total_reward = 0

        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            futures = []
            for i in range(self.num_workers):
                seed = random.randint(0, 1_000_000)
                future = executor.submit(
                    run_simulation_worker,
                    network_weights,
                    self.state_dim,
                    self.action_dim,
                    steps_per_worker,
                    seed,
                    self.global_state_dim,
                    curriculum_config  # ğŸ”§ V17ä¿®å¤ï¼šä¼ é€’è¯¾ç¨‹å­¦ä¹ é…ç½®
                )
                futures.append(future)

            for future in as_completed(futures):
                try:
                    worker_buffers, worker_reward = future.result()
                    total_reward += worker_reward
                    
                    for agent_id, worker_buffer in worker_buffers.items():
                        buffers[agent_id].states.extend(worker_buffer.states)
                        buffers[agent_id].global_states.extend(worker_buffer.global_states)
                        buffers[agent_id].actions.extend(worker_buffer.actions)
                        buffers[agent_id].rewards.extend(worker_buffer.rewards)
                        buffers[agent_id].values.extend(worker_buffer.values)
                        buffers[agent_id].action_probs.extend(worker_buffer.action_probs)
                        buffers[agent_id].dones.extend(worker_buffer.dones)
                        buffers[agent_id].truncateds.extend(worker_buffer.truncateds)
                except Exception as e:
                    print(f"âŒ ä¸€ä¸ªå¹¶è¡Œå·¥ä½œè¿›ç¨‹å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()

        return total_reward
    
    def update_policy(self, buffers, entropy_coeff: float) -> Dict[str, float]:
        """ğŸ”§ MAPPOæ”¹è¿›ï¼šæ­£ç¡®å¤„ç†å¤šæ™ºèƒ½ä½“çš„ç­–ç•¥æ›´æ–°"""
        all_states = []
        all_global_states = []
        all_actions = []
        all_action_probs = []
        all_advantages = []
        all_returns = []
        
        # ğŸ”§ ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å•ç‹¬è®¡ç®—advantagesï¼Œè€ƒè™‘æˆªæ–­
        for agent, buffer in buffers.items():
            if len(buffer.states) > 0:
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è·å–æˆªæ–­æ—¶çš„bootstrapä»·å€¼
                next_value_if_truncated = None
                if len(buffer.truncateds) > 0 and buffer.truncateds[-1]:
                    # å¦‚æœæœ€åä¸€æ­¥æ˜¯æˆªæ–­ï¼Œä½¿ç”¨æœ€åå­˜å‚¨çš„å…¨å±€çŠ¶æ€ä¼°è®¡ä»·å€¼
                    # æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥ä½¿ç”¨"ä¸‹ä¸€ä¸ª"å…¨å±€çŠ¶æ€ï¼Œä½†å¦‚æœæ²¡æœ‰ï¼Œå°±ç”¨æœ€åä¸€ä¸ª
                    last_global_state = buffer.global_states[-1]
                    next_value_if_truncated = self.shared_network.get_value(last_global_state)
                elif len(buffer.states) > 0 and not buffer.dones[-1]:
                    # å¦‚æœtrajectoryæ—¢ä¸ç»ˆæ­¢ä¹Ÿä¸æˆªæ–­ï¼ˆè¢«steps_per_episodeæˆªæ–­ï¼‰
                    # ä¹Ÿéœ€è¦bootstrap
                    last_global_state = buffer.global_states[-1]
                    next_value_if_truncated = self.shared_network.get_value(last_global_state)
                
                states, global_states, actions, action_probs, advantages, returns = buffer.get_batch(
                    next_value_if_truncated=next_value_if_truncated
                )
                
                all_states.extend(states)
                all_global_states.extend(global_states)
                all_actions.extend(actions)
                all_action_probs.extend(action_probs)
                all_advantages.extend(advantages)
                all_returns.extend(returns)
                
                buffer.clear()
        
        if len(all_states) == 0:
            return {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0}
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        all_states = np.array(all_states)
        all_global_states = np.array(all_global_states)
        all_actions = np.array(all_actions)
        all_action_probs = np.array(all_action_probs, dtype=np.float32) # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32
        all_advantages = np.array(all_advantages, dtype=np.float32)     # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32
        all_returns = np.array(all_returns, dtype=np.float32).reshape(-1, 1)
        
        # ğŸ”§ æ–°å¢ï¼šå¥–åŠ±æ ‡å‡†åŒ–ï¼ˆæé«˜è®­ç»ƒç¨³å®šæ€§ï¼‰
        returns_mean = np.mean(all_returns)
        returns_std = np.std(all_returns) + 1e-8
        all_returns = (all_returns - returns_mean) / returns_std
        
        # ğŸ”§ V32 ä½¿ç”¨é…ç½®æ–‡ä»¶çš„ç­–ç•¥æ›´æ–°æ¬¡æ•°
        losses = {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0, 'approx_kl': 0, 'clip_fraction': 0}
        num_updates = PPO_NETWORK_CONFIG["num_policy_updates"]
        
        # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ æ—©åœæœºåˆ¶ï¼Œé¿å…è¿‡åº¦æ›´æ–°
        for epoch in range(num_updates):
            batch_losses = self.shared_network.update(
                states=all_states,
                global_states=all_global_states,
                actions=all_actions,
                old_probs=all_action_probs,
                advantages=all_advantages,
                returns=all_returns,
                entropy_coeff=entropy_coeff # ä¼ é€’åŠ¨æ€ç†µç³»æ•°
            )
            
            for key in losses:
                losses[key] += batch_losses[key] / num_updates
            
            # ğŸ”§ æ–°å¢ï¼šå¦‚æœKLæ•£åº¦è¿‡å¤§ï¼Œæå‰åœæ­¢æ›´æ–°
            if batch_losses['approx_kl'] > 0.02:  # ç¨å¾®æé«˜KLé˜ˆå€¼
                if epoch > 0:  # è‡³å°‘æ›´æ–°ä¸€æ¬¡
                    break
        
        return losses
    
    def _independent_exam_evaluation(self, env, curriculum_config, seed):
        """ğŸ”§ V33 æ–°å¢ï¼šç‹¬ç«‹çš„è€ƒè¯•è¯„ä¼°ï¼Œç¡®ä¿æ¯è½®éƒ½æ˜¯å…¨æ–°çš„ä»¿çœŸ"""
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)
        
        observations, _ = env.reset(seed=seed)
        episode_reward = 0
        step_count = 0
        
        while step_count < 1200:
            actions = {}
            
            # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼Œä½†åŸºäºæ–°çš„éšæœºç¯å¢ƒçŠ¶æ€
            for agent in env.agents:
                if agent in observations:
                    state = tf.expand_dims(observations[agent], 0)
                    action_probs = self.shared_network.actor(state)
                    # ğŸ”§ ä½¿ç”¨ç¡®å®šæ€§è¯„ä¼°ï¼Œä½†ä¿ç•™å°‘é‡æ¢ç´¢
                    if random.random() < 0.1:  # 10%æ¦‚ç‡æ¢ç´¢ï¼Œé¿å…å®Œå…¨å¡æ­»
                        action = int(tf.random.categorical(tf.math.log(action_probs + 1e-8), 1)[0])
                    else:
                        action = int(tf.argmax(action_probs[0]))
                    actions[agent] = action
            
            observations, rewards, terminations, truncations, infos = env.step(actions)
            episode_reward += sum(rewards.values())
            step_count += 1
            
            if any(terminations.values()) or any(truncations.values()):
                break
        
        # è·å–æœ€ç»ˆç»Ÿè®¡
        final_stats = env.sim.get_final_stats()
        return {
            'mean_reward': episode_reward,
            'mean_makespan': final_stats.get('makespan', 0),
            'mean_utilization': final_stats.get('mean_utilization', 0),
            'mean_completed_parts': final_stats.get('total_parts', 0),
            'mean_tardiness': final_stats.get('total_tardiness', 0)
        }
    
    def quick_kpi_evaluation(self, num_episodes: int = 3, curriculum_config: Dict[str, Any] = None) -> Dict[str, float]:
        """ğŸ”§ V39ä¿®å¤ï¼šå¿«é€ŸKPIè¯„ä¼°ï¼ˆæ”¯æŒè¯¾ç¨‹å­¦ä¹ é…ç½®å’Œé™é»˜æ¨¡å¼ï¼‰"""
        # ğŸ”§ V39ä¿®å¤ï¼šåˆ›å»ºç¯å¢ƒæ—¶ä¼ é€’è¯¾ç¨‹é…ç½®ï¼ŒåŒ…æ‹¬é™é»˜æ¨¡å¼
        # è¯¾ç¨‹é…ç½®ç›´æ¥é€šè¿‡make_parallel_envä¼ é€’ï¼Œç”±ç¯å¢ƒå†…éƒ¨å¤„ç†
        if curriculum_config:
            env = make_parallel_env(curriculum_config)
        else:
            # ğŸ”§ V39 ä¿®å¤ä¸€ä¸ªæ½œåœ¨bugï¼šæ­£ç¡®è§£åŒ…create_environmentçš„è¿”å›å€¼
            env, _ = self.create_environment()
        
        total_rewards = []
        makespans = []
        utilizations = []
        completed_parts_list = []
        tardiness_list = []
        
        for episode in range(num_episodes):
            observations, _ = env.reset()
            episode_reward = 0
            step_count = 0
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„æ­¥æ•°é™åˆ¶
            while step_count < 1200:
                actions = {}
                
                # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥è¯„ä¼°
                for agent in env.agents:
                    if agent in observations:
                        state = tf.expand_dims(observations[agent], 0)
                        action_probs = self.shared_network.actor(state)
                        action = int(tf.argmax(action_probs[0]))
                        actions[agent] = action
                
                observations, rewards, terminations, truncations, infos = env.step(actions)
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if any(terminations.values()) or any(truncations.values()):
                    break
            
            # è·å–æœ€ç»ˆç»Ÿè®¡
            final_stats = env.sim.get_final_stats()
            total_rewards.append(episode_reward)
            makespans.append(final_stats.get('makespan', 0))
            utilizations.append(final_stats.get('mean_utilization', 0))
            completed_parts_list.append(final_stats.get('total_parts', 0))
            tardiness_list.append(final_stats.get('total_tardiness', 0))
        
        # ğŸ”§ V37 æ–°å¢ï¼šæ£€æŸ¥ç¯å¢ƒé‡ç½®ä¿¡å·
        strategy_reset_signal = getattr(env.sim, '_trigger_strategy_reset', False)
        if strategy_reset_signal:
            self._env_strategy_reset_signal = True
        
        env.close()
        
        return {
            'mean_reward': np.mean(total_rewards),
            'mean_makespan': np.mean(makespans),
            'mean_utilization': np.mean(utilizations),
            'mean_completed_parts': np.mean(completed_parts_list),
            'mean_tardiness': np.mean(tardiness_list)
        }
    
    def simple_evaluation(self, num_episodes: int = 5) -> Dict[str, float]:
        """ğŸ”§ ä¿®å¤ç‰ˆï¼šç®€å•è¯„ä¼°ï¼Œè¿”å›æ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡"""
        env, _ = self.create_environment()
        
        total_rewards = []
        total_steps = []
        makespans = []
        completed_parts = []
        utilizations = []
        tardiness_list = []
        
        for episode in range(num_episodes):
            observations, _ = env.reset()
            episode_reward = 0
            step_count = 0
            
            while step_count < 1200:
                actions = {}
                
                # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥è¯„ä¼°
                for agent in env.agents:
                    if agent in observations:
                        state = tf.expand_dims(observations[agent], 0)
                        action_probs = self.shared_network.actor(state)
                        action = int(tf.argmax(action_probs[0]))
                        actions[agent] = action
                
                observations, rewards, terminations, truncations, infos = env.step(actions)
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if any(terminations.values()) or any(truncations.values()):
                    break
            
            # ğŸ”§ ä¿®å¤ï¼šè·å–å®Œæ•´çš„ä¸šåŠ¡æŒ‡æ ‡
            final_stats = env.sim.get_final_stats()
            total_rewards.append(episode_reward)
            total_steps.append(step_count)
            makespans.append(final_stats.get('makespan', 0))
            completed_parts.append(final_stats.get('total_parts', 0))
            utilizations.append(final_stats.get('mean_utilization', 0))
            tardiness_list.append(final_stats.get('total_tardiness', 0))
        
        env.close()
        
        return {
            'mean_reward': np.mean(total_rewards),
            'std_reward': np.std(total_rewards),
            'mean_steps': np.mean(total_steps),
            'mean_makespan': np.mean(makespans),
            'mean_completed_parts': np.mean(completed_parts),
            'mean_utilization': np.mean(utilizations),
            'mean_tardiness': np.mean(tardiness_list)
        }
    
    
    def train(self, max_episodes: int = 1000, steps_per_episode: int = 200, 
              eval_frequency: int = 20, adaptive_mode: bool = True):
        """ğŸ”§ V31 è‡ªé€‚åº”è®­ç»ƒä¸»å¾ªç¯ï¼šæ ¹æ®æ€§èƒ½è‡ªåŠ¨è°ƒæ•´è®­ç»ƒç­–ç•¥å’Œè½®æ•°"""
        # ğŸ”§ V31 è‡ªé€‚åº”æ¨¡å¼ï¼šæœ€å¤§è½®æ•°ä½œä¸ºä¸Šé™ï¼Œå®é™…è½®æ•°æ ¹æ®æ€§èƒ½åŠ¨æ€å†³å®š

        if adaptive_mode:
            self.training_targets["max_episodes"] = max_episodes
        
        # ğŸ”§ V16ï¼šæ˜¾ç¤ºè¯¾ç¨‹å­¦ä¹ é…ç½®
        if CURRICULUM_CONFIG.get("enabled", False):
            print(f"ğŸ“š è¯¾ç¨‹å­¦ä¹ å·²å¯ç”¨ï¼Œå…±{len(CURRICULUM_CONFIG['stages'])}ä¸ªé˜¶æ®µ:")
            for i, stage in enumerate(CURRICULUM_CONFIG["stages"]):
                print(f"   é˜¶æ®µ{i+1}: {stage['name']} - {stage['iterations']}è½®ï¼Œè®¢å•{stage['orders_scale']*100:.0f}%")
        print("=" * 80)
        
        if not validate_config():
            print("âŒ é…ç½®éªŒè¯å¤±è´¥")
            return
        
        # è®­ç»ƒå¼€å§‹æ—¶é—´è®°å½•
        training_start_time = time.time()
        training_start_datetime = datetime.now()
        print(f"ğŸ• è®­ç»ƒå¼€å§‹æ—¶é—´: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        
        # ğŸ”§ V16ï¼šè¯¾ç¨‹å­¦ä¹ ç®¡ç†
        curriculum_enabled = CURRICULUM_CONFIG.get("enabled", False)
        current_stage = 0
        stage_episode_count = 0
        
        # ğŸ”§ V8 ä¼˜åŒ–: ä¸å†éœ€è¦åˆ›å»ºä¸»ç¯å¢ƒï¼Œåªåˆ›å»ºç¼“å†²åŒº
        buffers = {
            agent: ExperienceBuffer() 
            for agent in self.agent_ids
        }
        
        best_reward = float('-inf')
        best_makespan = float('inf')
        
        # ğŸ”§ V27 æ ¸å¿ƒä¿®å¤ï¼šä¸ºè¯¾ç¨‹å­¦ä¹ çš„æ¯ä¸ªé˜¶æ®µç‹¬ç«‹è·Ÿè¸ªæœ€ä½³åˆ†æ•°
        stage_best_scores = [-1.0] * len(CURRICULUM_CONFIG["stages"])
        
        try:
            for episode in range(max_episodes):
                iteration_start_time = time.time()
                
                # ğŸ”§ V17å…³é”®ä¿®å¤ï¼šè¯¾ç¨‹å­¦ä¹ é˜¶æ®µç®¡ç†
                current_curriculum_config = None
                if curriculum_enabled:
                    stage_config = CURRICULUM_CONFIG["stages"][current_stage]
                    
                    # ğŸ”§ V31 å¼ºåŒ–æ¯•ä¸šè€ƒè¯•æœºåˆ¶ï¼šä½¿ç”¨æ–°çš„é«˜æ ‡å‡†é—¨æ§›ï¼Œé˜²æ­¢å¸¦ç—…æ¯•ä¸š
                    if stage_episode_count >= stage_config["iterations"]:
                        if current_stage < len(CURRICULUM_CONFIG["stages"]) - 1:
                            # ğŸ”§ V33 ä¿®å¤ï¼šæš‚åœè®­ç»ƒè®¡æ—¶ï¼Œéš”ç¦»è€ƒè¯•æ—¶é—´
                            iteration_pause_time = time.time()
                            
                            print("\n" + "="*60)
                            print(f"ğŸ“ é˜¶æ®µ '{stage_config['name']}' è®­ç»ƒå®Œæˆï¼Œå¼€å§‹å¼ºåŒ–æ¯•ä¸šè€ƒè¯•...")
                            
                            # ğŸ”§ V31 ä½¿ç”¨æ–°çš„æ¯•ä¸šé—¨æ§›é…ç½®
                            graduation_config = CURRICULUM_CONFIG.get("graduation_config", {})
                            
                            # ğŸ”§ ä¿®å¤ï¼šä»å½“å‰é˜¶æ®µé…ç½®ä¸­è·å–æ¯•ä¸šé˜ˆå€¼
                            current_threshold = stage_config.get("graduation_thresholds", 95.0)
                            exam_episodes = graduation_config.get("exam_episodes", 5)
                            stability_requirement = graduation_config.get("stability_requirement", 3)
                            max_retries = graduation_config.get("max_retries", 3)
                            retry_extension = graduation_config.get("retry_extension", 15)
                            
                            # ğŸ”§ V34 ä¿®å¤ï¼šæ¯•ä¸šè€ƒè¯•åº”æ£€éªŒå½“å‰é˜¶æ®µçš„æŒæ¡æƒ…å†µï¼Œè€Œä¸æ˜¯ç”¨ä¸‹ä¸€é˜¶æ®µçš„æ ‡å‡†
                            current_stage_data = CURRICULUM_CONFIG["stages"][current_stage]
                            exam_target_parts = int(get_total_parts_count() * current_stage_data['orders_scale'])
                            exam_config = {
                                'orders_scale': current_stage_data.get('orders_scale', 1.0),
                                'time_scale': current_stage_data.get('time_scale', 1.0),
                                'stage_name': f"è€ƒè¯•: {current_stage_data.get('name', '')}"
                            }
                            
                            # ğŸ”§ V33 ä¿®å¤ï¼šå¼ºåŒ–è€ƒè¯•éšæœºæ€§ï¼Œç¡®ä¿æ¯è½®è€ƒè¯•ç»“æœç‹¬ç«‹
                            exam_results = []
                            for exam_round in range(exam_episodes):
                                # å…³é”®ä¿®å¤ï¼šä¸ºæ¯è½®è€ƒè¯•è®¾ç½®ä¸åŒçš„éšæœºç§å­
                                exam_seed = random.randint(0, 1000000) + exam_round * 1000
                                
                                # åˆ›å»ºç‹¬ç«‹çš„è¯„ä¼°ç¯å¢ƒï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
                                temp_env = make_parallel_env(exam_config)
                                temp_env.reset(seed=exam_seed)
                                
                                # æ‰§è¡Œç‹¬ç«‹çš„è¯„ä¼°è½®æ¬¡
                                exam_kpi = self._independent_exam_evaluation(temp_env, exam_config, exam_seed)
                                temp_env.close()
                                
                                exam_completed_parts = exam_kpi.get('mean_completed_parts', 0)
                                exam_completion_rate = (exam_completed_parts / exam_target_parts) * 100 if exam_target_parts > 0 else 0
                                exam_results.append(exam_completion_rate)
                                print(f"   ç¬¬{exam_round+1}è½®è€ƒè¯•: {exam_completed_parts:.1f}/{exam_target_parts} é›¶ä»¶ ({exam_completion_rate:.1f}%)")
                            
                            # è®¡ç®—ç¨³å®šæ€§ï¼šéœ€è¦è¿ç»­å¤šæ¬¡è¾¾åˆ°é—¨æ§›
                            avg_completion_rate = sum(exam_results) / len(exam_results)
                            passed_exams = sum(1 for rate in exam_results if rate >= current_threshold)
                            stability_achieved = passed_exams >= stability_requirement
                            
                            print(f"   è€ƒè¯•ç»“æœ: å¹³å‡ {avg_completion_rate:.1f}% | é€šè¿‡é—¨æ§›: {current_threshold:.1f}% | è¾¾æ ‡æ¬¡æ•°: {passed_exams}/{exam_episodes}")
                            print(f"   ç¨³å®šæ€§è¦æ±‚: {stability_requirement}æ¬¡è¾¾æ ‡")
                            
                            # ğŸ”§ V37 ä¿®å¤ï¼šç¨³å®šæ€§è¾¾åˆ°å³é€šè¿‡ï¼Œæ— éœ€é‡å¤æ£€æŸ¥å¹³å‡åˆ†æ•°
                            if stability_achieved:
                                # å…³é”®ä¿®å¤ï¼šéœ€è¦è·å–ä¸‹ä¸€é˜¶æ®µçš„æ•°æ®æ¥æ‰“å°æ—¥å¿—
                                next_stage_data = CURRICULUM_CONFIG["stages"][current_stage + 1]
                                print(f"   âœ… æ¯•ä¸šè€ƒè¯•é€šè¿‡ï¼è¿›å…¥ä¸‹ä¸€é˜¶æ®µ: '{next_stage_data['name']}'")
                                current_stage += 1
                                stage_episode_count = 0
                                if not hasattr(self, '_stage_retry_count'):
                                    self._stage_retry_count = {}
                                self._stage_retry_count[current_stage] = 0  # é‡ç½®é‡è€ƒè®¡æ•°
                            else:
                                if not hasattr(self, '_stage_retry_count'):
                                    self._stage_retry_count = {}
                                retry_count = self._stage_retry_count.get(current_stage, 0)
                                
                                if retry_count < max_retries:
                                    self._stage_retry_count[current_stage] = retry_count + 1
                                    print(f"   âŒ è€ƒè¯•æœªé€šè¿‡ã€‚å»¶é•¿{retry_extension}è½®è®­ç»ƒåé‡è€ƒ (ç¬¬{retry_count+1}/{max_retries}æ¬¡é‡è€ƒ)")
                                    stage_config["iterations"] += retry_extension
                                else:
                                    print(f"   âš ï¸ å·²è¾¾æœ€å¤§é‡è€ƒæ¬¡æ•°ï¼Œå¼ºåˆ¶è¿›å…¥ä¸‹ä¸€é˜¶æ®µï¼ˆä½†å¯èƒ½è¡¨ç°ä¸ä½³ï¼‰")
                                    current_stage += 1
                                    stage_episode_count = 0
                                    self._stage_retry_count[current_stage] = 0
                            
                            print("="*60 + "\n")
                            
                            # ğŸ”§ V33 ä¿®å¤ï¼šæ¢å¤è®­ç»ƒè®¡æ—¶ï¼Œè¡¥å¿è€ƒè¯•æ—¶é—´
                            exam_duration = time.time() - iteration_pause_time
                            iteration_start_time += exam_duration  # å…³é”®ä¿®å¤ï¼šè¡¥å¿è€ƒè¯•æ—¶é—´
                            
                        # å¦‚æœæ˜¯æœ€åé˜¶æ®µï¼Œåˆ™ä¸å†åˆ‡æ¢
                    
                    # è·å–å½“å‰é˜¶æ®µé…ç½® (å¯èƒ½å·²æ›´æ–°)
                    stage = CURRICULUM_CONFIG["stages"][current_stage]
                    current_curriculum_config = {
                        'orders_scale': stage.get('orders_scale', 1.0),
                        'time_scale': stage.get('time_scale', 1.0),
                        'stage_name': stage.get('name', f'Stage {current_stage}')
                    }
                    
                    # ğŸ”§ V17å¢å¼ºï¼šè¯¦ç»†çš„é˜¶æ®µåˆ‡æ¢å’ŒçŠ¶æ€æ—¥å¿—
                    if stage_episode_count == 0:
                        print(f"ğŸ“š [å›åˆ {episode+1}] ğŸ”„ è¯¾ç¨‹å­¦ä¹ é˜¶æ®µåˆ‡æ¢!")
                        print(f"   æ–°é˜¶æ®µ: {stage['name']}")
                        print(f"   è®¢å•æ¯”ä¾‹: {stage['orders_scale']} (ç›®æ ‡é›¶ä»¶æ•°: {int(get_total_parts_count() * stage['orders_scale'])})")
                        print(f"   æ—¶é—´æ¯”ä¾‹: {stage['time_scale']} (æ—¶é—´é™åˆ¶: {int(1200 * stage['time_scale'])}åˆ†é’Ÿ)")
                        print(f"   è®¡åˆ’è®­ç»ƒè½®æ•°: {stage['iterations']}")
                        
                        # ğŸ”§ V30 å…³é”®ä¿®å¤ï¼šç¡®ä¿è¯¾ç¨‹é…ç½®æ­£ç¡®ä¼ é€’åˆ°æ‰€æœ‰ç¯å¢ƒ
                        print(f"ğŸ”§ å½“å‰è¯¾ç¨‹é…ç½®å°†ä¼ é€’ç»™æ‰€æœ‰worker: orders_scale={stage['orders_scale']}, time_scale={stage['time_scale']}")
                        
                        print("-" * 60)
                    
                    # ğŸ”§ V17æ–°å¢ï¼šæ¯10è½®æ˜¾ç¤ºé˜¶æ®µçŠ¶æ€
                    if episode % 10 == 0:
                        progress = stage_episode_count / stage['iterations'] * 100
                        print(f"ğŸ“š è¯¾ç¨‹çŠ¶æ€: {stage['name']} ({stage_episode_count}/{stage['iterations']}, {progress:.1f}%)")
                        print(f"   å½“å‰éš¾åº¦: {int(get_total_parts_count() * stage['orders_scale'])}é›¶ä»¶, {stage['time_scale']:.1f}xæ—¶é—´")    
                    stage_episode_count += 1
                

                collect_start_time = time.time()
                episode_reward = self.collect_experience_parallel(buffers, steps_per_episode, current_curriculum_config)
                collect_duration = time.time() - collect_start_time
                
                # ğŸ”§ V6 å®‰å…¨çš„ç­–ç•¥æ›´æ–°ï¼ˆåŒ…å«å†…å­˜æ£€æŸ¥ï¼‰
                update_start_time = time.time()
                losses = self.update_policy(buffers, entropy_coeff=self.current_entropy_coeff)
                update_duration = time.time() - update_start_time
                
                # è®°å½•ç»Ÿè®¡
                iteration_end_time = time.time()
                iteration_duration = iteration_end_time - iteration_start_time
                self.iteration_times.append(iteration_duration)
                self.episode_rewards.append(episode_reward)

                
                # æå‰è¿›è¡ŒKPIè¯„ä¼°ï¼Œä»¥ä¾¿æ•´åˆTensorBoardæ—¥å¿—
                kpi_results = self.quick_kpi_evaluation(num_episodes=2, curriculum_config=current_curriculum_config)
                self.kpi_history.append(kpi_results)

                # ğŸ”§ æ ¸å¿ƒæ”¹é€ ï¼šè®¡ç®—å½“å‰å›åˆçš„ç»¼åˆè¯„åˆ†
                current_score = self._calculate_score(kpi_results, current_curriculum_config)
                
                # ğŸ”§ æ–°å¢ï¼šæ™ºèƒ½ç†µç³»æ•°è°ƒæ•´ï¼ˆåŸºäºæ€§èƒ½ï¼‰
                completion_rate_kpi = (kpi_results.get('mean_completed_parts', 0) / get_total_parts_count()) * 100
                if episode > 100:  # å‰100è½®ä¿æŒé«˜æ¢ç´¢
                    # å¦‚æœå®Œæˆç‡é«˜ï¼Œå¯ä»¥é™ä½æ¢ç´¢ï¼›å¦åˆ™ä¿æŒæ¢ç´¢
                    if completion_rate_kpi >= 95:  # é«˜å®Œæˆç‡æ—¶æ‰é™ä½ç†µ
                        self.current_entropy_coeff = max(
                            self.min_entropy_coeff,
                            self.current_entropy_coeff * self.entropy_decay_rate
                        )
                    elif completion_rate_kpi < 80:  # å®Œæˆç‡ä½æ—¶å¢åŠ æ¢ç´¢
                        self.current_entropy_coeff = min(
                            PPO_NETWORK_CONFIG["entropy_coeff"],
                            self.current_entropy_coeff * 1.01  # ç¼“æ…¢å¢åŠ 
                        )

                # ğŸ”§ V36 ç»Ÿä¸€TensorBoardæ—¥å¿—è®°å½•ï¼Œå¹¶æ ¹æ®è¯¾ç¨‹é˜¶æ®µåŠ¨æ€åˆ‡æ¢run
                if TENSORBOARD_AVAILABLE:
                    # æ ¹æ®è¯¾ç¨‹é˜¶æ®µåˆ‡æ¢runï¼Œåœ¨æ‚¬åœæç¤ºä¸­æ˜¾ç¤ºé˜¶æ®µå
                    run_name = "train_default" # Fallback run name
                    if curriculum_enabled and current_curriculum_config:
                        # Get stage name and sanitize it for use as a directory name
                        run_name = current_curriculum_config['stage_name'].replace(" ", "_")
                    
                    if self.train_writer is None or self.current_tensorboard_run_name != run_name:
                        if self.train_writer is not None:
                            self.train_writer.close()
                        
                        logdir = os.path.join(self.tensorboard_dir, run_name)
                        self.train_writer = tf.summary.create_file_writer(logdir)
                        self.current_tensorboard_run_name = run_name
                        print(f"ğŸ“Š TensorBoard runå·²åˆ‡æ¢è‡³: '{run_name}'")

                    if self.train_writer:
                        with self.train_writer.as_default():
                            # è®­ç»ƒæ ¸å¿ƒæŒ‡æ ‡
                            tf.summary.scalar('Training/Episode_Reward', episode_reward, step=episode)
                            tf.summary.scalar('Training/Actor_Loss', losses['actor_loss'], step=episode)
                            tf.summary.scalar('Training/Critic_Loss', losses['critic_loss'], step=episode)
                            tf.summary.scalar('Training/Entropy', losses['entropy'], step=episode)
                            tf.summary.scalar('Training/KL_Divergence', losses['approx_kl'], step=episode)
                            tf.summary.scalar('Training/Clip_Fraction', losses['clip_fraction'], step=episode)
                            # æ€§èƒ½æŒ‡æ ‡
                            tf.summary.scalar('Performance/Iteration_Duration', iteration_duration, step=episode)
                            tf.summary.scalar('Performance/CPU_Collection_Time', collect_duration, step=episode)
                            tf.summary.scalar('Performance/GPU_Update_Time', update_duration, step=episode)
                            # ä¸šåŠ¡KPIæŒ‡æ ‡
                            tf.summary.scalar('KPI/Makespan', kpi_results['mean_makespan'], step=episode)
                            tf.summary.scalar('KPI/Completed_Parts', kpi_results['mean_completed_parts'], step=episode)
                            tf.summary.scalar('KPI/Utilization', kpi_results['mean_utilization'], step=episode)
                            tf.summary.scalar('KPI/Tardiness', kpi_results['mean_tardiness'], step=episode)
                            # ğŸŒŸ æ–°å¢ï¼šè®°å½•ç»¼åˆè¯„åˆ†
                            tf.summary.scalar('KPI/Score', current_score, step=episode)
                            
                            self.train_writer.flush()
                
                # ğŸ”§ æ ¸å¿ƒæ”¹é€ ï¼šåŠ¨æ€æ—©åœé€»è¾‘ - åœ¨å®Œæˆ"å®Œæ•´æŒ‘æˆ˜"é˜¶æ®µçš„æŒ‡å®šè½®æ•°åï¼Œæ‰å¼€å§‹è¯„ä¼°
                should_continue = True
                reason = "ç»§ç»­è®­ç»ƒ"
                estimated_remaining = 0
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æœ€ç»ˆé˜¶æ®µï¼ˆå®Œæ•´æŒ‘æˆ˜ï¼‰
                is_final_stage = curriculum_enabled and (current_stage == len(CURRICULUM_CONFIG["stages"]) - 1)
                
                if is_final_stage:
                    # è·å–æœ€ç»ˆé˜¶æ®µå¿…é¡»å®Œæˆçš„è¯¾ç¨‹è½®æ•°
                    final_stage_iterations = CURRICULUM_CONFIG["stages"][-1].get("iterations", 100)
                    
                    # åªæœ‰åœ¨å®Œæˆäº†æœ€ç»ˆé˜¶æ®µçš„æŒ‡å®šè¯¾ç¨‹è½®æ•°åï¼Œæ‰å¼€å§‹æ—©åœè¯„ä¼°
                    if stage_episode_count > final_stage_iterations:
                        completion_rate_check = (kpi_results.get('mean_completed_parts', 0) / get_total_parts_count()) * 100
                        should_continue, reason, estimated_remaining = self.should_continue_training(episode + 1, current_score, completion_rate_check)
                        
                        # æ¯10è½®æ‰“å°ä¸€æ¬¡æ—©åœè¯„ä¼°çŠ¶æ€
                        if episode % 10 == 0:
                            print(f"ğŸ“Š æœ€ç»ˆé˜¶æ®µæ—©åœè¯„ä¼°: {reason}")
                    else:
                        remaining_curriculum_eps = final_stage_iterations - stage_episode_count
                        reason = f"æœ€ç»ˆé˜¶æ®µè¯¾ç¨‹è¿˜éœ€ {remaining_curriculum_eps} è½®"
                
                # ğŸ”§ V31 å…³é”®ï¼šæ£€æŸ¥æ˜¯å¦åº”è¯¥æå‰ç»“æŸè®­ç»ƒ
                if not should_continue:
                    print(f"\nğŸ è‡ªé€‚åº”è®­ç»ƒæå‰ç»“æŸ: {reason}")
                    break
                
                # ğŸ”§ V36 æ–°å¢ï¼šè®°å½•å½“å‰è¯¾ç¨‹é˜¶æ®µä¿¡æ¯ä¾›å…¶ä»–æ–¹æ³•ä½¿ç”¨
                if current_curriculum_config:
                    self._current_orders_scale = current_curriculum_config.get('orders_scale', 1.0)
                
                # ğŸ”§ é‡æ„ç‰ˆï¼šç®€åŒ–çš„æ€§èƒ½ç›‘æ§ï¼Œç§»é™¤å¤æ‚çš„é‡å¯æœºåˆ¶
                # åŸºç¡€æ€§èƒ½è·Ÿè¸ªï¼ˆç”¨äºè°ƒè¯•å’Œç›‘æ§ï¼‰
                current_performance = kpi_results.get('mean_completed_parts', 0)
                if not hasattr(self, '_performance_history'):
                    self._performance_history = []
                
                self._performance_history.append(current_performance)
                # åªä¿ç•™æœ€è¿‘20è½®çš„å†å²
                if len(self._performance_history) > 20:
                    self._performance_history.pop(0)
                
                # ğŸ”§ V31 å…³é”®ï¼šæ£€æŸ¥æ˜¯å¦åº”è¯¥æå‰ç»“æŸè®­ç»ƒ
                if not should_continue:
                    print(f"\nğŸ è‡ªé€‚åº”è®­ç»ƒæå‰ç»“æŸ: {reason}")
                    break
                
                # ğŸ”§ V38ä¿®å¤ï¼šæ¯30å›åˆè¿›è¡Œä¸€æ¬¡å®Œæ•´éš¾åº¦è¯„ä¼°ï¼ˆé™é»˜æ¨¡å¼ï¼Œé¿å…è¾“å‡ºæ±¡æŸ“ï¼‰
                if episode > 0 and episode % 30 == 0:
                    print("\n" + "="*60)
                    print("ğŸ“ è¿›è¡Œå®Œæ•´éš¾åº¦è¯„ä¼°ï¼ˆ100%è®¢å•ï¼Œæ ‡å‡†æ—¶é—´ï¼‰...")
                    full_config = {
                        'orders_scale': 1.0,
                        'time_scale': 1.0,
                        'stage_name': 'å®Œæ•´è¯„ä¼°',
                        'silent_evaluation': True  # ğŸ”§ V38 å…³é”®ï¼šå¯ç”¨é™é»˜æ¨¡å¼
                    }
                    full_kpi = self.quick_kpi_evaluation(num_episodes=3, curriculum_config=full_config)
                    
                    # è®¡ç®—çœŸå®æ€§èƒ½æŒ‡æ ‡
                    real_completion = full_kpi.get('mean_completed_parts', 0)
                    real_completion_rate = real_completion / get_total_parts_count() * 100
                    real_makespan = full_kpi.get('mean_makespan', 0)
                    real_utilization = full_kpi.get('mean_utilization', 0)
                    
                    # ğŸ”§ V34 ä¿®å¤ï¼šè·å–å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¿®å¤è®¾å¤‡åˆ©ç”¨ç‡æ˜¾ç¤ºå¼‚å¸¸
                    real_tardiness = full_kpi.get('mean_tardiness', 0) / 60  # è½¬æ¢ä¸ºåˆ†é’Ÿ
                    real_reward = full_kpi.get('mean_reward', 0)
                    
                    print(f"ğŸ¯ å®Œæ•´éš¾åº¦è¯„ä¼°ç»“æœï¼ˆ3è½®å¹³å‡ï¼‰:")
                    print(f"   å¹³å‡å®Œæˆé›¶ä»¶: {real_completion:.1f}/{get_total_parts_count()} ({real_completion_rate:.1f}%)")
                    print(f"   å¹³å‡æ€»å®Œå·¥æ—¶é—´: {real_makespan:.1f}åˆ†é’Ÿ")
                    print(f"   å¹³å‡è®¾å¤‡åˆ©ç”¨ç‡: {real_utilization*100:.1f}%")
                    print(f"   å¹³å‡å»¶æœŸæ—¶é—´: {real_tardiness:.1f}åˆ†é’Ÿ") 
                    print(f"   å¹³å‡å¥–åŠ±: {real_reward:.1f}")
                    
                    # è¯„ä¼°è¿›å±•
                    if real_completion_rate > 90:
                        print(f"ğŸ† ä¼˜ç§€ï¼æ¥è¿‘å®Œå…¨æŒæ¡ä»»åŠ¡!")
                    elif real_completion_rate > 60:
                        print(f"ğŸ’ª è‰¯å¥½ï¼å·²å…·å¤‡åŸºæœ¬èƒ½åŠ›!")
                    elif real_completion_rate > 30:
                        print(f"ğŸ“ˆ è¿›æ­¥ä¸­ï¼ç»§ç»­åŠªåŠ›!")
                    else:
                        print(f"ğŸ“š ä»éœ€æ›´å¤šè®­ç»ƒ!")
                    print("="*60 + "\n")
                
                # ğŸ”§ V12 TensorBoard KPIè®°å½• (V36å·²æ•´åˆ)
                
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®æ›´æ–°æœ€ä½³è®°å½•ï¼ˆåªæœ‰å½“makespan > 0æ—¶æ‰æ›´æ–°ï¼‰
                current_makespan = kpi_results['mean_makespan']
                if current_makespan > 0 and current_makespan < best_makespan:
                    best_makespan = current_makespan
                
                # ------------------- ç»Ÿä¸€æ—¥å¿—è¾“å‡ºå¼€å§‹ -------------------
                
                # å‡†å¤‡KPIæ•°æ®ç”¨äºæ—¥å¿—æ˜¾ç¤º
                makespan = kpi_results['mean_makespan']
                completed_parts = kpi_results['mean_completed_parts']
                utilization = kpi_results['mean_utilization']
                tardiness = kpi_results['mean_tardiness']
                # current_score å·²ç»åœ¨å‰é¢é€šè¿‡ _calculate_score è®¡ç®—è¿‡äº†
                
                if not hasattr(self, 'best_score'):
                    self.best_score = float('-inf')

                model_update_info = ""
                # ğŸ”§ æ ¸å¿ƒæ”¹é€ ï¼šåŒºåˆ†"å…¨å±€æœ€ä½³"å’Œ"æœ€ç»ˆé˜¶æ®µæœ€ä½³"
                # 1. æ›´æ–°å…¨å±€æœ€ä½³åˆ†æ•°ï¼ˆç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼‰
                if current_score > self.best_score:
                    self.best_score = current_score

                # 2. æ›´æ–°è¯¾ç¨‹å„é˜¶æ®µæœ€ä½³åˆ†æ•°å¹¶ä¿å­˜æ¨¡å‹
                if curriculum_enabled:
                    if current_score > stage_best_scores[current_stage]:
                        stage_best_scores[current_stage] = current_score
                        stage_name = current_curriculum_config['stage_name'].replace(" ", "_")
                        model_path = self.save_model(f"{self.models_dir}/{stage_name}_best")
                        if model_path:
                            stage_display_name = current_curriculum_config['stage_name']
                            model_update_info = f"âœ… {stage_display_name}é˜¶æ®µæœ€ä½³å¾—åˆ†åˆ·æ–°ï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: {model_path}"

                    # 3. å¦‚æœæ˜¯æœ€ç»ˆé˜¶æ®µï¼Œåˆ™æ›´æ–°"æœ€ç»ˆé˜¶æ®µæœ€ä½³æ¨¡å‹"
                    if current_stage == len(CURRICULUM_CONFIG["stages"]) - 1:
                        if current_score > self.final_stage_best_score:
                            self.final_stage_best_score = current_score
                            self.final_stage_best_kpi = kpi_results.copy()
                            self.final_stage_best_episode = episode + 1 # ğŸ”§ è®°å½•æœ€ä½³KPIçš„å›åˆæ•°
                            final_model_path = self.save_model(f"{self.models_dir}/final_challenge_best")
                            model_update_info = f" ğŸ†æœ€ç»ˆé˜¶æ®µæœ€ä½³! æ¨¡å‹ä¿å­˜è‡³: {final_model_path}"
                        
                        # ğŸ”§ æ ¸å¿ƒæ”¹é€ ï¼šæ£€æŸ¥å¹¶æ›´æ–°"åŒè¾¾æ ‡"æœ€ä½³æ¨¡å‹
                        completion_rate_kpi = (kpi_results.get('mean_completed_parts', 0) / get_total_parts_count()) * 100
                        if completion_rate_kpi >= 100 and current_score > self.best_score_dual_objective:
                            self.best_score_dual_objective = current_score
                            self.best_kpi_dual_objective = kpi_results.copy()
                            self.best_episode_dual_objective = episode + 1
                            dual_objective_best_path = self.save_model(f"{self.models_dir}/dual_objective_best")
                            model_update_info = f" â­åŒè¾¾æ ‡æœ€ä½³!æ¨¡å‹ä¿å­˜è‡³: {dual_objective_best_path}"

                else: # éè¯¾ç¨‹å­¦ä¹ æ¨¡å¼
                    # åœ¨éè¯¾ç¨‹å­¦ä¹ æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬å°†è®­ç»ƒè§†ä¸ºä¸€ä¸ªå•ä¸€çš„"æœ€ç»ˆæŒ‘æˆ˜"é˜¶æ®µ
                    # 1. æ›´æ–°"æœ€ç»ˆæŒ‘æˆ˜"æœ€ä½³æ¨¡å‹ (ç­‰åŒäºå…¨å±€æœ€ä½³)
                    if current_score > self.final_stage_best_score:
                        self.final_stage_best_score = current_score
                        self.final_stage_best_kpi = kpi_results.copy()
                        self.final_stage_best_episode = episode + 1 # è®°å½•æœ€ä½³KPIçš„å›åˆæ•°
                        final_model_path = self.save_model(f"{self.models_dir}/final_challenge_best")
                        if final_model_path:
                            model_update_info = f" ğŸ†å…¨å±€æœ€ä½³! æ¨¡å‹ä¿å­˜è‡³: {final_model_path}"
                    
                    # 2. æ£€æŸ¥å¹¶æ›´æ–°"åŒè¾¾æ ‡"æœ€ä½³æ¨¡å‹
                    completion_rate_kpi = (kpi_results.get('mean_completed_parts', 0) / get_total_parts_count()) * 100
                    if completion_rate_kpi >= 100 and current_score > self.best_score_dual_objective:
                        self.best_score_dual_objective = current_score
                        self.best_kpi_dual_objective = kpi_results.copy()
                        self.best_episode_dual_objective = episode + 1
                        dual_objective_best_path = self.save_model(f"{self.models_dir}/dual_objective_best")
                        if dual_objective_best_path:
                            model_update_info = f" â­åŒè¾¾æ ‡æœ€ä½³!æ¨¡å‹ä¿å­˜è‡³: {dual_objective_best_path}"
                
                # ğŸ”§ V33 ä¼˜åŒ–ï¼šä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·è¦æ±‚çš„æ—¥å¿—æ ¼å¼
                # ç¬¬ä¸€è¡Œï¼šå›åˆä¿¡æ¯å’Œæ€§èƒ½æ•°æ®
                line1 = f"ğŸ”‚ å›åˆ {episode + 1:3d}/{max_episodes} | å¥–åŠ±: {episode_reward:.1f} | ActoræŸå¤±: {losses['actor_loss']:.4f}| â±ï¸æœ¬è½®ç”¨æ—¶: {iteration_duration:.1f}s (CPUé‡‡é›†: {collect_duration:.1f}s, GPUæ›´æ–°: {update_duration:.1f}s)"
                
                # ç¬¬äºŒè¡Œï¼šKPIæ•°æ®å’Œé˜¶æ®µä¿¡æ¯
                target_parts_str = f"/{int(get_total_parts_count() * current_curriculum_config['orders_scale'])}" if curriculum_enabled and current_curriculum_config else f"/{get_total_parts_count()}"
                stage_info = f"   | é˜¶æ®µï¼š'{current_curriculum_config['stage_name']}'" if curriculum_enabled and current_curriculum_config else ""
                line2 = f"ğŸ“Š KPI - æ€»å®Œå·¥æ—¶é—´: {makespan:.1f}min  | è®¾å¤‡åˆ©ç”¨ç‡: {utilization:.1%} | å»¶æœŸæ—¶é—´: {tardiness:.1f}min |  å®Œæˆé›¶ä»¶æ•°: {completed_parts:.0f}{target_parts_str}{stage_info}"
                
                # ç¬¬ä¸‰è¡Œï¼šè¯„åˆ†å’Œæ¨¡å‹æ›´æ–°ä¿¡æ¯
                if curriculum_enabled:
                    stage_best_str = f" (é˜¶æ®µæœ€ä½³: {stage_best_scores[current_stage]:.3f})"
                    line3_score = f"ğŸš¥ å›åˆè¯„åˆ†: {current_score:.3f} (å…¨å±€æœ€ä½³: {self.best_score:.3f}){stage_best_str}"
                else:
                    line3_score = f"ğŸš¥ å›åˆè¯„åˆ†: {current_score:.3f} (å…¨å±€æœ€ä½³: {self.best_score:.3f})"
                line3 = f"{line3_score}{model_update_info}" if model_update_info else line3_score

                avg_time = np.mean(self.iteration_times)
                remaining_episodes = max_episodes - (episode + 1)
                estimated_remaining = remaining_episodes * avg_time
                progress_percent = ((episode + 1) / max_episodes) * 100
                current_time = datetime.now().strftime('%H:%M:%S')
                finish_str = ""
                if remaining_episodes > 0:
                    finish_time = time.time() + estimated_remaining
                    finish_str = time.strftime('%H:%M:%S', time.localtime(finish_time))
                line4 = f"ğŸ”® å½“å‰è®­ç»ƒè¿›åº¦: {progress_percent:.1f}% | å½“å‰æ—¶é—´ï¼š{current_time} | é¢„è®¡å®Œæˆæ—¶é—´: {finish_str}"

                # æ‰“å°æ—¥å¿—
                print(line1)
                print(line2)
                print(line3)
                print(line4)
                print() # æ¯ä¸ªå›åˆåæ·»åŠ ä¸€ä¸ªç©ºè¡Œ
                
                # ------------------- ç»Ÿä¸€æ—¥å¿—è¾“å‡ºç»“æŸ -------------------
                        
            
            # ğŸ”§ ä¿®å¤ç‰ˆï¼šç®€åŒ–çš„è®­ç»ƒå®Œæˆç»Ÿè®¡
            training_end_time = time.time()
            training_end_datetime = datetime.now()
            total_training_time = training_end_time - training_start_time
            
            print("\n" + "=" * 80)
            print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
            print(f"ğŸ• è®­ç»ƒå¼€å§‹: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ğŸ è®­ç»ƒç»“æŸ: {training_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f}åˆ†é’Ÿ ({total_training_time:.1f}ç§’)")
            
            # è®­ç»ƒæ•ˆç‡ç»Ÿè®¡
            if self.iteration_times:
                avg_iteration_time = np.mean(self.iteration_times)
                print(f"âš¡ å¹³å‡æ¯è½®: {avg_iteration_time:.1f}s | è®­ç»ƒæ•ˆç‡: {len(self.iteration_times)/total_training_time*60:.1f}è½®/åˆ†é’Ÿ")

            # ğŸ”§ Bugä¿®å¤ï¼šè¾“å‡ºæœ€ç»ˆçš„ã€å¯é çš„æœ€ä½³KPI
            print("\n" + "="*40)
            print("ğŸ† æœ€ç»ˆæœ€ä½³KPIè¡¨ç° (åŒé‡æ ‡å‡†æœ€ä½³) ğŸ†")
            print("="*40)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹è¾¾åˆ°äº†åŒé‡æ ‡å‡†
            if self.best_episode_dual_objective != -1:
                best_kpi = self.best_kpi_dual_objective
                best_episode_to_report = self.best_episode_dual_objective
            else:
                print("âš ï¸ æœªæ‰¾åˆ°åŒæ—¶æ»¡è¶³100%å®Œæˆç‡å’Œç›®æ ‡åˆ†æ•°çš„æ¨¡å‹ï¼Œå°†æŠ¥å‘Šæœ€ç»ˆé˜¶æ®µçš„æœ€ä½³åˆ†æ•°æ¨¡å‹ã€‚")
                best_kpi = self.final_stage_best_kpi
                best_episode_to_report = self.final_stage_best_episode

            target_parts_final = get_total_parts_count() # æœ€ç»ˆè¯„ä¼°æ€»æ˜¯åŸºäºå®Œæ•´ä»»åŠ¡
            completion_rate_final = (best_kpi.get('mean_completed_parts', 0) / target_parts_final) * 100 if target_parts_final > 0 else 0
            
            print(f"   (åœ¨ç¬¬ {best_episode_to_report} å›åˆå–å¾—)") # ğŸ”§ æ–°å¢
            print(f"   å®Œæˆé›¶ä»¶: {best_kpi.get('mean_completed_parts', 0):.1f} / {target_parts_final} ({completion_rate_final:.1f}%)")
            print(f"   æ€»å®Œå·¥æ—¶é—´: {best_kpi.get('mean_makespan', 0):.1f} åˆ†é’Ÿ")
            print(f"   è®¾å¤‡åˆ©ç”¨ç‡: {best_kpi.get('mean_utilization', 0):.1%}")
            print(f"   æ€»å»¶æœŸæ—¶é—´: {best_kpi.get('mean_tardiness', 0):.1f} åˆ†é’Ÿ")
            print("="*40)
            
            return {
                'training_time': total_training_time,
                'kpi_history': self.kpi_history,
                'iteration_times': self.iteration_times,
                'best_kpi': self.best_kpi_dual_objective if self.best_episode_dual_objective != -1 else self.final_stage_best_kpi
            }
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None
        
        finally:
            # ğŸ”§ V8 ä¼˜åŒ–: ä¸»å¾ªç¯ä¸­æ²¡æœ‰envéœ€è¦å…³é—­
            pass
    
    def save_model(self, filepath: str) -> str:
        """ä¿å­˜æ¨¡å‹å¹¶è¿”å›è·¯å¾„"""
        actor_path = f"{filepath}_actor.keras"
        try:
            self.shared_network.actor.save(actor_path)
            self.shared_network.critic.save(f"{filepath}_critic.keras")
            return actor_path
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return ""

    def _calculate_score(self, kpi_results: Dict[str, float], curriculum_config: Dict) -> float:
        """ç»Ÿä¸€è®¡ç®—å›åˆè¯„åˆ†çš„è¾…åŠ©å‡½æ•°"""
        makespan = kpi_results.get('mean_makespan', 0)
        completed_parts = kpi_results.get('mean_completed_parts', 0)
        utilization = kpi_results.get('mean_utilization', 0)
        tardiness = kpi_results.get('mean_tardiness', 0)

        if completed_parts == 0:
            return 0.0
        
        makespan_score = max(0, 1 - makespan / (SIMULATION_TIME * 1.5)) # ä½¿ç”¨1.5å€ä»¿çœŸæ—¶é—´ä½œä¸ºåŸºå‡†
        utilization_score = utilization
        tardiness_score = max(0, 1 - tardiness / (SIMULATION_TIME * 2.0)) # ä½¿ç”¨2å€ä»¿çœŸæ—¶é—´ä½œä¸ºåŸºå‡†

        target_parts = get_total_parts_count()
        if curriculum_config:
            target_parts = int(get_total_parts_count() * curriculum_config.get('orders_scale', 1.0))
        
        completion_score = completed_parts / target_parts if target_parts > 0 else 0
        
        current_score = (
            completion_score * 0.5 +
            tardiness_score * 0.25 +
            makespan_score * 0.15 +
            utilization_score * 0.1
        )
        return current_score

def main():
    
    print(f"âœ¨ è®­ç»ƒè¿›ç¨‹PID: {os.getpid()}")

    # è®¾ç½®éšæœºç§å­
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    tf.random.set_seed(RANDOM_SEED)
    
    try:
        max_episodes = 1000  # æœ€å¤§è½®æ•°ä¸Šé™ï¼Œå®é™…è½®æ•°æ ¹æ®æ€§èƒ½åŠ¨æ€å†³å®š
        steps_per_episode = 1500  # ä¸è¯„ä¼°ä¿æŒä¸€è‡´çš„æ­¥æ•°
        
        # ğŸ”§ V32 ä½¿ç”¨é…ç½®æ–‡ä»¶çš„è‡ªé€‚åº”è®­ç»ƒç›®æ ‡é…ç½®
        training_targets = ADAPTIVE_TRAINING_CONFIG.copy()
        training_targets["max_episodes"] = max_episodes  # åªè¦†ç›–æœ€å¤§è½®æ•°
        
        print("ğŸš€ å¯åŠ¨V31è‡ªé€‚åº”PPOè®­ç»ƒç³»ç»Ÿ")
        print("=" * 80)
        print(f"ğŸ¯ è®­ç»ƒç›®æ ‡: ç»¼åˆè¯„åˆ†è¾¾åˆ° {training_targets['target_score']:.2f}")
        print(f"âš–ï¸ ç¨³å®šæ€§è¦æ±‚: è¿ç»­{training_targets['target_consistency']}æ¬¡è¾¾åˆ°ç›®æ ‡")
        print(f"ğŸ“Š è½®æ•°ä¸Šé™: {training_targets['max_episodes']}è½® (å®Œæ•´æŒ‘æˆ˜é˜¶æ®µå®Œæˆåå¼€å§‹æ—©åœè¯„ä¼°)")
        print(f"ğŸ”„ æ—©åœè€å¿ƒ: {training_targets['early_stop_patience']}è½®æ— æ”¹è¿›")
        print("=" * 80)
        print("ğŸ”§ æ ¸å¿ƒé…ç½®:")
        print("  å·¥ä½œç«™:")
        for station, config in WORKSTATIONS.items():
            print(f"    - {station}: æ•°é‡={config['count']}, å®¹é‡={config['capacity']}")
        
        grad_config = CURRICULUM_CONFIG.get("graduation_config", {})
        print("  æ¯•ä¸šè€ƒè¯•:")
        print(f"    - è€ƒè¯•è½®æ•°: {grad_config.get('exam_episodes', 'N/A')}")
        print(f"    - ç¨³å®šè¦æ±‚: {grad_config.get('stability_requirement', 'N/A')}æ¬¡é€šè¿‡")
        print(f"    - æœ€å¤§é‡è¯•: {grad_config.get('max_retries', 'N/A')}æ¬¡")
        print(f"    - è¡¥è¯¾è½®æ•°: {grad_config.get('retry_extension', 'N/A')}è½®")
        
        print(f"  è®¾å¤‡æ•…éšœ: {'å¯ç”¨' if EQUIPMENT_FAILURE.get('enabled', False) else 'ç¦ç”¨'}")
        print(f"  ç´§æ€¥æ’å•: {'å¯ç”¨' if EMERGENCY_ORDERS.get('enabled', False) else 'ç¦ç”¨'}")
        print("-" * 40)
        
        trainer = SimplePPOTrainer(
            initial_lr=LEARNING_RATE_CONFIG["initial_lr"],  # ğŸ”§ V32ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶çš„å­¦ä¹ ç‡
            total_train_episodes=max_episodes,  # ä¼ é€’æœ€å¤§è½®æ•°
            steps_per_episode=steps_per_episode,
            training_targets=training_targets   # ğŸ”§ V32æ ¸å¿ƒï¼šä¼ é€’è‡ªé€‚åº”è®­ç»ƒç›®æ ‡
        )
        
        # ğŸ”§ V31 å¯åŠ¨è‡ªé€‚åº”è®­ç»ƒï¼šç³»ç»Ÿå°†æ ¹æ®æ€§èƒ½è‡ªåŠ¨å†³å®šä½•æ—¶åœæ­¢
        results = trainer.train(
            max_episodes=max_episodes,           # æœ€å¤§è½®æ•°ï¼ˆä¸Šé™ï¼‰
            steps_per_episode=steps_per_episode,
            eval_frequency=20,                  # è¯„ä¼°é¢‘ç‡
            adaptive_mode=True                  # ğŸ”§ V31æ ¸å¿ƒï¼šå¯ç”¨è‡ªé€‚åº”æ¨¡å¼
        )
        
        if results:
            print("\nğŸ‰ è‡ªé€‚åº”è®­ç»ƒæˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“Š å®é™…è®­ç»ƒè½®æ•°: {len(trainer.iteration_times)}")
            final_completion_rate = (results['best_kpi'].get('mean_completed_parts', 0) / get_total_parts_count()) * 100 if get_total_parts_count() > 0 else 0
            print(f"ğŸ¯ æœ€ç»ˆç›®æ ‡è¾¾æˆ: {trainer.adaptive_state['target_achieved_count']}æ¬¡è¿ç»­è¾¾æ ‡ (åŸºäºæœ€ç»ˆé˜¶æ®µåˆ†æ•°)")
            
            best_episode_final = trainer.best_episode_dual_objective if trainer.best_episode_dual_objective != -1 else trainer.final_stage_best_episode
            print(f"ğŸ“ˆ å†å²æœ€ä½³æ€§èƒ½ (åŒé‡æ ‡å‡†ï¼Œç¬¬ {best_episode_final} å›åˆ): {final_completion_rate:.1f}% ({results['best_kpi'].get('mean_completed_parts', 0):.1f}ä¸ªé›¶ä»¶)")
        else:
            print("\nâŒ è®­ç»ƒå¤±è´¥")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # ğŸ”§ V10 å…³é”®ä¿®å¤: è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•ä¸º'spawn'ï¼Œé¿å…TensorFlowçš„forkä¸å®‰å…¨é—®é¢˜
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    main()