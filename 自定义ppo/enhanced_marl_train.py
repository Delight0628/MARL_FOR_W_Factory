"""
å…¨åŠŸèƒ½å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒè„šæœ¬
åŒ…å«TensorBoardå¯è§†åŒ–ã€åŸºå‡†ç®—æ³•å¯¹æ¯”ã€è¯¦ç»†è¯„ä¼°æŒ‡æ ‡
"""

import os
import sys
import time
import json
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from collections import defaultdict, deque
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime

# è®¾ç½®TensorFlowæ—¥å¿—çº§åˆ«
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.get_logger().setLevel('ERROR')

# æ·»åŠ ç¯å¢ƒè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)  # å›åˆ°MARL_FOR_W_Factoryç›®å½•
sys.path.append(parent_dir)

from environments.w_factory_env import make_parallel_env
from environments.w_factory_config import *

# TensorBoardæ”¯æŒ
try:
    from tensorflow.keras.callbacks import TensorBoard
    from tensorflow.summary import create_file_writer, scalar
    TENSORBOARD_AVAILABLE = True
    print("âœ“ TensorBoardæ”¯æŒå·²å¯ç”¨")
except ImportError:
    TENSORBOARD_AVAILABLE = False
    print("âš ï¸ TensorBoardä¸å¯ç”¨ï¼Œå°†è·³è¿‡å¯è§†åŒ–åŠŸèƒ½")

# å¯è§†åŒ–æ”¯æŒ
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    plt.style.use('seaborn-v0_8')
    VISUALIZATION_AVAILABLE = True
    print("âœ“ å¯è§†åŒ–æ”¯æŒå·²å¯ç”¨")
except ImportError:
    VISUALIZATION_AVAILABLE = False
    print("âš ï¸ å¯è§†åŒ–åº“ä¸å¯ç”¨")

class PPONetwork:
    """PPOç½‘ç»œå®ç°ï¼ˆä¸åŸç‰ˆç›¸åŒï¼‰"""
    
    def __init__(self, state_dim: int, action_dim: int, lr: float = 3e-4):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.lr = lr
        
        # æ„å»ºç½‘ç»œ
        self.actor, self.critic = self._build_networks()
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = tf.keras.optimizers.Adam(lr)
        self.critic_optimizer = tf.keras.optimizers.Adam(lr)
        
    def _build_networks(self):
        """æ„å»ºActor-Criticç½‘ç»œ"""
        # Actorç½‘ç»œ
        actor_input = tf.keras.layers.Input(shape=(self.state_dim,))
        actor_hidden1 = tf.keras.layers.Dense(512, activation='relu')(actor_input)
        actor_hidden2 = tf.keras.layers.Dense(256, activation='relu')(actor_hidden1)
        actor_output = tf.keras.layers.Dense(self.action_dim, activation='softmax')(actor_hidden2)
        actor = tf.keras.Model(inputs=actor_input, outputs=actor_output)
        
        # Criticç½‘ç»œ
        critic_input = tf.keras.layers.Input(shape=(self.state_dim,))
        critic_hidden1 = tf.keras.layers.Dense(512, activation='relu')(critic_input)
        critic_hidden2 = tf.keras.layers.Dense(256, activation='relu')(critic_hidden1)
        critic_output = tf.keras.layers.Dense(1)(critic_hidden2)
        critic = tf.keras.Model(inputs=critic_input, outputs=critic_output)
        
        return actor, critic
    
    def get_action_and_value(self, state: np.ndarray) -> Tuple[int, float, float]:
        """è·å–åŠ¨ä½œã€åŠ¨ä½œæ¦‚ç‡å’ŒçŠ¶æ€ä»·å€¼"""
        state = tf.expand_dims(state, 0)
        
        action_probs = self.actor(state)
        action_dist = tf.random.categorical(tf.math.log(action_probs), 1)
        action = int(action_dist[0, 0])
        
        action_prob = float(action_probs[0, action])
        value = float(self.critic(state)[0, 0])
        
        return action, action_prob, value
    
    def get_value(self, state: np.ndarray) -> float:
        """è·å–çŠ¶æ€ä»·å€¼"""
        state = tf.expand_dims(state, 0)
        return float(self.critic(state)[0, 0])
    
    def update(self, states: np.ndarray, actions: np.ndarray, 
               old_probs: np.ndarray, advantages: np.ndarray, 
               returns: np.ndarray, clip_ratio: float = 0.2) -> Dict[str, float]:
        """PPOæ›´æ–°"""
        
        # Actoræ›´æ–°
        with tf.GradientTape() as tape:
            action_probs = self.actor(states)
            action_probs_selected = tf.reduce_sum(
                action_probs * tf.one_hot(actions, self.action_dim), axis=1
            )
            
            ratio = action_probs_selected / (old_probs + 1e-8)
            clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)
            actor_loss = -tf.reduce_mean(
                tf.minimum(ratio * advantages, clipped_ratio * advantages)
            )
            
            entropy = -tf.reduce_sum(action_probs * tf.math.log(action_probs + 1e-8), axis=1)
            actor_loss -= 0.01 * tf.reduce_mean(entropy)
        
        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)
        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))
        
        # Criticæ›´æ–°
        with tf.GradientTape() as tape:
            values = tf.squeeze(self.critic(states))
            critic_loss = tf.reduce_mean(tf.square(returns - values))
        
        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)
        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))
        
        return {
            'actor_loss': float(actor_loss),
            'critic_loss': float(critic_loss),
            'entropy': float(tf.reduce_mean(entropy))
        }

class ExperienceBuffer:
    """ç»éªŒç¼“å†²åŒºï¼ˆä¸åŸç‰ˆç›¸åŒï¼‰"""
    
    def __init__(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.action_probs = []
        self.dones = []
        
    def store(self, state, action, reward, value, action_prob, done):
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.values.append(value)
        self.action_probs.append(action_prob)
        self.dones.append(done)
    
    def get_batch(self, gamma=0.99, lam=0.95):
        states = np.array(self.states)
        actions = np.array(self.actions)
        rewards = np.array(self.rewards)
        values = np.array(self.values)
        action_probs = np.array(self.action_probs)
        dones = np.array(self.dones)
        
        advantages = np.zeros_like(rewards)
        last_advantage = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]
            
            delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
            advantages[t] = delta + gamma * lam * (1 - dones[t]) * last_advantage
            last_advantage = advantages[t]
        
        returns = advantages + values
        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)
        
        return states, actions, action_probs, advantages, returns
    
    def clear(self):
        self.states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.values.clear()
        self.action_probs.clear()
        self.dones.clear()

# =============================================================================
# ğŸ”§ V7 ä¿®å¤ï¼šåŸºäºçœŸå®ä»¿çœŸçš„åŸºå‡†ç®—æ³•å®ç°
# =============================================================================

class SimulationBasedScheduler:
    """åŸºäºä»¿çœŸçš„è°ƒåº¦ç®—æ³•åŸºç±» - ğŸ”§ ä¿®å¤ï¼šåœ¨ç›¸åŒç¯å¢ƒä¸­å…¬å¹³ç«äº‰"""
    
    def __init__(self, algorithm: str):
        self.algorithm = algorithm
        self.stats = {}
    
    def get_action_for_station(self, station_name: str, queue_items: List, current_time: float) -> int:
        """æ ¹æ®è°ƒåº¦è§„åˆ™é€‰æ‹©åŠ¨ä½œ - å­ç±»å¿…é¡»å®ç°"""
        raise NotImplementedError
    
    def run_simulation(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„ä»¿çœŸè¯„ä¼°"""
        # åˆ›å»ºç¯å¢ƒ
        env, _ = self._create_evaluation_env()
        
        # é‡ç½®ç¯å¢ƒ
        observations, _ = env.reset()
        episode_steps = 0
        max_steps = 1000  # é˜²æ­¢æ— é™å¾ªç¯
        
        while episode_steps < max_steps:
            # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“ç”ŸæˆåŸºäºè§„åˆ™çš„åŠ¨ä½œ
            actions = {}
            for agent in env.agents:
                if agent in observations:
                    station_name = agent.replace("agent_", "")
                    # ğŸ”§ ä¿®å¤ï¼šæ›´é²æ£’åœ°è·å–é˜Ÿåˆ—çŠ¶æ€
                    try:
                        if hasattr(env, 'sim') and env.sim:
                            queue_items = env.sim.queues[station_name].items
                            current_time = env.sim.current_time
                            action = self.get_action_for_station(station_name, queue_items, current_time)
                        elif hasattr(env, 'pz_env') and hasattr(env.pz_env, 'sim'):
                            queue_items = env.pz_env.sim.queues[station_name].items
                            current_time = env.pz_env.sim.current_time
                            action = self.get_action_for_station(station_name, queue_items, current_time)
                        else:
                            action = 1 if len(observations[agent]) > 0 else 0  # åŸºäºè§‚æµ‹çš„ç®€å•ç­–ç•¥
                    except Exception as e:
                        action = 0  # å‡ºé”™æ—¶ç©ºé—²
                    actions[agent] = action
            
            # æ‰§è¡ŒåŠ¨ä½œ
            observations, rewards, terminations, truncations, infos = env.step(actions)
            episode_steps += 1
            
            # æ£€æŸ¥æ˜¯å¦ç»“æŸ
            if any(terminations.values()) or any(truncations.values()):
                if any(infos.values()) and "final_stats" in list(infos.values())[0]:
                    self.stats = list(infos.values())[0]["final_stats"]
                break
        
        env.close()
        return self.stats
    
    def _create_evaluation_env(self):
        """åˆ›å»ºè¯„ä¼°ç¯å¢ƒ"""
        from environments.w_factory_env import make_parallel_env
        env = make_parallel_env()
        return env, None

class FIFOScheduler(SimulationBasedScheduler):
    """å…ˆè¿›å…ˆå‡ºè°ƒåº¦ç®—æ³• - ğŸ”§ ä¿®å¤ï¼šåŸºäºçœŸå®ä»¿çœŸ"""
    
    def __init__(self):
        super().__init__("FIFO")
    
    def get_action_for_station(self, station_name: str, queue_items: List, current_time: float) -> int:
        """FIFOè§„åˆ™ï¼šæ€»æ˜¯å¤„ç†é˜Ÿåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªé›¶ä»¶"""
        if len(queue_items) > 0:
            return 1  # å¤„ç†ç¬¬1ä¸ªé›¶ä»¶ï¼ˆFIFOï¼‰
        return 0  # ç©ºé—²
    
    def schedule(self, orders: List[Dict]) -> Dict[str, Any]:
        """è¿è¡ŒFIFOä»¿çœŸ"""
        return self.run_simulation()

class SPTScheduler(SimulationBasedScheduler):
    """æœ€çŸ­å¤„ç†æ—¶é—´ä¼˜å…ˆè°ƒåº¦ç®—æ³• - ğŸ”§ ä¿®å¤ï¼šåŸºäºçœŸå®ä»¿çœŸ"""
    
    def __init__(self):
        super().__init__("SPT")
    
    def get_action_for_station(self, station_name: str, queue_items: List, current_time: float) -> int:
        """SPTè§„åˆ™ï¼šé€‰æ‹©å‰©ä½™å¤„ç†æ—¶é—´æœ€çŸ­çš„é›¶ä»¶"""
        if len(queue_items) == 0:
            return 0  # ç©ºé—²
        
        # è®¡ç®—æ¯ä¸ªé›¶ä»¶çš„å‰©ä½™å¤„ç†æ—¶é—´
        min_time = float('inf')
        best_index = 0
        
        for i, part in enumerate(queue_items):
            if hasattr(part, 'product_type') and hasattr(part, 'current_step'):
                route = get_route_for_product(part.product_type)
                remaining_time = sum(
                    step['time'] for step in route[part.current_step:]
                )
                if remaining_time < min_time:
                    min_time = remaining_time
                    best_index = i
        
        # è¿”å›å¯¹åº”çš„åŠ¨ä½œï¼ˆ1=ç¬¬1ä¸ªï¼Œ2=ç¬¬2ä¸ªï¼Œ3=ç¬¬3ä¸ªï¼‰
        # ä½†è¦ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
        if best_index < 3:  # æˆ‘ä»¬çš„åŠ¨ä½œç©ºé—´åªæ”¯æŒå‰3ä¸ª
            return best_index + 1
        else:
            return 1  # é»˜è®¤å¤„ç†ç¬¬1ä¸ª
    
    def schedule(self, orders: List[Dict]) -> Dict[str, Any]:
        """è¿è¡ŒSPTä»¿çœŸ"""
        return self.run_simulation()

class EDDScheduler(SimulationBasedScheduler):
    """æœ€æ—©äº¤æœŸä¼˜å…ˆè°ƒåº¦ç®—æ³• - ğŸ”§ ä¿®å¤ï¼šåŸºäºçœŸå®ä»¿çœŸ"""
    
    def __init__(self):
        super().__init__("EDD")
    
    def get_action_for_station(self, station_name: str, queue_items: List, current_time: float) -> int:
        """EDDè§„åˆ™ï¼šé€‰æ‹©äº¤æœŸæœ€æ—©çš„é›¶ä»¶"""
        if len(queue_items) == 0:
            return 0  # ç©ºé—²
        
        # æ‰¾åˆ°äº¤æœŸæœ€æ—©çš„é›¶ä»¶
        earliest_due = float('inf')
        best_index = 0
        
        for i, part in enumerate(queue_items):
            if hasattr(part, 'due_date'):
                if part.due_date < earliest_due:
                    earliest_due = part.due_date
                    best_index = i
        
        # è¿”å›å¯¹åº”çš„åŠ¨ä½œï¼Œç¡®ä¿åœ¨åŠ¨ä½œç©ºé—´èŒƒå›´å†…
        if best_index < 3:
            return best_index + 1
        else:
            return 1  # é»˜è®¤å¤„ç†ç¬¬1ä¸ª
    
    def schedule(self, orders: List[Dict]) -> Dict[str, Any]:
        """è¿è¡ŒEDDä»¿çœŸ"""
        return self.run_simulation()

# =============================================================================
# å…¨åŠŸèƒ½MARLè®­ç»ƒå™¨
# =============================================================================

class FullFeaturedMARLTrainer:
    """å…¨åŠŸèƒ½å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        
        # åˆ›å»ºæ—¶é—´æˆ³ç”¨äºæ–‡ä»¶å‘½å
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ğŸ”§ V7 åŠ¨æ€æ¢æµ‹ç¯å¢ƒç©ºé—´
        temp_env, _ = self.create_environment()
        state_dim = temp_env.observation_space(temp_env.possible_agents[0]).shape[0]
        action_dim = temp_env.action_space(temp_env.possible_agents[0]).n
        self.agent_ids = temp_env.possible_agents
        temp_env.close()

        print("ğŸ”§ ç¯å¢ƒç©ºé—´è‡ªåŠ¨æ£€æµ‹ (è‡ªå®šä¹‰PPO):")
        print(f"   è§‚æµ‹ç©ºé—´ç»´åº¦ (State Dim): {state_dim}")
        print(f"   åŠ¨ä½œç©ºé—´ç»´åº¦ (Action Dim): {action_dim}")
        
        # å…±äº«ç­–ç•¥ç½‘ç»œ
        self.shared_network = PPONetwork(
            state_dim=state_dim,
            action_dim=action_dim,
            lr=self.config.get('lr', 3e-4)
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_lengths = []
        self.training_losses = []
        self.evaluation_history = []
        
        # æ€§èƒ½æŒ‡æ ‡å†å²
        self.makespan_history = []
        self.tardiness_history = []
        self.utilization_history = []
        
        # TensorBoardè®¾ç½®
        if TENSORBOARD_AVAILABLE:
            # ä½¿ç”¨ä¸´æ—¶ç›®å½•é¿å…ä¸­æ–‡è·¯å¾„é—®é¢˜
            import tempfile
            temp_base = tempfile.gettempdir()
            self.log_dir = os.path.join(temp_base, "marl_logs", f"training_{self.timestamp}")
            os.makedirs(self.log_dir, exist_ok=True)
            self.summary_writer = create_file_writer(self.log_dir)
            print(f"âœ“ TensorBoardæ—¥å¿—ç›®å½•: {self.log_dir}")
        
        # ç»“æœç›®å½•
        self.results_dir = f"results/full_training_{self.timestamp}"
        os.makedirs(self.results_dir, exist_ok=True)
        
    def log_to_tensorboard(self, metrics: Dict[str, float], step: int):
        """è®°å½•æŒ‡æ ‡åˆ°TensorBoard"""
        if not TENSORBOARD_AVAILABLE:
            return
        
        with self.summary_writer.as_default():
            for name, value in metrics.items():
                scalar(name, value, step=step)
            self.summary_writer.flush()
    
    def create_environment(self, enable_dynamic_events: bool = False):
        """åˆ›å»ºç¯å¢ƒ"""
        env = make_parallel_env()
        buffers = {
            agent: ExperienceBuffer() 
            for agent in env.possible_agents
        }
        return env, buffers
    
    def collect_experience(self, env, buffers, num_steps: int = 200) -> Dict[str, float]:
        """æ”¶é›†ç»éªŒ"""
        observations, _ = env.reset()
        episode_rewards = {agent: 0 for agent in env.possible_agents}
        step_count = 0
        
        for step in range(num_steps):
            actions = {}
            values = {}
            action_probs = {}
            
            for agent in env.agents:
                if agent in observations:
                    action, action_prob, value = self.shared_network.get_action_and_value(
                        observations[agent]
                    )
                    actions[agent] = action
                    values[agent] = value
                    action_probs[agent] = action_prob
            
            next_observations, rewards, terminations, truncations, _ = env.step(actions)
            
            for agent in env.agents:
                if agent in observations and agent in actions:
                    done = terminations.get(agent, False) or truncations.get(agent, False)
                    reward = rewards.get(agent, 0)
                    
                    buffers[agent].store(
                        state=observations[agent],
                        action=actions[agent],
                        reward=reward,
                        value=values[agent],
                        action_prob=action_probs[agent],
                        done=done
                    )
                    
                    episode_rewards[agent] += reward
            
            observations = next_observations
            step_count += 1
            
            if any(terminations.values()) or any(truncations.values()):
                observations, _ = env.reset()
        
        return episode_rewards
    
    def update_policy(self, buffers) -> Dict[str, float]:
        """æ›´æ–°ç­–ç•¥"""
        all_states = []
        all_actions = []
        all_action_probs = []
        all_advantages = []
        all_returns = []
        
        for agent, buffer in buffers.items():
            if len(buffer.states) > 0:
                states, actions, action_probs, advantages, returns = buffer.get_batch()
                
                all_states.extend(states)
                all_actions.extend(actions)
                all_action_probs.extend(action_probs)
                all_advantages.extend(advantages)
                all_returns.extend(returns)
                
                buffer.clear()
        
        if len(all_states) == 0:
            return {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0}
        
        all_states = np.array(all_states)
        all_actions = np.array(all_actions)
        all_action_probs = np.array(all_action_probs)
        all_advantages = np.array(all_advantages)
        all_returns = np.array(all_returns)
        
        losses = {'actor_loss': 0, 'critic_loss': 0, 'entropy': 0}
        num_updates = 5
        
        for _ in range(num_updates):
            batch_losses = self.shared_network.update(
                states=all_states,
                actions=all_actions,
                old_probs=all_action_probs,
                advantages=all_advantages,
                returns=all_returns
            )
            
            for key in losses:
                losses[key] += batch_losses[key] / num_updates
        
        return losses
    
    def run_baseline_comparison(self) -> Dict[str, Dict[str, float]]:
        """è¿è¡ŒåŸºå‡†ç®—æ³•å¯¹æ¯” - ğŸ”§ V7 ä¿®å¤ï¼šåŸºäºçœŸå®ä»¿çœŸçš„å…¬å¹³å¯¹æ¯”"""
        print("\n" + "=" * 60)
        print("ğŸ” åŸºå‡†ç®—æ³•å¯¹æ¯”æµ‹è¯• (åŸºäºçœŸå®ä»¿çœŸ)")
        print("=" * 60)
        print("ğŸ”§ ä¿®å¤è¯´æ˜: æ‰€æœ‰ç®—æ³•ç°åœ¨éƒ½åœ¨ç›¸åŒçš„SimPyä»¿çœŸç¯å¢ƒä¸­è¿è¡Œ")
        
        algorithms = {
            "FIFO": FIFOScheduler(),
            "SPT": SPTScheduler(),
            "EDD": EDDScheduler()
        }
        
        results = {}
        
        for name, scheduler in algorithms.items():
            print(f"è¿è¡Œ {name} ç®—æ³•...")
            start_time = time.time()
            
            try:
                stats = scheduler.schedule(BASE_ORDERS)
                end_time = time.time()
                
                stats['computation_time'] = end_time - start_time
                results[name] = stats
                
                # è¯¦ç»†è¾“å‡ºï¼Œä¾¿äºéªŒè¯
                makespan = stats.get('makespan', 0)
                tardiness = stats.get('total_tardiness', 0)
                utilization = stats.get('mean_utilization', 0)
                completed = stats.get('completed_parts', 0)
                
                print(f"  {name:4} - Makespan: {makespan:6.1f}, "
                      f"å»¶æœŸ: {tardiness:6.1f}, "
                      f"åˆ©ç”¨ç‡: {utilization:.1%}, "
                      f"å®Œæˆ: {completed}, "
                      f"æ—¶é—´: {stats['computation_time']:.4f}s")
                
            except Exception as e:
                print(f"  {name:4} - âŒ è¿è¡Œå¤±è´¥: {e}")
                # æä¾›é»˜è®¤å€¼é¿å…åç»­å´©æºƒ
                results[name] = {
                    'makespan': float('inf'),
                    'total_tardiness': float('inf'),
                    'max_tardiness': float('inf'),
                    'mean_utilization': 0,
                    'completed_parts': 0,
                    'computation_time': 0
                }
        
        return results
    
    def comprehensive_evaluation(self, num_episodes: int = 20) -> Dict[str, Any]:
        """å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print(f"\nğŸ“Š å…¨é¢æ€§èƒ½è¯„ä¼° ({num_episodes} å›åˆ)")
        print("=" * 60)
        
        env, _ = self.create_environment(enable_dynamic_events=False)
        
        eval_results = {
            'episode_rewards': [],
            'makespans': [],
            'total_tardiness': [],
            'max_tardiness': [],
            'completed_parts': [],
            'utilizations': [],
            'detailed_stats': []
        }
        
        for episode in range(num_episodes):
            observations, _ = env.reset()
            episode_reward = 0
            step_count = 0
            
            while step_count < 480:
                actions = {}
                for agent in env.agents:
                    if agent in observations:
                        state = tf.expand_dims(observations[agent], 0)
                        action_probs = self.shared_network.actor(state)
                        action = int(tf.argmax(action_probs[0]))
                        actions[agent] = action
                
                observations, rewards, terminations, truncations, infos = env.step(actions)
                episode_reward += sum(rewards.values())
                step_count += 1
                
                if any(terminations.values()) or any(truncations.values()):
                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ›´é²æ£’çš„final_statsè·å–
                    final_stats = None
                    
                    # å°è¯•ä»ä»»ä½•æ™ºèƒ½ä½“çš„infoä¸­è·å–final_stats
                    for agent_id, info in infos.items():
                        if isinstance(info, dict) and "final_stats" in info:
                            final_stats = info["final_stats"]
                            break
                    
                    if final_stats:
                        eval_results['makespans'].append(final_stats.get('makespan', 0))
                        eval_results['total_tardiness'].append(final_stats.get('total_tardiness', 0))
                        eval_results['max_tardiness'].append(final_stats.get('max_tardiness', 0))
                        eval_results['completed_parts'].append(final_stats.get('total_parts', 0))
                        eval_results['utilizations'].append(final_stats.get('mean_utilization', 0))
                        eval_results['detailed_stats'].append(final_stats)
                        print(f"    ğŸ” è·å–åˆ°stats: Makespan={final_stats.get('makespan', 0):.1f}, å®Œæˆ={final_stats.get('total_parts', 0)}")
                    else:
                        # å¦‚æœæ²¡æœ‰final_statsï¼Œæ‰‹åŠ¨ä»ç¯å¢ƒè·å–
                        if hasattr(env, 'sim') and env.sim:
                            current_stats = env.sim.get_final_stats()
                            eval_results['makespans'].append(current_stats.get('makespan', env.sim.current_time))
                            eval_results['total_tardiness'].append(current_stats.get('total_tardiness', 0))
                            eval_results['max_tardiness'].append(current_stats.get('max_tardiness', 0))
                            eval_results['completed_parts'].append(current_stats.get('total_parts', len(env.sim.completed_parts)))
                            eval_results['utilizations'].append(current_stats.get('mean_utilization', 0))
                            eval_results['detailed_stats'].append(current_stats)
                            print(f"    ğŸ”§ æ‰‹åŠ¨è·å–stats: Makespan={env.sim.current_time:.1f}, å®Œæˆ={len(env.sim.completed_parts)}")
                        else:
                            print(f"    âŒ æ— æ³•è·å–ç»Ÿè®¡æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                            eval_results['makespans'].append(step_count)  # ä½¿ç”¨æ­¥æ•°ä½œä¸ºå¤‡ç”¨
                            eval_results['total_tardiness'].append(0)
                            eval_results['max_tardiness'].append(0)
                            eval_results['completed_parts'].append(0)
                            eval_results['utilizations'].append(0)
                            eval_results['detailed_stats'].append({})
                    break
            
            eval_results['episode_rewards'].append(episode_reward)
            
            if (episode + 1) % 5 == 0:
                print(f"  è¯„ä¼°è¿›åº¦: {episode + 1}/{num_episodes}")
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        summary_stats = {
            'mean_reward': np.mean(eval_results['episode_rewards']),
            'std_reward': np.std(eval_results['episode_rewards']),
            'mean_makespan': np.mean(eval_results['makespans']) if eval_results['makespans'] else 0,
            'mean_tardiness': np.mean(eval_results['total_tardiness']) if eval_results['total_tardiness'] else 0,
            'mean_utilization': np.mean(eval_results['utilizations']) if eval_results['utilizations'] else 0,
            'mean_completed_parts': np.mean(eval_results['completed_parts']) if eval_results['completed_parts'] else 0,
        }
        
        eval_results['summary'] = summary_stats
        
        print(f"\nè¯„ä¼°ç»“æœ:")
        print(f"  å¹³å‡å¥–åŠ±: {summary_stats['mean_reward']:.2f} Â± {summary_stats['std_reward']:.2f}")
        print(f"  å¹³å‡Makespan: {summary_stats['mean_makespan']:.1f}")
        print(f"  å¹³å‡å»¶æœŸæ—¶é—´: {summary_stats['mean_tardiness']:.1f}")
        print(f"  å¹³å‡è®¾å¤‡åˆ©ç”¨ç‡: {summary_stats['mean_utilization']:.1%}")
        
        return eval_results
    
    def create_visualizations(self, baseline_results: Dict, eval_results: Dict):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        if not VISUALIZATION_AVAILABLE:
            print("âš ï¸ è·³è¿‡å¯è§†åŒ–ï¼ˆmatplotlibä¸å¯ç”¨ï¼‰")
            return
        
        print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå›¾è¡¨ç›®å½•
        viz_dir = os.path.join(self.results_dir, "visualizations")
        os.makedirs(viz_dir, exist_ok=True)
        
        # 1. è®­ç»ƒæ›²çº¿
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('MARLè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–', fontsize=16)
        
        # å¥–åŠ±æ›²çº¿
        axes[0, 0].plot(self.episode_rewards, alpha=0.7, label='Episode Reward')
        if len(self.episode_rewards) > 10:
            # ç§»åŠ¨å¹³å‡
            window = min(10, len(self.episode_rewards) // 4)
            moving_avg = pd.Series(self.episode_rewards).rolling(window=window).mean()
            axes[0, 0].plot(moving_avg, color='red', linewidth=2, label=f'Moving Avg ({window})')
        axes[0, 0].set_title('è®­ç»ƒå¥–åŠ±æ›²çº¿')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Reward')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # æŸå¤±æ›²çº¿
        if self.training_losses:
            actor_losses = [loss['actor_loss'] for loss in self.training_losses]
            critic_losses = [loss['critic_loss'] for loss in self.training_losses]
            
            axes[0, 1].plot(actor_losses, label='Actor Loss', alpha=0.7)
            axes[0, 1].plot(critic_losses, label='Critic Loss', alpha=0.7)
            axes[0, 1].set_title('è®­ç»ƒæŸå¤±æ›²çº¿')
            axes[0, 1].set_xlabel('Episode')
            axes[0, 1].set_ylabel('Loss')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        # åŸºå‡†å¯¹æ¯” - Makespan
        if baseline_results:
            algorithms = list(baseline_results.keys()) + ['MARL']
            # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨è·å–makespanï¼Œé¿å…KeyError
            makespans = [baseline_results[alg].get('makespan', 0) for alg in baseline_results.keys()]
            makespans.append(eval_results['summary']['mean_makespan'])
            
            colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']
            bars = axes[1, 0].bar(algorithms, makespans, color=colors[:len(algorithms)])
            axes[1, 0].set_title('Makespanå¯¹æ¯”')
            axes[1, 0].set_ylabel('Makespan')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, makespans):
                axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(makespans)*0.01,
                               f'{value:.1f}', ha='center', va='bottom')
        
        # åŸºå‡†å¯¹æ¯” - å»¶æœŸæ—¶é—´
        if baseline_results:
            # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨è·å–tardinessï¼Œé¿å…KeyError
            tardiness = [baseline_results[alg].get('total_tardiness', 0) for alg in baseline_results.keys()]
            tardiness.append(eval_results['summary']['mean_tardiness'])
            
            bars = axes[1, 1].bar(algorithms, tardiness, color=colors[:len(algorithms)])
            axes[1, 1].set_title('æ€»å»¶æœŸæ—¶é—´å¯¹æ¯”')
            axes[1, 1].set_ylabel('Total Tardiness')
            
            for bar, value in zip(bars, tardiness):
                axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(tardiness)*0.01,
                               f'{value:.1f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(viz_dir, 'training_overview.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. æ€§èƒ½é›·è¾¾å›¾
        if baseline_results:
            fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
            
            # æ ‡å‡†åŒ–æŒ‡æ ‡ï¼ˆè¶Šå°è¶Šå¥½çš„æŒ‡æ ‡éœ€è¦å–å€’æ•°ï¼‰
            metrics = ['Makespan', 'Tardiness', 'Computation Time']
            
            angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
            angles += angles[:1]  # é—­åˆå›¾å½¢
            
            for alg_name in list(baseline_results.keys()) + ['MARL']:
                if alg_name == 'MARL':
                    values = [
                        1 / (eval_results['summary']['mean_makespan'] + 1),
                        1 / (eval_results['summary']['mean_tardiness'] + 1),
                        1.0  # MARLè®¡ç®—æ—¶é—´è®¾ä¸ºæ ‡å‡†å€¼
                    ]
                else:
                    # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨è·å–åŸºå‡†æ•°æ®ï¼Œé¿å…KeyError
                    values = [
                        1 / (baseline_results[alg_name].get('makespan', 1) + 1),
                        1 / (baseline_results[alg_name].get('total_tardiness', 1) + 1),
                        1 / (baseline_results[alg_name].get('computation_time', 0.001) + 0.001)
                    ]
                
                values += values[:1]  # é—­åˆå›¾å½¢
                ax.plot(angles, values, 'o-', linewidth=2, label=alg_name)
                ax.fill(angles, values, alpha=0.25)
            
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(metrics)
            ax.set_title('ç®—æ³•æ€§èƒ½é›·è¾¾å›¾\n(æ•°å€¼è¶Šå¤§è¡¨ç¤ºæ€§èƒ½è¶Šå¥½)', pad=20)
            ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
            
            plt.savefig(os.path.join(viz_dir, 'performance_radar.png'), dpi=300, bbox_inches='tight')
            plt.close()
        
        print(f"âœ“ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {viz_dir}")
    
    def progressive_train(self, static_episodes: int = 80, dynamic_episodes: int = 20, 
                         steps_per_episode: int = 200):
        """é€’è¿›å¼è®­ç»ƒä¸»æµç¨‹"""
        print("ğŸš€ å…¨åŠŸèƒ½Wå·¥å‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ")
        print("=" * 60)
        print("åŠŸèƒ½ç‰¹æ€§:")
        print("  â€¢ é€’è¿›å¼è®­ç»ƒ (é™æ€â†’åŠ¨æ€)")
        print("  â€¢ TensorBoardå¯è§†åŒ–")
        print("  â€¢ åŸºå‡†ç®—æ³•å¯¹æ¯”")
        print("  â€¢ è¯¦ç»†æ€§èƒ½åˆ†æ")
        print("  â€¢ å›¾è¡¨å¯è§†åŒ–")
        print("=" * 60)
        
        if not validate_config():
            print("é…ç½®éªŒè¯å¤±è´¥")
            return None
        
        try:
            # é˜¶æ®µ1: é™æ€è®­ç»ƒ
            print("\nğŸ”„ é˜¶æ®µ1: é™æ€ç¯å¢ƒè®­ç»ƒ")
            static_results = self.static_training(static_episodes, steps_per_episode)
            
            # ä¸­æœŸè¯„ä¼°
            print("\nğŸ“Š ä¸­æœŸè¯„ä¼°...")
            mid_eval = self.comprehensive_evaluation(num_episodes=10)
            
            # é˜¶æ®µ2: åŠ¨æ€å¾®è°ƒ
            print("\nğŸ”„ é˜¶æ®µ2: åŠ¨æ€ç¯å¢ƒå¾®è°ƒ")
            dynamic_results = self.dynamic_training(dynamic_episodes, steps_per_episode)
            
            # æœ€ç»ˆè¯„ä¼°
            print("\nğŸ“Š æœ€ç»ˆè¯„ä¼°...")
            final_eval = self.comprehensive_evaluation(num_episodes=20)
            
            # åŸºå‡†ç®—æ³•å¯¹æ¯”
            baseline_results = self.run_baseline_comparison()
            
            # åˆ›å»ºå¯è§†åŒ–
            self.create_visualizations(baseline_results, final_eval)
            
            # ä¿å­˜æ¨¡å‹
            os.makedirs("models", exist_ok=True)
            self.save_model(f"models/full_marl_model_{self.timestamp}")
            
            # æ±‡æ€»ç»“æœ
            complete_results = {
                'training_phases': {
                    'static': static_results,
                    'dynamic': dynamic_results
                },
                'evaluations': {
                    'mid_evaluation': mid_eval,
                    'final_evaluation': final_eval
                },
                'baseline_comparison': baseline_results,
                'training_history': {
                    'episode_rewards': self.episode_rewards,
                    'training_losses': self.training_losses
                },
                'config': {
                    'algorithm': 'PPO/MAPPO',
                    'network': 'Shared Actor-Critic',
                    'training_approach': 'Progressive (Static â†’ Dynamic)',
                    'agents': list(WORKSTATIONS.keys()),
                    'state_dim': 2,
                    'action_dim': 2,
                    'timestamp': self.timestamp
                }
            }
            
            # ä¿å­˜ç»“æœ
            results_file = os.path.join(self.results_dir, 'complete_results.json')
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(complete_results, f, ensure_ascii=False, indent=2)
            
            # æ€§èƒ½åˆ†ææŠ¥å‘Š
            self.generate_performance_report(baseline_results, mid_eval, final_eval)
            
            print(f"\nğŸ“ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {self.results_dir}")
            
            if TENSORBOARD_AVAILABLE:
                print(f"ğŸ“Š TensorBoardå¯è§†åŒ–: tensorboard --logdir {self.log_dir}")
            
            return complete_results
            
        except Exception as e:
            print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def static_training(self, num_episodes: int, steps_per_episode: int):
        """é™æ€ç¯å¢ƒè®­ç»ƒ"""
        env, buffers = self.create_environment(enable_dynamic_events=False)
        start_time = time.time()
        
        for episode in range(num_episodes):
            episode_start = time.time()
            
            episode_rewards = self.collect_experience(env, buffers, steps_per_episode)
            losses = self.update_policy(buffers)
            
            total_reward = sum(episode_rewards.values())
            self.episode_rewards.append(total_reward)
            self.training_losses.append(losses)
            
            # TensorBoardè®°å½•
            if TENSORBOARD_AVAILABLE and episode % 5 == 0:
                metrics = {
                    'training/episode_reward': total_reward,
                    'training/actor_loss': losses['actor_loss'],
                    'training/critic_loss': losses['critic_loss'],
                    'training/entropy': losses['entropy']
                }
                self.log_to_tensorboard(metrics, episode)
            
            if (episode + 1) % 10 == 0:
                recent_rewards = self.episode_rewards[-10:]
                avg_reward = np.mean(recent_rewards)
                print(f"é™æ€è®­ç»ƒ {episode + 1:3d}/{num_episodes} | "
                      f"å¥–åŠ±: {total_reward:8.2f} | "
                      f"å¹³å‡: {avg_reward:8.2f} | "
                      f"ActoræŸå¤±: {losses['actor_loss']:.4f}")
        
        training_time = time.time() - start_time
        return {
            'phase': 'static',
            'training_time': training_time,
            'episode_rewards': self.episode_rewards.copy(),
            'avg_reward': np.mean(self.episode_rewards)
        }
    
    def dynamic_training(self, num_episodes: int, steps_per_episode: int):
        """åŠ¨æ€ç¯å¢ƒå¾®è°ƒ"""
        env, buffers = self.create_environment(enable_dynamic_events=True)
        
        # å¾®è°ƒå­¦ä¹ ç‡
        original_lr = self.shared_network.lr
        fine_tune_lr = original_lr * 0.1
        self.shared_network.actor_optimizer.learning_rate = fine_tune_lr
        self.shared_network.critic_optimizer.learning_rate = fine_tune_lr
        
        start_time = time.time()
        dynamic_rewards = []
        
        for episode in range(num_episodes):
            episode_rewards = self.collect_experience(env, buffers, steps_per_episode)
            losses = self.update_policy(buffers)
            
            total_reward = sum(episode_rewards.values())
            dynamic_rewards.append(total_reward)
            self.episode_rewards.append(total_reward)
            self.training_losses.append(losses)
            
            # TensorBoardè®°å½•
            if TENSORBOARD_AVAILABLE:
                metrics = {
                    'fine_tuning/episode_reward': total_reward,
                    'fine_tuning/actor_loss': losses['actor_loss'],
                    'fine_tuning/critic_loss': losses['critic_loss']
                }
                self.log_to_tensorboard(metrics, len(self.episode_rewards))
            
            if (episode + 1) % 5 == 0:
                recent_rewards = dynamic_rewards[-5:]
                avg_reward = np.mean(recent_rewards)
                print(f"åŠ¨æ€å¾®è°ƒ {episode + 1:2d}/{num_episodes} | "
                      f"å¥–åŠ±: {total_reward:8.2f} | "
                      f"å¹³å‡: {avg_reward:8.2f}")
        
        # æ¢å¤å­¦ä¹ ç‡
        self.shared_network.actor_optimizer.learning_rate = original_lr
        self.shared_network.critic_optimizer.learning_rate = original_lr
        
        training_time = time.time() - start_time
        return {
            'phase': 'dynamic',
            'training_time': training_time,
            'episode_rewards': dynamic_rewards,
            'avg_reward': np.mean(dynamic_rewards)
        }
    
    def generate_performance_report(self, baseline_results: Dict, mid_eval: Dict, final_eval: Dict):
        """ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š"""
        report_file = os.path.join(self.results_dir, 'performance_report.md')
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# Wå·¥å‚MARLè®­ç»ƒæ€§èƒ½æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## 1. è®­ç»ƒæ¦‚å†µ\n\n")
            f.write(f"- ç®—æ³•: PPO/MAPPO\n")
            f.write(f"- è®­ç»ƒæ–¹å¼: é€’è¿›å¼è®­ç»ƒ (é™æ€â†’åŠ¨æ€)\n")
            f.write(f"- æ€»è®­ç»ƒå›åˆ: {len(self.episode_rewards)}\n")
            f.write(f"- æ™ºèƒ½ä½“æ•°é‡: {len(WORKSTATIONS)}\n\n")
            
            f.write("## 2. åŸºå‡†ç®—æ³•å¯¹æ¯”\n\n")
            f.write("| ç®—æ³• | Makespan | æ€»å»¶æœŸæ—¶é—´ | æœ€å¤§å»¶æœŸ | è®¡ç®—æ—¶é—´(s) |\n")
            f.write("|------|----------|------------|----------|-------------|\n")
            
            for alg_name, stats in baseline_results.items():
                f.write(f"| {alg_name} | {stats['makespan']:.1f} | "
                       f"{stats['total_tardiness']:.1f} | "
                       f"{stats['max_tardiness']:.1f} | "
                       f"{stats['computation_time']:.4f} |\n")
            
            # MARLç»“æœ
            final_stats = final_eval['summary']
            f.write(f"| MARL | {final_stats['mean_makespan']:.1f} | "
                   f"{final_stats['mean_tardiness']:.1f} | "
                   f"N/A | N/A |\n\n")
            
            f.write("## 3. è®­ç»ƒé˜¶æ®µå¯¹æ¯”\n\n")
            mid_stats = mid_eval['summary']
            
            f.write("| æŒ‡æ ‡ | é™æ€è®­ç»ƒå | åŠ¨æ€å¾®è°ƒå | æ”¹è¿› |\n")
            f.write("|------|------------|------------|------|\n")
            f.write(f"| å¹³å‡å¥–åŠ± | {mid_stats['mean_reward']:.2f} | "
                   f"{final_stats['mean_reward']:.2f} | "
                   f"{((final_stats['mean_reward'] - mid_stats['mean_reward'])/mid_stats['mean_reward']*100):+.1f}% |\n")
            f.write(f"| å¹³å‡Makespan | {mid_stats['mean_makespan']:.1f} | "
                   f"{final_stats['mean_makespan']:.1f} | "
                   f"{((final_stats['mean_makespan'] - mid_stats['mean_makespan'])/mid_stats['mean_makespan']*100):+.1f}% |\n")
            
            f.write("\n## 4. ç»“è®º\n\n")
            
            # æ‰¾å‡ºæœ€ä½³åŸºå‡†ç®—æ³•
            best_baseline = min(baseline_results.keys(), 
                              key=lambda x: baseline_results[x]['makespan'])
            best_makespan = baseline_results[best_baseline]['makespan']
            marl_makespan = final_stats['mean_makespan']
            
            if marl_makespan < best_makespan:
                improvement = (best_makespan - marl_makespan) / best_makespan * 100
                f.write(f"âœ… MARLç›¸æ¯”æœ€ä½³åŸºå‡†ç®—æ³•({best_baseline})åœ¨Makespanä¸Šæå‡äº†{improvement:.1f}%\n\n")
            else:
                degradation = (marl_makespan - best_makespan) / best_makespan * 100
                f.write(f"âš ï¸ MARLç›¸æ¯”æœ€ä½³åŸºå‡†ç®—æ³•({best_baseline})åœ¨Makespanä¸Šä¸‹é™äº†{degradation:.1f}%\n")
                f.write("ä½†MARLå…·æœ‰æ›´å¼ºçš„é€‚åº”æ€§å’Œé²æ£’æ€§\n\n")
            
            f.write("### ä¸»è¦ä¼˜åŠ¿\n")
            f.write("- è‡ªé€‚åº”å†³ç­–èƒ½åŠ›\n")
            f.write("- å¤šæ™ºèƒ½ä½“ååŒä¼˜åŒ–\n")
            f.write("- å¯¹åŠ¨æ€äº‹ä»¶çš„é²æ£’æ€§\n")
            f.write("- æ— éœ€äººå·¥è§„åˆ™è®¾è®¡\n\n")
        
        print(f"ğŸ“„ æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        self.shared_network.actor.save(f"{filepath}_actor.keras")
        self.shared_network.critic.save(f"{filepath}_critic.keras")
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {filepath}_actor.keras å’Œ {filepath}_critic.keras")

    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        try:
            self.shared_network.actor = tf.keras.models.load_model(f"{filepath}_actor.keras")
            self.shared_network.critic = tf.keras.models.load_model(f"{filepath}_critic.keras")
            print(f"âœ… æ¨¡å‹å·²ä» {filepath} åŠ è½½")
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ Wå·¥å‚å…¨åŠŸèƒ½å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ")
    print("ğŸ¯ é›†æˆTensorBoardã€åŸºå‡†å¯¹æ¯”ã€å¯è§†åŒ–åˆ†æ")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    tf.random.set_seed(RANDOM_SEED)
    
    try:
        # åˆ›å»ºå…¨åŠŸèƒ½è®­ç»ƒå™¨
        trainer = FullFeaturedMARLTrainer({
            'lr': 3e-4,
        })
        
        # å¼€å§‹è®­ç»ƒ
        results = trainer.progressive_train(
            static_episodes=60,    # é™æ€ç¯å¢ƒè®­ç»ƒ
            dynamic_episodes=20,   # åŠ¨æ€ç¯å¢ƒå¾®è°ƒ
            steps_per_episode=200
        )
        
        if results:
            print("\n" + "ğŸ‰" * 25)
            print("ğŸ‰ å…¨åŠŸèƒ½MARLè®­ç»ƒå®Œæˆï¼")
            print("ğŸ‰" * 25)
            
            print("\nâœ… å®Œæˆçš„åŠŸèƒ½:")
            print("  â€¢ é€’è¿›å¼MARLè®­ç»ƒ")
            print("  â€¢ TensorBoardå¯è§†åŒ–")
            print("  â€¢ åŸºå‡†ç®—æ³•å¯¹æ¯” (FIFO/SPT/EDD)")
            print("  â€¢ è¯¦ç»†æ€§èƒ½åˆ†æ")
            print("  â€¢ å›¾è¡¨å¯è§†åŒ–")
            print("  â€¢ æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ")
            
            final_eval = results['evaluations']['final_evaluation']['summary']
            baseline_results = results['baseline_comparison']
            
            print(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½å¯¹æ¯” (ğŸ”§ V7ä¿®å¤ç‰ˆ - å…¬å¹³ä»¿çœŸå¯¹æ¯”):")
            marl_makespan = final_eval['mean_makespan']
            marl_utilization = final_eval['mean_utilization']
            
            print(f"  MARL - Makespan: {marl_makespan:.1f}, åˆ©ç”¨ç‡: {marl_utilization:.1%}")
            
            # è¯¦ç»†çš„åŸºå‡†å¯¹æ¯”
            best_baseline_makespan = float('inf')
            best_algorithm = "None"
            
            for alg, stats in baseline_results.items():
                makespan = stats.get('makespan', float('inf'))
                utilization = stats.get('mean_utilization', 0)
                completed = stats.get('completed_parts', 0)
                
                print(f"  {alg:4} - Makespan: {makespan:.1f}, åˆ©ç”¨ç‡: {utilization:.1%}, å®Œæˆ: {completed}")
                
                if makespan < best_baseline_makespan:
                    best_baseline_makespan = makespan
                    best_algorithm = alg
            
            # ğŸ”§ å…³é”®éªŒè¯ï¼šæ£€æŸ¥ç»“æœçš„åˆç†æ€§
            print(f"\nğŸ” ç»“æœéªŒè¯:")
            print(f"  æœ€ä½³ä¼ ç»Ÿç®—æ³•: {best_algorithm} (Makespan: {best_baseline_makespan:.1f})")
            
            if marl_makespan < best_baseline_makespan:
                improvement = (best_baseline_makespan - marl_makespan) / best_baseline_makespan * 100
                print(f"  âœ… MARLç›¸å¯¹æ”¹è¿›: {improvement:.1f}% (è¿™æ˜¯çœŸå®çš„æ€§èƒ½æå‡)")
            elif marl_makespan > best_baseline_makespan:
                degradation = (marl_makespan - best_baseline_makespan) / best_baseline_makespan * 100
                print(f"  âš ï¸  MARLè¡¨ç°: æ¯”æœ€ä½³åŸºå‡†å·®{degradation:.1f}% (éœ€è¦è¿›ä¸€æ­¥è®­ç»ƒ)")
            else:
                print(f"  ğŸ“Š MARLè¡¨ç°: ä¸æœ€ä½³åŸºå‡†ç›¸å½“")
            
            # åˆç†æ€§æ£€æŸ¥
            if marl_utilization > 0 and best_baseline_makespan != float('inf'):
                print(f"  âœ… è®¾å¤‡åˆ©ç”¨ç‡æ­£å¸¸: {marl_utilization:.1%}")
                print(f"  âœ… åŸºå‡†ç®—æ³•è¿è¡ŒæˆåŠŸ")
                print(f"  âœ… è¿™æ˜¯ä¸€ä¸ªå¯ä¿¡çš„å¯¹æ¯”ç»“æœ")
            else:
                print(f"  âŒ è­¦å‘Š: æ£€æµ‹åˆ°å¼‚å¸¸æ•°æ®ï¼Œç»“æœå¯èƒ½ä¸å¯ä¿¡")
            
            if TENSORBOARD_AVAILABLE:
                print(f"\nğŸ“ˆ æŸ¥çœ‹TensorBoard:")
                print(f"  tensorboard --logdir {trainer.log_dir}")
            
        else:
            print("\nâŒ è®­ç»ƒå¤±è´¥")
            
    except Exception as e:
        print(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 