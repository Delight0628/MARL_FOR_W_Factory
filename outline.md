# 面向动态柔性作业车间的双阶段多智能体强化学习调度方法研究

## 第一章 绪论

### 1.1 研究背景与意义

#### 1.1.1 研究背景

随着新一代信息技术与先进制造技术的深度融合，全球制造业正经历着一场深刻的数字化与智能化变革。在“工业4.0”和“中国制造2025”等国家战略的不断推进与深化下，特别是面对2024年以来国家提出的“加快发展新质生产力”和《智能制造典型场景参考指引（2025年版）》的政策导向，我国制造业正逐步从初级的数字化、网络化阶段迈向更高层次的智能化发展阶段。智能制造作为新一轮工业革命的核心驱动力，已成为衡量一个国家制造业综合实力和核心竞争力的重要标志。在这一背景下，生产调度作为制造系统中连接订单需求与实际生产执行的关键环节，发挥着类似于“神经中枢”的核心作用，其优化水平直接决定了制造过程的效率与灵活性。

与此同时，随着消费结构的升级以及市场环境的变迁，传统制造业的生态体系正经历着深刻的重构。

（1）市场需求从“大规模制造”转向“大规模定制”。

（2）生产系统的动态性与不确定性显著增加。

（3）智能调度成为转型的关键突破口。

#### 1.1.2 研究意义

本研究所聚焦的W工厂生产调度场景，是柔性作业车间调度问题（Flexible Job-shop Scheduling Problem, FJSP）的一个典型实例。该问题在“多品种、小批量”混线生产与动态扰动并存的背景下，呈现出路由分配与工序排序耦合、扰动不可预测、以及多目标权衡复杂等特征。因此，探索一种能够自学习、自适应并具备强泛化能力的智能调度方法，既具有重要学术价值，也具有显著工程意义。

### 1.2 国内外研究现状

#### 1.2.1 传统调度方法及其局限性

- **精确求解方法**：存在算力瓶颈，且依赖静态假设。
- **启发式调度规则**：短视且难以平衡多目标.
- **元启发式算法**：存在实时响应与调度稳定性矛盾.

#### 1.2.2 基于深度强化学习的调度研究进展

- **状态表征**：从人工特征到图结构嵌入.
- **决策机制**：规则选择与构造式决策两类.
- **训练算法与奖励设计**：PPO/Actor-Critic成为主流，稠密奖励与信用分配是关键.
- **动态适应与泛化**：域随机化与课程学习成为重要方向.

#### 1.2.3 多智能体强化学习在协同调度中的应用与挑战

- **CTDE范式**：训练阶段利用全局信息降低方差，执行阶段仅依赖局部观测.
- **关键挑战**：环境非平稳性、信用分配、异构智能体协同.

### 1.3 本文研究内容与创新点

#### 1.3.1 研究内容

- **环境建模**：构建基于DES的W工厂数字孪生环境，明确状态—动作—奖励及扰动注入机制.
- **算法设计**：采用CTDE下的MAPPO框架，使用参数共享Actor与集中式Critic实现协同调度.
- **训练框架**：提出双阶段课程学习以提升收敛稳定性与动态鲁棒性.
- **系统实现**：实现训练、评估与可视化原型系统，支撑工程验证.

#### 1.3.2 创新点

- **候选集+多头动作+掩码+无放回采样的并行分配机制**：在固定动作维度下表达动态队列决策，并将可行性约束显式注入采样过程.
- **双阶段课程学习**：先学习基础调度规律，再引入紧急插单与设备故障提升鲁棒性.
- **锚点任务轮换的稳定训练机制**：在第二阶段周期性插入基准任务回合，缓解灾难性遗忘.

### 1.4 论文组织结构

- **第1章**：绪论.
- **第2章**：理论基础与方法支撑.
- **第3章**：MARL环境建模.
- **第4章**：本文方法（MAPPO与训练策略）.
- **第5章**：实验与结果分析.
- **第6章**：调度系统原型实现.
- **第7章**：总结与展望.

## 第二章 理论基础

### 2.1 动态柔性作业车间调度问题建模基础

- **2.1.1 FJSP/DFJSP基本定义与约束类型**：工序先后约束、机器容量约束、不可抢占约束等.
- **2.1.2 动态扰动建模**：
  - **紧急插单**：泊松过程与指数到达间隔.
  - **设备故障**：MTBF/MTTR指数过程（可讨论Weibull扩展）.
- **2.1.3 从精确优化到学习式调度的动机**：实时性、非平稳性与重调度成本.

### 2.2 多智能体强化学习理论

- **2.2.1 Markov Game与Dec-POMDP**：说明“局部可观测+全局共享目标”的本质.
- **2.2.2 CTDE范式**：训练阶段可访问全局信息，执行阶段仅依赖局部观测.
- **2.2.3 策略梯度与Actor-Critic**：为PPO/MAPPO的目标函数做铺垫.

### 2.3 PPO与MAPPO算法基础

- **2.3.1 PPO-Clip目标与优势估计（GAE）**：给出核心公式与变量解释.
- **2.3.2 集中式价值函数的作用**：降低优势估计方差，缓解非平稳性.
- **2.3.3 训练稳定技巧**：优势标准化、梯度裁剪、价值裁剪等.

### 2.4 约束动作空间与可行性注入

- **2.4.1 动作掩码（Action Masking）**：将可行动作集合显式注入策略分布.
- **2.4.2 多头离散动作与互斥约束**：通过无放回采样避免重复选择.
- **2.4.3 候选集动作空间**：将动态队列决策映射为固定离散空间以提升可训练性.

### 2.5 课程学习与遗忘抑制

- **2.5.1 Curriculum Learning基本思想**：由易到难的任务序列.
- **2.5.2 灾难性遗忘与经验混合**：在新场景学习时保持旧场景性能下限.
- **2.5.3 本文训练策略的理论映射**：双阶段训练与锚点任务轮换的设计动机.

## 第三章 MARL环境建模

本章围绕W工厂家具生产车间的实际业务背景与生产特征，构建可用于多智能体强化学习训练与评估的高保真仿真环境。环境基于离散事件仿真（DES）实现，并封装为PettingZoo并行接口以支撑并行采样.

### 3.1 W工厂生产场景分析与抽象

- **3.1.1 资源配置与工位划分**：5个工作站智能体（带锯机、五轴加工机、砂光机、组装台、包装台），并行设备最大数为2.
- **3.1.2 工艺路线与资源约束**：产品路线三元组与容量/先后/队列约束.
- **3.1.3 订单建模与动态扰动注入**：回合初订单生成；回合内紧急插单与设备故障.

### 3.2 智能体与状态空间设计

- **3.2.1 异构工作站级智能体定义**.
- **3.2.2 层次化状态空间（132维观测）**：自身特征、全局摘要、队列统计、候选工件特征.

### 3.3 动作空间与交互机制

- **3.3.1 候选动作空间（11维：IDLE+10 candidates）**.
- **3.3.2 多头并行动作表示（K=2）**：统一表达并行设备分配.
- **3.3.3 动作掩码（Action Mask）与合法性保证**.
- **3.3.4 无放回采样（Sampling Without Replacement）**：多头互斥分配.

### 3.4 奖励函数设计

- **3.4.1 完工驱动层**：缓解稀疏奖励.
- **3.4.2 交期质量层**：延期惩罚与稳健化处理.
- **3.4.3 风险塑形层**：基于松弛时间的前瞻性引导.
- **3.4.4 系统稳定性约束层**：WIP、空闲与无效动作惩罚.

### 3.5 本章小结

## 第四章 本文方法：MAPPO调度框架

本章聚焦“如何在动态扰动与多工位耦合的复杂场景下学习可泛化的调度策略”，提出一种基于多智能体近端策略优化算法（MAPPO）的求解框架。方法采用集中训练—分散执行（CTDE）范式：执行阶段每个智能体仅使用局部观测做决策；训练阶段利用全局状态构建集中式价值函数，从而提供更低方差、更一致的优势估计信号以指导策略更新.

### 4.1 算法总体框架

#### 4.1.1 CTDE下的Actor-Critic建模

设系统包含 $n$ 个工作站智能体，第 $i$ 个智能体在时刻 $t$ 的局部观测为 $o_i(t)$，联合动作为 $\vec{a}(t)=(a_1(t),\dots,a_n(t))$，全局状态为 $s(t)$。执行阶段，智能体策略以局部观测为条件输出动作分布：

$$
a_i(t) \sim \pi_\theta\bigl(a_i(t)\mid o_i(t)\bigr)
$$

训练阶段，集中式价值函数以全局状态为输入估计状态价值：

$$
V_\phi\bigl(s(t), i\bigr) \approx \mathbb{E}\left[\sum_{k=0}^{\infty}\gamma^k r(t+k)\mid s(t)\right]
$$

其中 $\theta$ 与 $\phi$ 分别为Actor与Critic参数，$\gamma$ 为折扣因子。为提高样本效率，本文采用参数共享的Actor结构：所有工作站共享同一套策略网络参数，但通过观测向量中的工位类型编码与负荷状态实现条件化.

需要强调的是，本文集中式Critic的实现采用“agent-conditioned”形式：其输入由全局状态与智能体身份独热编码拼接而成，从而在共享网络参数的前提下保持对不同工位的价值评估区分.

#### 4.1.2 轨迹采样、GAE优势估计与PPO剪切更新

MAPPO训练流程可概括为“并行采样—优势估计—剪切更新—循环迭代”。令TD残差为：

$$
\delta_t=r(t)+\gamma V_\phi\bigl(s(t+1)\bigr)-V_\phi\bigl(s(t)\bigr)
$$

则GAE优势估计为：

$$
\hat{A}_t=\sum_{l=0}^{\infty}(\gamma\lambda)^l\,\delta_{t+l}
$$

定义概率比率为：

$$
r_t(\theta)=\frac{\pi_\theta(a_t\mid o_t)}{\pi_{\theta_{old}}(a_t\mid o_t)}
$$

则PPO剪切目标为：

$$
L^{clip}(\theta)=\mathbb{E}_t\left[\min\Bigl(r_t(\theta)\hat{A}_t,\;\operatorname{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A}_t\Bigr)\right]
$$

Critic通过最小化价值回归损失进行更新.

#### 4.1.3 与本文环境的接口对齐：掩码、多头与无冲突采样

由于W工厂存在并行设备，且动作采用“候选集离散选择”，执行阶段策略输出为“多头离散动作向量”。为保证动作合法性并减少无效探索，策略输出需结合动作掩码进行归一化与采样，并在多头之间采用无放回机制避免重复选择同一工件。训练阶段同样需要记录掩码约束下的对数概率，以保证PPO比率计算与更新目标一致.

### 4.2 网络结构与动作采样实现

- **4.2.1 Actor网络**：输入132维局部观测；输出为 $K=2$ 个Softmax头，每头输出11维离散分布.
- **4.2.2 Critic网络**：输入为全局状态与智能体身份one-hot拼接；输出标量价值.
- **4.2.3 动作掩码与无放回采样的一致性**：采样与logprob计算在同一掩码/互斥规则下进行.

### 4.3 双阶段课程学习训练策略

- **4.3.1 第一阶段（Foundation）**：关闭动态事件，覆盖多样初始订单与产品组合，学习基础调度规律.
- **4.3.2 第二阶段（Generalization）**：开启紧急插单与设备故障，并对扰动参数进行范围随机化提升鲁棒性.
- **4.3.3 阶段切换与完成判据**：基于综合评分、完工率、延期等指标设置一致性阈值.

### 4.4 稳定性增强机制

- **4.4.1 锚点任务轮换（Anchor）**：第二阶段训练中按回合周期插入固定基准订单回合，用于抑制灾难性遗忘.
- **4.4.2 归一化与裁剪**：状态/奖励归一化、优势标准化、PPO剪切与梯度裁剪.
- **4.4.3 熵正则项**：作为鼓励探索的机制给出，但默认配置可关闭自适应熵模块，并在实验中按需做消融.

### 4.5 算法伪代码与复杂度分析

- **4.5.1 训练流程伪代码**：采样、GAE、PPO更新、阶段切换、模型保存.
- **4.5.2 时间复杂度与并行加速来源**：并行采样、批量更新、推理复杂度.

## 第五章 实验与结果分析

### 5.1 实验设置

- **5.1.1 仿真参数与训练超参数**：与配置文件一致（仿真时长、订单规模、并行环境数、PPO epoch等）.
- **5.1.2 评价指标**：Makespan、Total Tardiness、完工率、设备利用率、WIP水平等.
- **5.1.3 对比基线**：FIFO/SPT/EDD等启发式规则；（可选）禁用关键模块的消融对比.

### 5.2 训练过程与收敛性分析

- **5.2.1 Foundation阶段收敛曲线**.
- **5.2.2 Generalization阶段性能回撤与再收敛**.
- **5.2.3 关键超参数敏感性（可选）**：候选规模、奖励权重、clip系数等.

### 5.3 静态场景性能对比

- **5.3.1 指标对比表与显著性检验（可选）**.
- **5.3.2 甘特图与瓶颈分析**：展示调度紧凑性与空闲碎片减少.

### 5.4 动态鲁棒性评估

- **5.4.1 故障强度变化下的鲁棒性曲线**.
- **5.4.2 插单强度变化下的响应能力**.
- **5.4.3 复合扰动下的综合表现**.

### 5.5 消融实验

- **5.5.1 去除双阶段课程学习（No Curriculum）**.
- **5.5.2 去除锚点任务轮换（No Anchor）**.
- **5.5.3 去除动作掩码/无放回采样（No Mask/No SWR）**.

## 第六章 调度原型系统设计与实现

### 6.1 系统需求与总体架构

- **6.1.1 功能需求**：订单配置、模型推理、仿真执行、KPI输出、可视化.
- **6.1.2 技术栈**：Python + SimPy + PettingZoo + TensorFlow；Streamlit + Plotly.

## 参考文献
