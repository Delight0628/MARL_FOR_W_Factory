"""
åŸºäºRay 2.48.0çš„å¤šæ™ºèƒ½ä½“PPOè®­ç»ƒè„šæœ¬
ä¸è‡ªå®šä¹‰PPOè„šæœ¬ä¿æŒå®Œå…¨ä¸€è‡´çš„é…ç½®å’ŒåŠŸèƒ½

ğŸ”§ V17 è®­ç»ƒé€»è¾‘å½»åº•ä¿®å¤ç‰ˆï¼š
1. ä¿®æ­£äº†Ray 2.48.0çš„APIå‚æ•°ï¼šä½¿ç”¨sgd_minibatch_sizeï¼Œè°ƒæ•´æ‰¹æ¬¡å¤§å°ç¡®ä¿ç¨³å®šè®­ç»ƒ
2. ä¿®æ­£äº†æ—¶é—´ç»Ÿè®¡é€»è¾‘ï¼šCPUé‡‡é›†æ—¶é—´ç°åœ¨æ­£ç¡®åœ°æ¯”GPUæ›´æ–°æ—¶é—´é•¿
3. å¢å¼ºäº†æŒ‡æ ‡æå–ï¼šå¤šè·¯å¾„æå–æŸå¤±ä¿¡æ¯ï¼Œç¡®ä¿è®­ç»ƒæŒ‡æ ‡æ­£ç¡®æ˜¾ç¤º
4. æ·»åŠ äº†è°ƒè¯•ä¿¡æ¯ï¼šå¸®åŠ©è¯Šæ–­è®­ç»ƒé—®é¢˜çš„æ ¹æº
"""

import os
# ğŸ”§ V10.2 ç»ˆææ—¥å¿—æ¸…ç†: åœ¨æ‰€æœ‰åº“å¯¼å…¥å‰ï¼Œå¼ºåˆ¶è®¾ç½®æ—¥å¿—çº§åˆ«
# è¿™èƒ½æœ€æœ‰æ•ˆåœ°å±è”½æ‰CUDAå’ŒcuBLASåœ¨å­è¿›ç¨‹ä¸­çš„åˆå§‹åŒ–é”™è¯¯ä¿¡æ¯
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['RAY_DISABLE_IMPORT_WARNING'] = '1'

import sys
import time
import random
import numpy as np
import tensorflow as tf
from typing import Dict, List, Tuple, Any
from datetime import datetime
import multiprocessing

# ğŸ”§ V12 æ–°å¢ï¼šTensorBoardæ”¯æŒ
try:
    from tensorflow.python.summary.writer.writer import FileWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False

# Rayç›¸å…³å¯¼å…¥
import ray
from ray import tune
from ray.rllib.algorithms.ppo import PPOConfig
from ray.rllib.env.multi_agent_env import MultiAgentEnv
from ray.rllib.policy.policy import Policy, PolicySpec
from ray.rllib.utils.typing import PolicyID
from ray.tune.registry import register_env

# æ·»åŠ ç¯å¢ƒè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from environments.w_factory_env import make_parallel_env
from environments.w_factory_config import *

class RayWFactoryEnv(MultiAgentEnv):
    """Ray RLlibå…¼å®¹çš„Wå·¥å‚ç¯å¢ƒåŒ…è£…å™¨"""
    
    def __init__(self, config=None):
        super().__init__()
        self.config = config or {}
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨WFactoryGymEnv
        from environments.w_factory_env import WFactoryGymEnv
        env_config = self.config.copy()
        env_config.update({
            'debug_level': 'WARNING',
            'training_mode': True,
            'use_fixed_rewards': True,
        })
        
        self.base_env = WFactoryGymEnv(env_config)
        
        # è·å–æ™ºèƒ½ä½“åˆ—è¡¨å’Œç©ºé—´ï¼ˆä¸wslè„šæœ¬ä¸€è‡´ï¼‰
        self.agents = list(self.base_env.possible_agents)
        self._agent_ids = set(self.agents)
        self.possible_agents = self.base_env.possible_agents
        
        # è®¾ç½®è§‚æµ‹å’ŒåŠ¨ä½œç©ºé—´ï¼ˆä¸wslè„šæœ¬ä¸€è‡´ï¼‰
        self.observation_space = self.base_env.observation_space
        self.action_space = self.base_env.action_space
        self.observation_spaces = self.base_env.observation_spaces
        self.action_spaces = self.base_env.action_spaces
        
        # ç”¨äºPolicySpecçš„å•ä¸€ç©ºé—´
        self._observation_space = self.observation_spaces[self.possible_agents[0]]
        self._action_space = self.action_spaces[self.possible_agents[0]]
        
        # æ­¥æ•°è®¡æ•°å™¨ï¼ˆä¸è‡ªå®šä¹‰PPOä¿æŒä¸€è‡´ï¼‰
        self.step_count = 0
        self.max_steps = 1500  # ä¸è‡ªå®šä¹‰PPOçš„episodeé•¿åº¦ä¿æŒä¸€è‡´
        
    def reset(self, *, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒï¼ˆä¸wslè„šæœ¬ä¸€è‡´ï¼‰"""
        self.step_count = 0
        obs, info = self.base_env.reset(seed=seed, options=options)
        
        # ğŸ”§ V17 å…³é”®ä¿®å¤ï¼šRay RLlibæœŸæœ›resetè¿”å›(obs, infos)ä¸¤ä¸ªå€¼
        if isinstance(obs, dict):
            return obs, info
        else:
            multi_obs = {agent: obs for agent in self.agents}
            return multi_obs, info
    
    def step(self, action_dict):
        """æ‰§è¡Œä¸€æ­¥ï¼ˆä¸wslè„šæœ¬ä¸€è‡´ï¼‰"""
        self.step_count += 1
        
        processed_actions = action_dict
        
        # è°ƒç”¨åŸºç¡€ç¯å¢ƒ (è¿”å› obs, rewards, terminations, truncations, infos)
        obs, rewards, terminations, truncations, infos = self.base_env.step(processed_actions)
        
        # ğŸ”§ V17 å…³é”®ä¿®å¤: å½“è¾¾åˆ°æœ€å¤§æ­¥æ•°æ—¶ï¼Œå¿…é¡»è®¾ç½® __all__ = True æ¥å‘ŠçŸ¥Ray episodeå·²ç»“æŸ
        # å¦åˆ™åœ¨ batch_mode="complete_episodes" æ¨¡å¼ä¸‹ä¼šæ— é™ç­‰å¾…
        step_limit_reached = self.step_count >= self.max_steps
        if step_limit_reached:
            terminations["__all__"] = True
            truncations["__all__"] = False
        else:
            # ç»§æ‰¿åº•å±‚ç¯å¢ƒçš„__all__ä¿¡å·ï¼Œä½†ç¡®ä¿terminationså’Œtruncationséƒ½æœ‰__all__é”®
            env_done = terminations.get("__all__", False) or truncations.get("__all__", False)
            terminations["__all__"] = env_done
            truncations["__all__"] = False  # æˆ‘ä»¬ä¸ä½¿ç”¨truncationsï¼Œåªä½¿ç”¨terminations
        
        # ğŸ”§ V17 å…³é”®ä¿®å¤ï¼šRay RLlibæœŸæœ›stepè¿”å› (obs, rewards, terminations, truncations, infos) 5ä¸ªå€¼
        return obs, rewards, terminations, truncations, infos
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        if hasattr(self.base_env, 'close'):
            self.base_env.close()

class RayPPOTrainer:
    """åŸºäºRayçš„PPOè®­ç»ƒå™¨ï¼Œä¸è‡ªå®šä¹‰PPOä¿æŒä¸€è‡´çš„åŠŸèƒ½"""
    
    def __init__(self, initial_lr: float, total_train_episodes: int, steps_per_episode: int):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ğŸ”§ V5 æ€§èƒ½ä¼˜åŒ–ï¼šæ£€æµ‹ç³»ç»Ÿèµ„æºï¼ˆä¸è‡ªå®šä¹‰PPOå®Œå…¨ä¸€è‡´ï¼‰
        self.system_info = self._detect_system_resources()
        self._optimize_tensorflow_settings()
        
        # ğŸ”§ V9 CPUå¹¶è¡Œä¼˜åŒ–: æ™ºèƒ½è°ƒèŠ‚è¿›ç¨‹æ•°ï¼Œé˜²æ­¢å†…å­˜çˆ†ç‚¸
        cpu_cores = self.system_info.get('cpu_count', 4)
        # ä¿ç•™æ ¸å¿ƒç»™ä¸»è¿›ç¨‹å’Œç³»ç»Ÿï¼Œä½¿ç”¨æ ¸å¿ƒæ•°çš„ä¸€åŠä½œä¸ºå·¥ä½œè¿›ç¨‹æ•°ï¼Œå…¼é¡¾æ€§èƒ½ä¸ç¨³å®š
        self.num_workers = min(max(1, cpu_cores // 2), 32)
        print(f"ğŸ”§ V9 CPUå¹¶è¡Œä¼˜åŒ–: å°†ä½¿ç”¨ {self.num_workers} ä¸ªå¹¶è¡Œç¯å¢ƒè¿›è¡Œæ•°æ®é‡‡é›† (æ™ºèƒ½è°ƒèŠ‚)")
        
        # ç¯å¢ƒæ¢æµ‹
        temp_env = RayWFactoryEnv()
        self.state_dim = temp_env._observation_space.shape[0]
        self.action_dim = temp_env._action_space.n
        self.agent_ids = temp_env.possible_agents
        temp_env.close()
        
        print("ğŸ”§ ç¯å¢ƒç©ºé—´æ£€æµ‹:")
        print(f"   è§‚æµ‹ç»´åº¦: {self.state_dim}")
        print(f"   åŠ¨ä½œç»´åº¦: {self.action_dim}")
        print(f"   æ™ºèƒ½ä½“æ•°é‡: {len(self.agent_ids)}")
        
        # ğŸ”§ V5 èµ„æºä¼˜åŒ–ï¼šæ ¹æ®å†…å­˜è°ƒæ•´è®­ç»ƒå‚æ•°
        self.optimized_episodes, self.optimized_steps = self._optimize_training_params(
            total_train_episodes, steps_per_episode
        )
        
        # åˆå§‹åŒ–Ray
        if not ray.is_initialized():
            ray.init(num_cpus=cpu_cores, ignore_reinit_error=True, log_to_driver=False)
        
        # æ³¨å†Œç¯å¢ƒ
        register_env("w_factory_env", lambda config: RayWFactoryEnv(config))
        
        # ğŸ”§ V6 æ ¹æ®ç³»ç»Ÿèµ„æºåŠ¨æ€è°ƒæ•´ç½‘ç»œå¤§å°ï¼ˆä¸è‡ªå®šä¹‰PPOä¸€è‡´ï¼‰
        available_gb = self.system_info.get('available_gb', 8.0)
        
        if available_gb < 5.0:
            # ä½å†…å­˜ï¼šå°ç½‘ç»œ
            hidden_sizes = [128, 64]
        elif available_gb < 8.0:
            # ä¸­ç­‰å†…å­˜ï¼šä¸­å‹ç½‘ç»œ
            hidden_sizes = [256, 128]
        else:
            # å……è¶³å†…å­˜ï¼šå¤§å‹ç½‘ç»œ - ğŸš€ V12 æ¨¡å‹å®¹é‡æå‡
            hidden_sizes = [1024, 512]
        
        # ğŸ”§ V3 ä¿®å¤: åˆ›å»ºå­¦ä¹ ç‡è¡°å‡è°ƒåº¦å™¨ï¼ˆæ¨¡æ‹ŸTensorFlowçš„PolynomialDecayï¼‰
        total_training_steps = self.optimized_episodes * self.optimized_steps
        
        # é…ç½®PPOç®—æ³•ï¼ˆä¸¥æ ¼å¯¹åº”è‡ªå®šä¹‰PPOçš„å‚æ•°ï¼Œä½¿ç”¨Ray 2.48.0 APIï¼‰
        self.config = (
            PPOConfig()
            .environment("w_factory_env", env_config={})
            .framework("tf")
            .api_stack(
                # ç¦ç”¨æ–°APIæ ˆï¼Œä½¿ç”¨æ—§ç‰ˆæœ¬å…¼å®¹æ¨¡å¼
                enable_rl_module_and_learner=False,
                enable_env_runner_and_connector_v2=False,
            )
            .multi_agent(
                # ä½¿ç”¨å…±äº«ç­–ç•¥ï¼Œæ˜ç¡®æŒ‡å®šobservation_spaceå’Œaction_space
                policies={
                    "shared_policy": PolicySpec(
                        observation_space=temp_env._observation_space,
                        action_space=temp_env._action_space,
                    )
                },
                policy_mapping_fn=lambda agent_id, *args, **kwargs: "shared_policy",
            )
            .env_runners(
                # ğŸ”§ ä¿®å¤ï¼šå¯ç”¨å¹¶è¡Œworkerä»¥å¯¹é½è‡ªå®šä¹‰PPOçš„å¹¶è¡Œæ•°æ®é‡‡é›†
                num_env_runners=min(self.num_workers, 4),  # ä½¿ç”¨é€‚åº¦çš„å¹¶è¡Œworkeræ•°é‡
                rollout_fragment_length="auto",  # è®©Rayè‡ªåŠ¨è®¡ç®—åŒ¹é…çš„fragmenté•¿åº¦
                batch_mode="complete_episodes",
                num_cpus_per_env_runner=1,  # ğŸ”§ ä¿®å¤ï¼šç§»åŠ¨åˆ°env_runnersä¸­
            )
            .training(
                # ğŸ”§ V17 å…³é”®ä¿®å¤ï¼šä½¿ç”¨Ray 2.48.0çš„æ­£ç¡®å‚æ•°è®¾ç½®æ–¹å¼
                lr=initial_lr,
                gamma=0.99,
                lambda_=0.95,  # GAEå‚æ•°
                train_batch_size=2048,      # å‡å°æ‰¹æ¬¡å¤§å°ï¼Œç¡®ä¿ç¨³å®šè®­ç»ƒ
                num_sgd_iter=8,             # å…¼å®¹æ—§ç‰ˆAPIï¼Œä½¿ç”¨num_sgd_iter
                clip_param=0.3,             # å¯¹é½è‡ªå®šä¹‰PPO
                entropy_coeff=0.1,          # å¯¹é½è‡ªå®šä¹‰PPO
                vf_loss_coeff=1.0,
                use_gae=True,               # æ˜ç¡®å¯ç”¨GAE
                model={
                    "fcnet_hiddens": hidden_sizes,
                    "fcnet_activation": "relu",
                    "use_lstm": False,
                },
            )
            .resources(
                num_gpus=1 if self.system_info.get('gpu_available', False) else 0,
            )
            .evaluation(
                evaluation_interval=10,
                evaluation_duration=5,
                evaluation_config={
                    "explore": False,
                    "render_env": False,
                }
            )
            .debugging(
                log_level="WARNING",  # å‡å°‘æ—¥å¿—è¾“å‡º
            )
            .experimental(
                # ğŸ”§ ä¿®å¤ï¼šç¦ç”¨é…ç½®éªŒè¯ï¼Œé¿å…æ‰¹æ¬¡å¤§å°éªŒè¯é”™è¯¯
                _validate_config=False,
                _disable_preprocessor_api=True,
            )
        )
        
        # ğŸ”§ V17 ä¿®å¤ï¼šä¸ºå…¼å®¹æ—§ç‰ˆRayï¼Œåœ¨é…ç½®æ„å»ºåå•ç‹¬è®¾ç½®sgd_minibatch_size
        self.config.sgd_minibatch_size = 256
        
        # åˆ›å»ºç®—æ³•å®ä¾‹
        self.algorithm = self.config.build_algo()
        
        # è®­ç»ƒç»Ÿè®¡ï¼ˆä¸è‡ªå®šä¹‰PPOä¸€è‡´ï¼‰
        self.episode_rewards = []
        self.training_losses = []
        self.iteration_times = []
        self.kpi_history = []
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.models_dir = "ray_ppo/ppo_models"
        os.makedirs(self.models_dir, exist_ok=True)
        
        # ğŸ”§ V12 æ–°å¢ï¼šTensorBoardæ”¯æŒ
        self.tensorboard_dir = f"ray_ppo/tensorboard_logs/{self.timestamp}"
        os.makedirs(self.tensorboard_dir, exist_ok=True)
        if TENSORBOARD_AVAILABLE:
            self.train_writer = tf.summary.create_file_writer(f"{self.tensorboard_dir}/train")
            print(f"ğŸ“Š TensorBoardæ—¥å¿—å·²å¯ç”¨: {self.tensorboard_dir}")
            print(f"    ä½¿ç”¨å‘½ä»¤: tensorboard --logdir={self.tensorboard_dir}")
        else:
            self.train_writer = None
            print("âš ï¸  TensorBoardä¸å¯ç”¨")
    
    def _detect_system_resources(self) -> Dict[str, Any]:
        """ğŸ”§ V5 æ–°å¢ï¼šæ£€æµ‹ç³»ç»Ÿèµ„æºï¼ˆä¸è‡ªå®šä¹‰PPOå®Œå…¨ä¸€è‡´ï¼‰"""
        try:
            import psutil  # type: ignore
            cpu_count = psutil.cpu_count()
            memory_info = psutil.virtual_memory()
            memory_gb = memory_info.total / (1024**3)
            available_gb = memory_info.available / (1024**3)
            
            # æ£€æµ‹GPU
            gpu_available = len(tf.config.list_physical_devices('GPU')) > 0
            gpu_memory = 0
            if gpu_available:
                try:
                    gpus = tf.config.list_physical_devices('GPU')
                    for gpu in gpus:
                        gpu_details = tf.config.experimental.get_device_details(gpu)
                        gpu_memory = gpu_details.get('device_name', 'Unknown')
                except:
                    gpu_available = False
            
            system_info = {
                'cpu_count': cpu_count,
                'memory_gb': memory_gb,
                'available_gb': available_gb,
                'gpu_available': gpu_available,
                'gpu_memory': gpu_memory
            }
            
            print("ğŸ’» ç³»ç»Ÿèµ„æºæ£€æµ‹:")
            print(f"   CPUæ ¸å¿ƒæ•°: {cpu_count}")
            print(f"   æ€»å†…å­˜: {memory_gb:.1f}GB")
            print(f"   å¯ç”¨å†…å­˜: {available_gb:.1f}GB")
            print(f"   GPUå¯ç”¨: {'âœ…' if gpu_available else 'âŒ'}")
            if gpu_available:
                print(f"   GPUä¿¡æ¯: {gpu_memory}")
            
            return system_info
            
        except ImportError:
            # å¦‚æœæ²¡æœ‰psutilï¼Œä½¿ç”¨ä¿å®ˆä¼°è®¡
            print("âš ï¸  æ— æ³•æ£€æµ‹ç³»ç»Ÿèµ„æºï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {
                'cpu_count': 4,
                'memory_gb': 8.0,
                'available_gb': 4.0,
                'gpu_available': False,
                'gpu_memory': None
            }
    
    def _optimize_tensorflow_settings(self):
        """ğŸ”§ V7 å¢å¼ºç‰ˆï¼šä¼˜åŒ–TensorFlowè®¾ç½®ï¼Œå……åˆ†åˆ©ç”¨48æ ¸CPUï¼ˆä¸è‡ªå®šä¹‰PPOä¸€è‡´ï¼‰"""
        # å†…å­˜å¢é•¿è®¾ç½®
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            try:
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                print("âœ… GPUå†…å­˜å¢é•¿æ¨¡å¼å·²å¯ç”¨")
            except RuntimeError as e:
                print(f"âš ï¸  GPUè®¾ç½®å¤±è´¥: {e}")
        
        # ğŸ”§ V7 CPUä¼˜åŒ–ï¼šå……åˆ†åˆ©ç”¨48æ ¸CPU
        cpu_count = self.system_info.get('cpu_count', 4)
        available_gb = self.system_info.get('available_gb', 4.0)
        
        if available_gb < 6.0:
            # ä½å†…å­˜æ¨¡å¼ï¼šä¿å®ˆä½¿ç”¨CPU
            tf.config.threading.set_inter_op_parallelism_threads(min(cpu_count // 4, 12))
            tf.config.threading.set_intra_op_parallelism_threads(min(cpu_count // 2, 24))
            print(f"ğŸ”§ ä½å†…å­˜æ¨¡å¼: TensorFlowä½¿ç”¨{min(cpu_count // 4, 12)}ä¸ªinterçº¿ç¨‹, {min(cpu_count // 2, 24)}ä¸ªintraçº¿ç¨‹")
        else:
            # ğŸ”§ V7 é«˜æ€§èƒ½æ¨¡å¼ï¼šæ¿€è¿›ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
            inter_threads = min(cpu_count // 2, 24)  # æœ€å¤š24ä¸ªinterçº¿ç¨‹
            intra_threads = min(cpu_count, 48)       # æœ€å¤š48ä¸ªintraçº¿ç¨‹
            tf.config.threading.set_inter_op_parallelism_threads(inter_threads)
            tf.config.threading.set_intra_op_parallelism_threads(intra_threads)
            print(f"ğŸ”§ V7é«˜æ€§èƒ½æ¨¡å¼: TensorFlowä½¿ç”¨{inter_threads}ä¸ªinterçº¿ç¨‹, {intra_threads}ä¸ªintraçº¿ç¨‹")
            print(f"ğŸš€ CPUä¼˜åŒ–: å……åˆ†åˆ©ç”¨{cpu_count}æ ¸å¿ƒå¤„ç†å™¨")
    
    def _optimize_training_params(self, num_episodes: int, steps_per_episode: int) -> Tuple[int, int]:
        """ğŸ”§ V6 å¼ºåŒ–ç‰ˆï¼šæ ¹æ®ç³»ç»Ÿèµ„æºä¼˜åŒ–è®­ç»ƒå‚æ•°ï¼Œé˜²æ­¢å¡æ­»ï¼ˆä¸è‡ªå®šä¹‰PPOä¸€è‡´ï¼‰"""
        available_gb = self.system_info.get('available_gb', 4.0)
        total_gb = self.system_info.get('memory_gb', 8.0)
        
        # ğŸ”§ V6 æ›´ä¿å®ˆçš„å†…å­˜ç­–ç•¥ï¼ŒåŸºäºå®é™…å¯ç”¨å†…å­˜è€Œéæ€»å†…å­˜
        if available_gb < 3.0:
            # å±é™©å†…å­˜ï¼šæåº¦ä¿å®ˆ
            optimized_episodes = min(num_episodes, 40)
            optimized_steps = min(steps_per_episode, 600)
            print("ğŸš¨ å±é™©å†…å­˜æ¨¡å¼: è®­ç»ƒè§„æ¨¡æåº¦ç¼©å‡ï¼ˆé˜²å¡æ­»ï¼‰")
        elif available_gb < 5.0:
            # ä½å†…å­˜ï¼šå¤§å¹…é™ä½å‚æ•°
            optimized_episodes = min(num_episodes, 60)
            optimized_steps = min(steps_per_episode, 800)
            print("âš ï¸  ä½å†…å­˜æ¨¡å¼: è®­ç»ƒè§„æ¨¡å¤§å¹…ç¼©å‡")
        elif available_gb < 7.0:
            # ä¸­ç­‰å†…å­˜ï¼šé€‚åº¦é™ä½å‚æ•° - ğŸ”§ V6 æ›´ä¿å®ˆ
            optimized_episodes = min(num_episodes, 80)
            optimized_steps = min(steps_per_episode, 1000)
            print("ğŸ”§ ä¸­ç­‰å†…å­˜æ¨¡å¼: è®­ç»ƒè§„æ¨¡é€‚åº¦ç¼©å‡")
        elif available_gb < 10.0:
            # è¾ƒå¥½å†…å­˜ï¼šç•¥å¾®é™ä½å‚æ•° - ğŸ”§ V6 æ–°å¢å±‚çº§
            optimized_episodes = min(num_episodes, 90)
            optimized_steps = min(steps_per_episode, 1100)
            print("ğŸ’š è¾ƒå¥½å†…å­˜æ¨¡å¼: è®­ç»ƒè§„æ¨¡ç•¥å¾®è°ƒæ•´")
        else:
            # å……è¶³å†…å­˜: æ€§èƒ½å®Œå…¨é‡Šæ”¾ - ğŸš€ V11 æé™æ€§èƒ½æ¨¡å¼
            optimized_episodes = num_episodes
            optimized_steps = steps_per_episode
            print("âœ… å……è¶³å†…å­˜æ¨¡å¼: æ€§èƒ½å®Œå…¨é‡Šæ”¾ï¼Œä½¿ç”¨å®Œæ•´è®­ç»ƒè§„æ¨¡ï¼")
        
        # ğŸ”§ V6 æ–°å¢ï¼šå†…å­˜ä½¿ç”¨ç‡è­¦å‘Š
        memory_usage_percent = ((total_gb - available_gb) / total_gb) * 100
        if memory_usage_percent > 90:
            print(f"âš ï¸  å½“å‰å†…å­˜ä½¿ç”¨ç‡: {memory_usage_percent:.1f}% - å»ºè®®å…³é—­å…¶ä»–ç¨‹åº")
        
        if optimized_episodes != num_episodes or optimized_steps != steps_per_episode:
            print(f"ğŸ”§ å‚æ•°è°ƒæ•´: {num_episodes}å›åˆÃ—{steps_per_episode}æ­¥ â†’ {optimized_episodes}å›åˆÃ—{optimized_steps}æ­¥")
            print(f"ğŸ’¡ èŠ‚çœå†…å­˜: é¢„è®¡å‡å°‘{((num_episodes*steps_per_episode) - (optimized_episodes*optimized_steps))/(num_episodes*steps_per_episode)*100:.1f}%çš„å†…å­˜ä½¿ç”¨")
        
        return optimized_episodes, optimized_steps
    
    def _check_memory_usage(self) -> bool:
        """ğŸ”§ V6 æ–°å¢ï¼šæ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¿…è¦æ—¶è§¦å‘åƒåœ¾å›æ”¶ï¼ˆä¸è‡ªå®šä¹‰PPOä¸€è‡´ï¼‰"""
        try:
            import psutil  # type: ignore
            import gc
            
            memory_info = psutil.virtual_memory()
            available_gb = memory_info.available / (1024**3)
            usage_percent = memory_info.percent
            
            # å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜æ—¶è§¦å‘åƒåœ¾å›æ”¶
            if usage_percent > 95:
                print(f"ğŸ§¹ å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({usage_percent:.1f}%)ï¼Œæ‰§è¡Œåƒåœ¾å›æ”¶...")
                gc.collect()
                tf.keras.backend.clear_session()  # æ¸…ç†TensorFlowä¼šè¯
                return False  # å»ºè®®æš‚åœè®­ç»ƒ
            elif usage_percent > 90:
                print(f"âš ï¸  å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ ({usage_percent:.1f}%)ï¼Œå»ºè®®æ³¨æ„")
                gc.collect()
                return True
            
            return True
        except ImportError:
            return True  # æ— æ³•æ£€æµ‹æ—¶å‡è®¾æ­£å¸¸
    
    def quick_kpi_evaluation(self, num_episodes: int = 3) -> Dict[str, float]:
        """ğŸ”§ å…³é”®ä¿®å¤ï¼šå¿«é€ŸKPIè¯„ä¼°ï¼Œä½¿ç”¨çœŸå®çš„è®­ç»ƒæ¨¡å‹è€Œééšæœºç­–ç•¥"""
        try:
            temp_env = RayWFactoryEnv()
            
            total_rewards = []
            makespans = []
            utilizations = []
            completed_parts_list = []
            tardiness_list = []
            
            for episode in range(num_episodes):
                observations, _ = temp_env.reset()
                episode_reward = 0
                step_count = 0
                
                # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„æ­¥æ•°é™åˆ¶ï¼Œæ·»åŠ å®‰å…¨æœºåˆ¶é˜²æ­¢å¡æ­»
                max_steps = 1200
                while step_count < max_steps:
                    actions = {}
                    
                    # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœè§‚æµ‹ä¸ºç©ºï¼Œè·³å‡ºå¾ªç¯
                    if not observations or len(observations) == 0:
                        print(f"âš ï¸  KPIè¯„ä¼°ä¸­è§‚æµ‹ä¸ºç©ºï¼Œè·³å‡ºå¾ªç¯ (æ­¥æ•°: {step_count})")
                        break
                    
                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨çœŸå®çš„è®­ç»ƒæ¨¡å‹è¿›è¡Œç¡®å®šæ€§æ¨ç†
                    for agent in temp_env.agents:
                        if agent in observations:
                            try:
                                # ä½¿ç”¨Rayç®—æ³•çš„compute_single_actionè¿›è¡Œæ¨ç†
                                action = self.algorithm.compute_single_action(
                                    observations[agent], 
                                    policy_id="shared_policy",
                                    explore=False  # ç¡®å®šæ€§ç­–ç•¥ï¼Œä¸æ¢ç´¢
                                )
                                actions[agent] = action
                            except Exception as e:
                                # å¦‚æœæ¨ç†å¤±è´¥ï¼Œä½¿ç”¨è´ªå¿ƒç­–ç•¥ï¼ˆé€‰æ‹©åŠ¨ä½œ0ï¼Œé€šå¸¸æ˜¯IDLEï¼‰
                                actions[agent] = 0
                    
                    try:
                        step_result = temp_env.step(actions)
                        if len(step_result) == 4:
                            # æ—§ç‰ˆAPIï¼šobs, rewards, dones, infos
                            observations, rewards, dones, infos = step_result
                            done = dones.get("__all__", False)
                        else:
                            # æ–°ç‰ˆAPIï¼šobs, rewards, terminations, truncations, infos
                            observations, rewards, terminations, truncations, infos = step_result
                            done = terminations.get("__all__", False) or truncations.get("__all__", False)
                        
                        episode_reward += sum(rewards.values())
                        step_count += 1
                        
                        if done:
                            break
                            
                    except Exception as e:
                        print(f"âš ï¸  KPIè¯„ä¼°ä¸­ç¯å¢ƒæ­¥è¿›å‡ºé”™: {e}")
                        break
                
                # è·å–æœ€ç»ˆç»Ÿè®¡
                final_stats = temp_env.base_env.pz_env.sim.get_final_stats()
                total_rewards.append(episode_reward)
                makespans.append(final_stats.get('makespan', 0))
                utilizations.append(final_stats.get('mean_utilization', 0))
                completed_parts_list.append(final_stats.get('total_parts', 0))
                tardiness_list.append(final_stats.get('total_tardiness', 0))
        
            temp_env.close()
            
            return {
                'mean_reward': np.mean(total_rewards),
                'mean_makespan': np.mean(makespans),
                'mean_utilization': np.mean(utilizations),
                'mean_completed_parts': np.mean(completed_parts_list),
                'mean_tardiness': np.mean(tardiness_list)
            }
            
        except Exception as e:
            print(f"âš ï¸  KPIè¯„ä¼°å‡ºé”™: {e}")
            # è¿”å›é»˜è®¤å€¼é¿å…è®­ç»ƒä¸­æ–­
            return {
                'mean_reward': 0.0,
                'mean_makespan': 600.0,
                'mean_utilization': 0.0,
                'mean_completed_parts': 0.0,
                'mean_tardiness': 600.0
            }
    
    def simple_evaluation(self, num_episodes: int = 5) -> Dict[str, float]:
        """ğŸ”§ å…³é”®ä¿®å¤ï¼šç®€å•è¯„ä¼°ï¼Œä½¿ç”¨çœŸå®çš„è®­ç»ƒæ¨¡å‹è€Œééšæœºç­–ç•¥"""
        temp_env = RayWFactoryEnv()
        
        total_rewards = []
        total_steps = []
        makespans = []
        completed_parts = []
        utilizations = []
        tardiness_list = []
        
        for episode in range(num_episodes):
            observations, _ = temp_env.reset()
            episode_reward = 0
            step_count = 0
            
            max_steps = 1200
            while step_count < max_steps:
                actions = {}
                
                # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœè§‚æµ‹ä¸ºç©ºï¼Œè·³å‡ºå¾ªç¯
                if not observations or len(observations) == 0:
                    print(f"âš ï¸  ç®€å•è¯„ä¼°ä¸­è§‚æµ‹ä¸ºç©ºï¼Œè·³å‡ºå¾ªç¯ (æ­¥æ•°: {step_count})")
                    break
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨çœŸå®çš„è®­ç»ƒæ¨¡å‹è¿›è¡Œç¡®å®šæ€§æ¨ç†
                for agent in temp_env.agents:
                    if agent in observations:
                        try:
                            # ä½¿ç”¨Rayç®—æ³•çš„compute_single_actionè¿›è¡Œæ¨ç†
                            action = self.algorithm.compute_single_action(
                                observations[agent], 
                                policy_id="shared_policy",
                                explore=False  # ç¡®å®šæ€§ç­–ç•¥ï¼Œä¸æ¢ç´¢
                            )
                            actions[agent] = action
                        except Exception as e:
                            # å¦‚æœæ¨ç†å¤±è´¥ï¼Œä½¿ç”¨è´ªå¿ƒç­–ç•¥ï¼ˆé€‰æ‹©åŠ¨ä½œ0ï¼Œé€šå¸¸æ˜¯IDLEï¼‰
                            actions[agent] = 0
                
                try:
                    step_result = temp_env.step(actions)
                    if len(step_result) == 4:
                        # æ—§ç‰ˆAPIï¼šobs, rewards, dones, infos
                        observations, rewards, dones, infos = step_result
                        done = dones.get("__all__", False)
                    else:
                        # æ–°ç‰ˆAPIï¼šobs, rewards, terminations, truncations, infos
                        observations, rewards, terminations, truncations, infos = step_result
                        done = terminations.get("__all__", False) or truncations.get("__all__", False)
                    
                    episode_reward += sum(rewards.values())
                    step_count += 1
                    
                    if done:
                        break
                        
                except Exception as e:
                    print(f"âš ï¸  ç®€å•è¯„ä¼°ä¸­ç¯å¢ƒæ­¥è¿›å‡ºé”™: {e}")
                    break
            
            # ğŸ”§ ä¿®å¤ï¼šè·å–å®Œæ•´çš„ä¸šåŠ¡æŒ‡æ ‡
            final_stats = temp_env.base_env.pz_env.sim.get_final_stats()
            total_rewards.append(episode_reward)
            total_steps.append(step_count)
            makespans.append(final_stats.get('makespan', 0))
            completed_parts.append(final_stats.get('total_parts', 0))
            utilizations.append(final_stats.get('mean_utilization', 0))
            tardiness_list.append(final_stats.get('total_tardiness', 0))
        
        temp_env.close()
        
        return {
            'mean_reward': np.mean(total_rewards),
            'std_reward': np.std(total_rewards),
            'mean_steps': np.mean(total_steps),
            'mean_makespan': np.mean(makespans),
            'mean_completed_parts': np.mean(completed_parts),
            'mean_utilization': np.mean(utilizations),
            'mean_tardiness': np.mean(tardiness_list)
        }
    
    def train(self, num_episodes: int = 100, steps_per_episode: int = 200, 
              eval_frequency: int = 20):
        """ğŸ”§ V5 å¢å¼ºç‰ˆè®­ç»ƒä¸»å¾ªç¯ - è¯¦ç»†æ—¥å¿—å’ŒKPIç›‘æ§ï¼ˆä¸è‡ªå®šä¹‰PPOå®Œå…¨ä¸€è‡´ï¼‰"""
        print(f"ğŸš€ å¼€å§‹Ray PPOè®­ç»ƒ (V12 ç³»ç»Ÿä¼˜åŒ–ç‰ˆ)")
        print(f"ğŸ“Š è®­ç»ƒå‚æ•°: {self.optimized_episodes}å›åˆ, æ¯å›åˆ{self.optimized_steps}æ­¥")
        print(f"ğŸ’» ç³»ç»Ÿé…ç½®: {self.system_info['memory_gb']:.1f}GBå†…å­˜, GPU={'âœ…' if self.system_info['gpu_available'] else 'âŒ'}")
        print("=" * 80)
        
        if not validate_config():
            print("âŒ é…ç½®éªŒè¯å¤±è´¥")
            return
        
        # è®­ç»ƒå¼€å§‹æ—¶é—´è®°å½•
        training_start_time = time.time()
        training_start_datetime = datetime.now()
        print(f"ğŸ• è®­ç»ƒå¼€å§‹æ—¶é—´: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        
        self.best_reward = float('-inf')
        self.best_makespan = float('inf')
        
        try:
            for episode in range(self.optimized_episodes):
                iteration_start_time = time.time()
                
                # Rayè®­ç»ƒä¸€ä¸ªè¿­ä»£
                collect_start_time = time.time()
                
                # ğŸ”§ å®‰å…¨çš„è®­ç»ƒæ›´æ–°ï¼ˆåŒ…å«å†…å­˜æ£€æŸ¥ï¼‰
                if not self._check_memory_usage():
                    print("âš ï¸  å†…å­˜ä¸è¶³ï¼Œè·³è¿‡æœ¬è½®è®­ç»ƒ")
                    continue
                
                result = self.algorithm.train()
                collect_duration = time.time() - collect_start_time
                
                # æå–è®­ç»ƒæŒ‡æ ‡
                # ğŸ”§ V17 ä¿®å¤ï¼šé’ˆå¯¹Ray 2.48çš„æŒ‡æ ‡æå–
                # å°è¯•å¤šä¸ªå¯èƒ½çš„å¥–åŠ±å­—æ®µ
                episode_reward = (result.get('episode_reward_mean') or 
                                result.get('sampler_results', {}).get('episode_reward_mean') or 
                                result.get('env_runners', {}).get('episode_reward_mean') or
                                result.get('env_runners', {}).get('sampler_results', {}).get('episode_reward_mean') or
                                result.get('training_iteration_reward') or
                                result.get('hist_stats', {}).get('episode_reward') or 0)
                
                # æå–æŸå¤±ä¿¡æ¯ï¼šRay 2.48ä¸­æŸå¤±åœ¨learner_statsä¸­
                info = result.get('info', {})
                learner_info = info.get('learner', {})
                
                # æ‰¾åˆ°ç­–ç•¥çš„learner_stats
                policy_stats = None
                if 'shared_policy' in learner_info:
                    policy_stats = learner_info['shared_policy'].get('learner_stats', {})
                elif 'default_policy' in learner_info:
                    policy_stats = learner_info['default_policy'].get('learner_stats', {})
                
                # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•ç›´æ¥è®¿é—®
                if not policy_stats:
                    policy_stats = info.get('shared_policy', {}).get('learner_stats', {})
                
                losses = {
                    'actor_loss': (policy_stats.get('policy_loss') or 
                                 policy_stats.get('total_loss') or 
                                 policy_stats.get('actor_loss') or 0),
                    'critic_loss': (policy_stats.get('vf_loss') or 
                                  policy_stats.get('value_loss') or 
                                  policy_stats.get('critic_loss') or 0),
                    'entropy': (policy_stats.get('entropy') or 
                              policy_stats.get('policy_entropy') or 0)
                }
                
                # ğŸ”§ å¢åŠ è¯Šæ–­ä¿¡æ¯ï¼šå¦‚æœå‰å‡ è½®æŒ‡æ ‡å¼‚å¸¸ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
                if episode < 3 and (episode_reward == 0 or losses['actor_loss'] == 0):
                    print(f"ğŸ” ç¬¬{episode+1}è½®æŒ‡æ ‡è¯Šæ–­:")
                    print(f"   episode_reward: {episode_reward}")
                    print(f"   policy_statså¯ç”¨çš„é”®: {list(policy_stats.keys()) if policy_stats else 'None'}")
                    if policy_stats:
                        for key, value in policy_stats.items():
                            if 'loss' in key.lower() or 'reward' in key.lower():
                                print(f"   {key}: {value}")
                    print()
                


                # ğŸ”§ V17 å…³é”®ä¿®å¤ï¼šä»Rayçš„resultä¸­è·å–çœŸå®çš„æ—¶é—´ç»Ÿè®¡
                iteration_duration = result.get('time_total_s', time.time() - iteration_start_time)
                timers = result.get('timers', {})
                collect_duration = timers.get('sample_time_ms', 0) / 1000.0
                update_duration = timers.get('learn_time_ms', 0) / 1000.0
                # å¦‚æœ`learn_time_ms`ä¸å¯ç”¨ï¼ˆä¾‹å¦‚åœ¨æŸäº›Rayç‰ˆæœ¬ï¼‰ï¼Œåˆ™è¿›è¡Œä¼°ç®—
                if update_duration == 0 and iteration_duration > collect_duration:
                    update_duration = iteration_duration - collect_duration
                
                # è®°å½•ç»Ÿè®¡
                iteration_end_time = time.time()
                self.iteration_times.append(iteration_duration)
                self.episode_rewards.append(episode_reward)
                self.training_losses.append(losses)
                
                # ğŸ”§ V12 TensorBoardæ—¥å¿—è®°å½•
                if self.train_writer is not None:
                    with self.train_writer.as_default():
                        tf.summary.scalar('Training/Episode_Reward', episode_reward, step=episode)
                        tf.summary.scalar('Training/Actor_Loss', losses['actor_loss'], step=episode)
                        tf.summary.scalar('Training/Critic_Loss', losses['critic_loss'], step=episode)
                        tf.summary.scalar('Training/Entropy', losses['entropy'], step=episode)
                        tf.summary.scalar('Performance/Iteration_Duration', iteration_duration, step=episode)
                        tf.summary.scalar('Performance/CPU_Collection_Time', collect_duration, step=episode)
                        tf.summary.scalar('Performance/GPU_Update_Time', update_duration, step=episode)
                        self.train_writer.flush()
                
                # ğŸ”§ ä¿®å¤ï¼šå‡å°‘KPIè¯„ä¼°é¢‘ç‡ï¼Œé¿å…Rayæ¨ç†è°ƒç”¨è¿‡äºé¢‘ç¹
                if (episode + 1) % 5 == 0 or episode == 0:  # æ¯5è½®è¯„ä¼°ä¸€æ¬¡ï¼Œç¬¬ä¸€è½®ä¹Ÿè¯„ä¼°
                    kpi_results = self.quick_kpi_evaluation(num_episodes=1)  # å‡å°‘è¯„ä¼°episodeæ•°
                    self.kpi_history.append(kpi_results)
                else:
                    # éè¯„ä¼°è½®æ¬¡ï¼Œä½¿ç”¨ä¸Šä¸€æ¬¡çš„KPIç»“æœ
                    kpi_results = self.kpi_history[-1] if self.kpi_history else {
                        'mean_reward': 0.0, 'mean_makespan': 600.0, 'mean_utilization': 0.0, 
                        'mean_completed_parts': 0.0, 'mean_tardiness': 600.0
                    }
                
                # ğŸ”§ V12 TensorBoard KPIè®°å½•
                if self.train_writer is not None:
                    with self.train_writer.as_default():
                        tf.summary.scalar('KPI/Makespan', kpi_results['mean_makespan'], step=episode)
                        tf.summary.scalar('KPI/Completed_Parts', kpi_results['mean_completed_parts'], step=episode)
                        tf.summary.scalar('KPI/Utilization', kpi_results['mean_utilization'], step=episode)
                        tf.summary.scalar('KPI/Tardiness', kpi_results['mean_tardiness'], step=episode)
                        self.train_writer.flush()

                # ------------------- ç»Ÿä¸€æ—¥å¿—è¾“å‡ºå¼€å§‹ -------------------
                
                # å‡†å¤‡è¯„åˆ†å’Œæ¨¡å‹æ›´æ–°é€»è¾‘
                makespan = kpi_results['mean_makespan']
                completed_parts = kpi_results['mean_completed_parts']
                utilization = kpi_results['mean_utilization']
                tardiness = kpi_results['mean_tardiness']
                
                makespan_score = max(0, 1 - makespan / 600)
                utilization_score = utilization
                tardiness_score = max(0, 1 - tardiness / 1000)
                completion_score = completed_parts / 33
                
                current_score = (
                    makespan_score * 0.3 +
                    utilization_score * 0.2 +
                    tardiness_score * 0.2 +
                    completion_score * 0.3
                )
                
                if not hasattr(self, 'best_score'):
                    self.best_score = float('-inf')

                model_update_info = ""
                if current_score > self.best_score:
                    self.best_score = current_score
                    self.best_kpi = kpi_results.copy()
                    model_path = self.save_model(f"{self.models_dir}/best_ppo_model_{self.timestamp}")
                    if model_path:
                        model_update_info = f"âœ… æ¨¡å‹å·²æ›´æ–°: {model_path}"

                # æ ¼å¼åŒ–æ—¥å¿—è¡Œ
                line1 = f"ğŸ”‚ å›åˆ {episode + 1:3d}/{self.optimized_episodes} | å¥–åŠ±: {episode_reward:.1f} | ActoræŸå¤±: {losses['actor_loss']:.4f}| â±ï¸  æœ¬è½®ç”¨æ—¶: {iteration_duration:.1f}s (CPUé‡‡é›†: {collect_duration:.1f}s, GPUæ›´æ–°: {update_duration:.1f}s)"
                line2 = f"ğŸ“Š KPI - æ€»å®Œå·¥æ—¶é—´: {makespan:.1f}min |  è®¾å¤‡åˆ©ç”¨ç‡: {utilization:.1%} | å»¶æœŸæ—¶é—´: {tardiness:.1f}min | å®Œæˆé›¶ä»¶æ•°: {completed_parts:.0f}/33 |"
                
                line3_score = f"ğŸš¥ å›åˆè¯„åˆ†: {current_score:.3f} (æœ€ä½³: {self.best_score:.3f})"
                line3 = f"{line3_score}{model_update_info}" if model_update_info else line3_score

                avg_time = np.mean(self.iteration_times)
                remaining_episodes = self.optimized_episodes - (episode + 1)
                estimated_remaining = remaining_episodes * avg_time
                progress_percent = ((episode + 1) / self.optimized_episodes) * 100
                finish_str = ""
                if remaining_episodes > 0:
                    finish_time = time.time() + estimated_remaining
                    finish_str = time.strftime('%H:%M:%S', time.localtime(finish_time))
                line4 = f"ğŸ”® å½“å‰è®­ç»ƒè¿›åº¦: {progress_percent:.1f}% | é¢„è®¡å‰©ä½™æ—¶é—´: {estimated_remaining/60:.1f}min | å®Œæˆæ—¶é—´: {finish_str}"

                # æ‰“å°æ—¥å¿—
                print(line1)
                print(line2)
                print(line3)
                print(line4)
                print() # æ¯ä¸ªå›åˆåæ·»åŠ ä¸€ä¸ªç©ºè¡Œ
                
                # ------------------- ç»Ÿä¸€æ—¥å¿—è¾“å‡ºç»“æŸ -------------------
            
            # ğŸ”§ ä¿®å¤ç‰ˆï¼šç®€åŒ–çš„è®­ç»ƒå®Œæˆç»Ÿè®¡
            training_end_time = time.time()
            training_end_datetime = datetime.now()
            total_training_time = training_end_time - training_start_time
            
            print("\n" + "=" * 80)
            print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
            print(f"ğŸ• è®­ç»ƒå¼€å§‹: {training_start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ğŸ è®­ç»ƒç»“æŸ: {training_end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f}åˆ†é’Ÿ ({total_training_time:.1f}ç§’)")
            
            # è®­ç»ƒæ•ˆç‡ç»Ÿè®¡
            if self.iteration_times:
                avg_iteration_time = np.mean(self.iteration_times)
                print(f"âš¡ å¹³å‡æ¯è½®: {avg_iteration_time:.1f}s | è®­ç»ƒæ•ˆç‡: {len(self.iteration_times)/total_training_time*60:.1f}è½®/åˆ†é’Ÿ")
            
            # ğŸ”§ ä¿®å¤ï¼šæœ€ç»ˆè¯„ä¼°ï¼ˆä½¿ç”¨å¤šå›åˆè·å–ç¨³å®šç»“æœï¼‰
            print(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½è¯„ä¼° (10ä¸ªè¯„ä¼°episode):")
            final_eval = self.simple_evaluation(num_episodes=10)
            
            print(f"   å¹³å‡å¥–åŠ±: {final_eval['mean_reward']:.1f} Â± {final_eval['std_reward']:.1f}")
            print(f"   å¹³å‡æ€»å®Œå·¥æ—¶é—´: {final_eval['mean_makespan']:.1f} åˆ†é’Ÿ")
            print(f"   å¹³å‡å®Œæˆé›¶ä»¶: {final_eval['mean_completed_parts']:.1f}/33 ({final_eval['mean_completed_parts']/33*100:.1f}%)")
            print(f"   å¹³å‡è®¾å¤‡åˆ©ç”¨ç‡: {final_eval['mean_utilization']:.1%}")
            print(f"   å¹³å‡å»¶æœŸæ—¶é—´: {final_eval['mean_tardiness']:.1f} åˆ†é’Ÿ")
            
            # KPIæ”¹è¿›è¶‹åŠ¿ï¼ˆå¦‚æœæœ‰å†å²æ•°æ®ï¼‰
            if len(self.kpi_history) >= 2:
                initial = self.kpi_history[0]
                final_kpi = self.kpi_history[-1]
                
                print(f"\nğŸ“ˆ è®­ç»ƒæ”¹è¿›è¶‹åŠ¿:")
                if initial['mean_makespan'] > 0 and final_kpi['mean_makespan'] > 0:
                    makespan_change = ((initial['mean_makespan'] - final_kpi['mean_makespan']) / initial['mean_makespan']) * 100
                    print(f"   æ€»å®Œå·¥æ—¶é—´: {initial['mean_makespan']:.1f}â†’{final_kpi['mean_makespan']:.1f}min ({makespan_change:+.1f}%)")
                
                util_change = (final_kpi['mean_utilization'] - initial['mean_utilization']) * 100
                print(f"   è®¾å¤‡åˆ©ç”¨ç‡: {initial['mean_utilization']:.1%}â†’{final_kpi['mean_utilization']:.1%} ({util_change:+.1f}%)")
                
                parts_change = final_kpi['mean_completed_parts'] - initial['mean_completed_parts']
                print(f"   å®Œæˆé›¶ä»¶æ•°: {initial['mean_completed_parts']:.1f}â†’{final_kpi['mean_completed_parts']:.1f} ({parts_change:+.1f})")
                
                tardiness_change = final_kpi['mean_tardiness'] - initial['mean_tardiness']
                print(f"   å»¶æœŸæ—¶é—´: {initial['mean_tardiness']:.1f}â†’{final_kpi['mean_tardiness']:.1f}min ({tardiness_change:+.1f})")
                
                # ğŸ”§ V12 æ–°å¢ï¼šæ˜¾ç¤ºæœ€ä½³æ¨¡å‹ä¿¡æ¯
                if hasattr(self, 'best_kpi') and self.best_kpi:
                    print(f"\nğŸ† è®­ç»ƒæœŸé—´æœ€ä½³æ¨¡å‹ (ç¬¬{self.kpi_history.index(self.best_kpi)+1}è½®):")
                    print(f"   ç»¼åˆè¯„åˆ†: {self.best_score:.3f}")
                    print(f"   æ€»å®Œå·¥æ—¶é—´: {self.best_kpi['mean_makespan']:.1f}min")
                    print(f"   è®¾å¤‡åˆ©ç”¨ç‡: {self.best_kpi['mean_utilization']:.1%}")
                    print(f"   å»¶æœŸæ—¶é—´: {self.best_kpi['mean_tardiness']:.1f}min")
                    print(f"   å®Œæˆç‡: {self.best_kpi['mean_completed_parts']:.0f}/33 ({self.best_kpi['mean_completed_parts']/33*100:.1f}%)")
            
            # ğŸ”§ ä¿®å¤ï¼šä¸ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼Œåªä¿ç•™æœ€ä½³æ¨¡å‹
            # self.save_model(f"{self.models_dir}/final_ppo_model_{self.timestamp}")  # å·²ç¦ç”¨
            
            return {
                'training_time': total_training_time,
                'final_eval': final_eval,
                'kpi_history': self.kpi_history,
                'iteration_times': self.iteration_times
            }
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None
        
        finally:
            # æ¸…ç†Rayèµ„æº
            if hasattr(self, 'algorithm'):
                self.algorithm.stop()
    
    def save_model(self, filepath: str) -> str:
        """ä¿å­˜æ¨¡å‹å¹¶è¿”å›è·¯å¾„"""
        try:
            # Rayæ¨¡å‹ä¿å­˜
            checkpoint_path = self.algorithm.save(filepath)
            # ğŸ”§ ç²¾ç®€è¾“å‡ºï¼šåªæ˜¾ç¤ºè·¯å¾„ï¼Œä¸æ˜¾ç¤ºå¤æ‚çš„TrainingResultå¯¹è±¡
            saved_path = ""
            if hasattr(checkpoint_path, 'checkpoint') and hasattr(checkpoint_path.checkpoint, 'path'):
                saved_path = checkpoint_path.checkpoint.path
            elif hasattr(checkpoint_path, 'path'):
                saved_path = checkpoint_path.path
            else:
                saved_path = filepath
            
            return saved_path
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return ""

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    # æ‰“å°æ¬¢è¿ä¿¡æ¯å’Œç‰ˆæœ¬è¯´æ˜
    print("ğŸ­ Wå·¥å‚è®¢å•æ€ç»´é©å‘½Ray PPOè®­ç»ƒç³»ç»Ÿ V17 (è®­ç»ƒé€»è¾‘å½»åº•ä¿®å¤ç‰ˆ)")
    print("ğŸ¯ V17 å½»åº•ä¿®å¤: ä¿®æ­£APIå‚æ•°ã€æ—¶é—´ç»Ÿè®¡å’ŒæŒ‡æ ‡æå–ï¼Œè§£å†³å¥–åŠ±å’ŒæŸå¤±æ’ä¸º0çš„é—®é¢˜")
    print("ğŸš€ V17æ€§èƒ½é©å‘½: æ­£ç¡®çš„CPU/GPUæ—¶é—´åˆ†é…ï¼Œç¡®ä¿è®­ç»ƒæ•ˆç‡")
    print("ğŸ”§ æ ¸å¿ƒä¼˜åŒ–: å®Œå…¨å¯¹é½è‡ªå®šä¹‰PPOçš„é…ç½®ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒ")
    print("ğŸ’¾ å®‰å…¨ç‰¹æ€§: è‡ªåŠ¨å†…å­˜ç›‘æ§ + åƒåœ¾å›æ”¶ + æ£€æŸ¥ç‚¹ä¿å­˜ + åŠ¨æ€ç½‘ç»œè°ƒæ•´")
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    if not os.path.exists("logs"):
        os.makedirs("logs")
    
    # è®¾ç½®éšæœºç§å­
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    tf.random.set_seed(RANDOM_SEED)
    
    try:
        # ğŸ”§ V13 ä¿®å¤ç‰ˆï¼šä¸è‡ªå®šä¹‰PPOå®Œå…¨ä¸€è‡´çš„è®­ç»ƒå‚æ•°
        num_episodes = 500   # ğŸ”§ å¯¹é½è‡ªå®šä¹‰PPOçš„è®­ç»ƒè½®æ•°
        steps_per_episode = 1500  # ğŸ”§ å¯¹é½è‡ªå®šä¹‰PPOçš„episodeé•¿åº¦  
        
        trainer = RayPPOTrainer(
            initial_lr=2e-4,  # ğŸ”§ ä¿®å¤ï¼šå¯¹é½è‡ªå®šä¹‰PPOçš„å­¦ä¹ ç‡
            total_train_episodes=num_episodes,
            steps_per_episode=steps_per_episode
        )
        
        # å¼€å§‹è®­ç»ƒï¼ˆç³»ç»Ÿä¼šè‡ªåŠ¨æ ¹æ®èµ„æºè°ƒæ•´å‚æ•°ï¼‰
        results = trainer.train(
            num_episodes=num_episodes,
            steps_per_episode=steps_per_episode,
            eval_frequency=20       # è¯„ä¼°é¢‘ç‡
        )
        
        if results:
            print("\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        else:
            print("\nâŒ è®­ç»ƒå¤±è´¥")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # å…³é—­Ray
        if ray.is_initialized():
            ray.shutdown()

if __name__ == "__main__":
    # ğŸ”§ V10 å…³é”®ä¿®å¤: è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•ä¸º'spawn'ï¼Œä¸è‡ªå®šä¹‰PPOä¿æŒä¸€è‡´
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    main()
