图2-1 Entity Hierarchy and Flow in W Factory
flowchart LR
  %% Fig 2-1: Entity hierarchy and flow (English labels only)
  O["Order j\n(quantity, due_date, priority, arrival_time)"] --> P["Parts p in order j"]
  P --> R["Route\n(operations / steps)"]
  R --> W1["Workstation i"]
  W1 --> M["Parallel Machines (K_i)"]
  M --> Q["Queue / Buffer"]
  Q -->|dispatch| M
  M -->|process| W2["Next Workstation"]

图2-2 Decision Points Embedded in DES
sequenceDiagram
  %% Fig 2-2: Decision points embedded in DES (English labels only)
  participant DES as DES Engine (continuous time τ)
  participant Agent as Dispatch Policy (step t)
  participant WS as Workstations / Queues

  DES->>WS: Process jobs until next event
  WS-->>DES: Event occurs (machine idle / job arrival / breakdown)
  DES->>Agent: Request decision at step t (observe state)
  Agent->>DES: Action a(t) (assign candidates or idle)
  DES->>WS: Apply dispatching decisions
  DES->>DES: Advance to next event (τ jumps)
  Note over DES,Agent: Episode ends -> compute KPI & Score

图2-3 DES Event Calendar and Decision Insertion Loop
flowchart TD
  %% Fig 2-3: DES event calendar and decision insertion loop (English labels only)

  A["Initialize system<br>τ = 0, event calendar"] --> B["Pop next event<br>advance τ"]
  B --> C["Update queues/resources<br>job arrival, completion,<br>breakdown, repair"]
  C --> D{"Decision condition met?<br>any idle machine & non-empty queue"}
  D -- "No" --> B
  D -- "Yes" --> E["Request policy action<br>increase decision step t"]
  E --> F["Dispatch selected candidates<br>start processing"]
  F --> G["Schedule future events<br>completion events, etc."]
  G --> H{"Episode ends?<br>τ reaches horizon or all done"}
  H -- "No" --> B
  H -- "Yes" --> I["Compute KPI<br>makespan, tardiness,<br>utilization, completion rate"]

图2-4 RL Loop Mapped to W Factory Scheduling
flowchart LR
  %% Fig 2-4: RL loop mapping (English labels only)

  S["State (s<sub>t</sub>)<br>queues, machines, due slack,<br>arrivals, failures"] --> A["Action (a<sub>t</sub>)<br>dispatch candidates / idle"]
  A --> E["Environment Transition<br>DES advances time, executes jobs"]
  E --> R["Reward (r<sub>t</sub>)<br>progress, tardiness risk,<br>utilization, stability"]
  R --> S2["Next State (s<sub>t+1</sub>)"]
  S2 -->|iterate| S

图2-5 Markov Game Interaction
flowchart TB
  %% Fig 2-5: Markov game interaction (English labels only)

  S["Global state (s<sub>t</sub>)<br>DES snapshot"] --> O1["Local obs (o<sub>t</sub><sup>1</sup>)"]
  S --> O2["Local obs (o<sub>t</sub><sup>2</sup>)"]
  S --> ON["Local obs (o<sub>t</sub><sup>N</sup>)"]

  O1 --> A1["Agent 1 action (a<sub>t</sub><sup>1</sup>)"]
  O2 --> A2["Agent 2 action (a<sub>t</sub><sup>2</sup>)"]
  ON --> AN["Agent N action (a<sub>t</sub><sup>N</sup>)"]

  A1 --> JA["Joint action (a<sub>t</sub>)"]
  A2 --> JA
  AN --> JA

  JA --> T["DES transition<br>s<sub>t+1</sub> ~ P(.|s<sub>t</sub>,a<sub>t</sub>)"]
  T --> S2["Next global state (s<sub>t+1</sub>)"]
  S2 --> S

















图3-4 观测向量构成与信息来源

flowchart TB
    Q[Queue of station i] --> QS[Queue summary features 30D]
    Q --> CS[Candidate set build]
    CS --> CF[Candidate features 90D]

    S[Station status] --> SF[Self features 8D]
    G[Global stats] --> GF[Global features 4D]

    SF --> O[Observation vector 132D]
    GF --> O
    QS --> O
    CF --> O

图3-5 多头并行分配动作示意

flowchart LR
    O[Observation 132D] --> P[Policy network]
    P --> H1[Head 1: choose action]
    P --> H2[Head 2: choose action]
    H1 --> A1[Action for machine 1]
    H2 --> A2[Action for machine 2]
    A1 --> EX[Execute assignments]
    A2 --> EX

图3-8 动作掩码与采样流程
flowchart TB
    A[Get observation] --> B[Build candidate set]
    B --> C[Compute action mask]
    C --> D[Policy outputs logits or probs]
    D --> E[Masking and renormalize]
    E --> F[Sample multi-head actions]
    F --> G[Execute in simulator]
    G --> H[Next state and reward]
图3-9 Reward computation pipeline
flowchart TB
    S[State at step t] --> A[Actions from agents]
    A --> X[Simulator step]
    X --> E[Events detection]
    E --> C1[Part completed?]
    E --> C2[Episode finished?]
    E --> D1[Compute tardiness for finished parts]
    E --> S1[Compute slack for WIP parts]
    E --> W1[Compute WIP level]
    E --> I1[Check idle and invalid actions]

    C1 --> R1[Completion reward]
    C2 --> R2[All-done bonus]
    D1 --> R3[Due-date penalty]
    S1 --> R4[Slack shaping]
    W1 --> R5[WIP penalty]
    I1 --> R6[Idle/Invalid penalty]

    R1 --> SUM[Weighted sum reward]
    R2 --> SUM
    R3 --> SUM
    R4 --> SUM
    R5 --> SUM
    R6 --> SUM

    SUM --> OUT[Return rewards to RL algorithm]

图4-1 MAPPO training and execution pipeline（English, Mermaid）
flowchart TB
    A[Initialize policy and value networks] --> B[Collect trajectories with parallel envs]
    B --> C[Store transitions in buffer]
    C --> D[Compute returns and GAE advantages]
    D --> E[Update actor with PPO-Clip objective]
    D --> F[Update critic with value loss]
    E --> G[Sync new policy weights]
    F --> G
    G --> B

    subgraph Execution
        X[Local observation oi] --> Y[Shared actor policy]
        Y --> Z[Masked multi-head actions]
    end

图4-2（Mermaid）：Actor-Critic I/O & Action Sampling Pipeline

flowchart LR
  %% Fig 4-2: Actor-Critic and action sampling pipeline (English labels only)

  subgraph ENV["Environment"]
    O["Local Observation o<sub>i</sub>(t)<br>(dim=132)"]
    S["Global State s(t)"]
    ID["Agent ID One-Hot e<sub>i</sub>"]
  end

  subgraph ACTOR["Actor Network (Decentralized)"]
    B["Backbone MLP"]
    H1["Head 1 Softmax<br>(|A|=N+1)"]
    H2["Head 2 Softmax<br>(|A|=N+1)"]
  end

  subgraph MASK["Mask & Conflict Handling"]
    M["Action Mask m<sub>i,k</sub>(t)<br>(legal=1, illegal=0)"]
    MS["Masked Softmax<br>Normalize on legal actions"]
    SWR["Mutual Exclusion<br>Sampling w/o Replacement"]
    LP["Log-Prob Recording<br>(after mask & exclusion)"]
  end

  subgraph OUT["Output"]
    A1["Action a<sub>i,1</sub>(t)"]
    A2["Action a<sub>i,2</sub>(t)"]
    AV["Joint Action Vector<br>[a<sub>i,1</sub>(t), a<sub>i,2</sub>(t)]"]
  end

  subgraph CRITIC["Critic Network (Centralized, Agent-Conditioned)"]
    C0["Concat [s(t); e<sub>i</sub>]"]
    C1["Value MLP"]
    V["Value V(s(t), i)"]
  end

  O --> B
  B --> H1
  B --> H2

  H1 --> M
  H2 --> M
  M --> MS --> SWR --> A1
  SWR --> A2
  A1 --> AV
  A2 --> AV
  SWR --> LP

  S --> C0
  ID --> C0
  C0 --> C1 --> V

  图4-3（Mermaid）：Masked Softmax Mechanism
  flowchart LR
  %% Fig 4-3: Masked softmax mechanism (English labels only)

  Z["Raw Logits z<sub>i,k</sub><br>(size=N+1)"] --> M["Mask m<sub>i,k</sub><br>(size=N+1)<br>1=legal, 0=illegal"]
  M --> AZ["Apply Mask<br>set illegal to -INF"]
  AZ --> EX["Exp(.) on legal entries"]
  EX --> SUM["Sum over legal actions"]
  SUM --> PI["Masked Policy<br>p(a|o)=exp(z<sub>a</sub>)/sum<sub>legal</sub> exp(z)"]
  PI --> SA["Sample Action a<sub>i,k</sub>"]

图4-4 Two-Stage Curriculum Learning Overview（Mermaid）
---
config:
  layout: dagre
---
flowchart LR
 subgraph STAGE1["Stage 1: Foundation<br>(Stable Environment)"]
    direction TB
        S1["Initialize Stable Env"]
        COL1["Collect Trajectories"]
        UP1["PPO Update<br>(Actor/Critic)"]
        EVAL1["Evaluate Metrics<br>(score, completion, tardiness)"]
        GATE1{"Graduation Check<br>Stage 1?"}
  end
 subgraph STAGE2["Stage 2: Generalization<br>(Dynamic Disturbances)"]
    direction TB
        S2["Add Dynamic Disturbances"]
        COL2["Collect Trajectories<br>(with disturbances)"]
        UP2["PPO Update<br>(Actor/Critic)"]
        EVAL2["Evaluate Metrics<br>(robustness KPIs)"]
        GATE2{"Convergence Check<br>Stage 2?"}
  end
    S1 --> COL1
    COL1 --> UP1
    UP1 --> EVAL1
    EVAL1 --> GATE1
    S2 --> COL2
    COL2 --> UP2
    UP2 --> EVAL2
    EVAL2 --> GATE2
    START["Start Training"] --> STAGE1
    GATE2 -- Yes --> END["Save Final Policy<br>Stop Training"]
    GATE2 -- No --> COL2
    GATE1 -- Yes --> S2
    GATE1 -- No --> COL1

     START:::titleNode
     STAGE1:::stageSubgraph
     END:::titleNode
     STAGE2:::stageSubgraph
    classDef titleNode fill:#222,stroke:#fff,stroke-width:1,color:#fff
    classDef stageSubgraph fill:#333,stroke:#666,stroke-width:1
    style START fill:#FFFFFF,color:#000000,stroke:#000000
    style STAGE1 fill:#FFFFFF
    style END color:#000000,fill:transparent,stroke:#000000
    style STAGE2 fill:#FFFFFF

图4-5 Graduation Gate Based on Metrics（Mermaid）
flowchart LR
  %% Fig 4-5: Graduation gate based on metrics (English labels only)

  WIN["Evaluation Window\n(e.g., last M episodes)"] --> KPI["Compute KPIs\nscore, completion, tardiness"]
  KPI --> TH{"Meet Thresholds?"}

  TH -- "No" --> RESET["Reset Consistency Counter\nc = 0"]
  TH -- "Yes" --> INC["Increase Counter\nc = c + 1"]

  INC --> CHECK{"c >= C_req ?"}
  RESET --> WIN
  CHECK -- "No" --> WIN
  CHECK -- "Yes" --> SWITCH["Stage Switch\nor Training Stop"]

图4-6 Anchor Episode Rotation（Mermaid）
flowchart LR
  %% Fig 4-6: Anchor episode rotation (English labels only)

  subgraph STAGE2["Stage 2 Training Stream"]
    E1["Episode t"]
    E2["Episode t+1"]
    E3["Episode t+2"]
    EA["Anchor Episode<br>(baseline scenario)"]
    E4["Episode t+3"]
    E5["Episode t+4"]
  end

  E1 --> E2 --> E3 --> EA --> E4 --> E5

  subgraph MIX["Batch Mixing"]
    DYN["Dynamic Episodes<br>(with disturbances)"]
    ANC["Anchor Episodes<br>(baseline)"]
    BATCH["Training Batch<br>Mixed Distribution"]
  end

  E1 --> DYN
  E2 --> DYN
  E3 --> DYN
  EA --> ANC
  DYN --> BATCH
  ANC --> BATCH

  BATCH --> UPD["PPO Update<br>(stability-constrained)"]

图4-7 Normalization & Clipping in PPO Update
flowchart TD
  %% Fig 4-7: Normalization & clipping pipeline (English labels only)

  TRAJ["Collected Trajectories"] --> R["Rewards"]
  TRAJ --> OBS["Observations"]
  OBS --> NOBS["State Normalization"]
  R --> NR["Reward Normalization"]

  NR --> GAE["GAE Advantage Estimation"]
  GAE --> AN["Advantage Normalization"]

  AN --> RATIO["Policy Ratio r_t"]
  RATIO --> CLIP["PPO Clip\nlimit ratio change"]

  CLIP --> LOSS["Surrogate Loss\n+ Value Loss\n+ Entropy Term"]
  LOSS --> GC["Gradient Clipping\nlimit grad norm"]
  GC --> OPT["Optimizer Step\nupdate parameters"]

图4-8 Entropy Schedule Across Stages
  flowchart LR
  %% Fig 4-8: Entropy schedule across stages (English labels only)

  S1["Stage 1: Foundation"] --> H1["Higher Entropy Weight\n(Encourage Exploration)"]
  H1 --> D1["Decay Entropy\n(stabilize policy)"]

  D1 --> SW["Stage Switch"]

  SW --> S2["Stage 2: Generalization"] --> H2["Re-boost or Keep Entropy\n(adapt to disturbances)"]
  H2 --> D2["Decay Entropy Again\n(converge robust policy)"]

  D2 --> END["Final Policy\n(low variance behavior)"]

图4-9 Overall Training Loop
flowchart TD
  %% Fig 4-9: Overall Training Loop (English labels only)

  A["Initialize<br>Actor (theta), Critic (phi)<br>Normalization Stats"] --> B["Set Stage = Foundation"]
  B --> C{"Converged?"}

  C -- "No" --> D["Configure Environment<br>(Stage Settings)"]
  D --> E["Parallel Rollout<br>M envs, T steps"]
  E --> F["Masked Action Sampling<br>+ Mutual Exclusion<br>+ LogProb Recording"]
  F --> G["GAE Advantage Estimation<br>(using Critic)"]
  G --> H["PPO Update<br>E epochs, minibatches<br>Clip + Value Loss + Entropy"]
  H --> I["Stability Ops<br>Normalization + Gradient Clip"]
  I --> J["Evaluate Metrics<br>(score, completion, tardiness)"]
  J --> K{"Curriculum Gate<br>or Stop?"}

  K -- "Stay in Stage" --> C
  K -- "Switch to Generalization" --> L["Stage = Generalization<br>Enable Anchor Rotation"]
  L --> C
  K -- "Stop Training" --> M["Save Final Policy<br>(Actor for deployment)"]
  C -- "Yes" --> M

图5-3: KPI extraction and scoring pipeline
  flowchart LR
  %% Fig 5-3: KPI extraction and scoring pipeline (English labels only)

  A["Run One Episode<br>(policy or heuristic)"] --> B["Simulation Final Stats<br>(final_stats)"]
  B --> C["KPI Set<br>makespan, utilization,<br>total_tardiness, completed_parts"]
  C --> D["Episode Score<br>weighted aggregation"]
  D --> E["Training Gate<br>(graduation/convergence)"]
  C --> F["Logging & Parsing<br>(log -> table/plots)"]
  B --> G["Optional Gantt History<br>(for visualization)"]
  
图Fig 5-4: Baselines and ablations map
flowchart LR
  %% Fig 5-4: Baselines and ablations map (English labels only)

  M["Proposed Method<br>Two-Stage MAPPO"] --> B["Baselines<br>(Heuristics)"]
  B --> B1["FIFO"]
  B --> B2["SPT"]
  B --> B3["EDD"]

  M --> A["Ablations<br>(Module Removal)"]
  A --> A1["No Curriculum"]
  A --> A2["No Anchor"]
  A --> A3["No Mask"]
  A --> A4["No SWR"]