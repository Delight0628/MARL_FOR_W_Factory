"""
åŸºäºStable-Baselines3çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒè„šæœ¬
æ›¿ä»£Ray RLlibï¼Œè§£å†³Windowså…¼å®¹æ€§é—®é¢˜
ä½¿ç”¨SuperSuitåŒ…è£…å™¨å®ç°çœŸæ­£çš„MARL
"""

import os
import sys
import time
import json
import numpy as np
from typing import Dict, Any, List
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ ç¯å¢ƒè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)  # å›åˆ°MARL_FOR_W_Factoryç›®å½•
sys.path.append(parent_dir)

from environments.w_factory_env import make_parallel_env
from environments.w_factory_config import *

# å¯¼å…¥å¼ºåŒ–å­¦ä¹ åº“
try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
    from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
    from stable_baselines3.common.monitor import Monitor
    import supersuit as ss
    print("âœ“ Stable-Baselines3 å’Œ SuperSuit å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–åº“: {e}")
    print("è¯·å®‰è£…: pip install stable-baselines3[extra] supersuit")
    sys.exit(1)

class MARLTrainingCallback(BaseCallback):
    """MARLè®­ç»ƒå›è°ƒå‡½æ•°"""
    
    def __init__(self, eval_freq: int = 1000, verbose: int = 1):
        super().__init__(verbose)
        self.eval_freq = eval_freq
        self.episode_rewards = []
        self.episode_lengths = []
        
    def _on_step(self) -> bool:
        # è®°å½•è®­ç»ƒç»Ÿè®¡
        if len(self.model.ep_info_buffer) > 0:
            for info in self.model.ep_info_buffer:
                if 'r' in info:
                    self.episode_rewards.append(info['r'])
                if 'l' in info:
                    self.episode_lengths.append(info['l'])
        
        # å®šæœŸè¾“å‡ºè®­ç»ƒè¿›åº¦
        if self.num_timesteps % self.eval_freq == 0:
            if len(self.episode_rewards) > 0:
                recent_rewards = self.episode_rewards[-10:]
                avg_reward = np.mean(recent_rewards)
                print(f"æ­¥æ•°: {self.num_timesteps:8d} | "
                      f"å¹³å‡å¥–åŠ±: {avg_reward:8.2f} | "
                      f"å›åˆæ•°: {len(self.episode_rewards):4d}")
        
        return True

def create_env():
    """åˆ›å»ºå¹¶åŒ…è£…ç¯å¢ƒ"""
    print("åˆ›å»ºPettingZooç¯å¢ƒ...")
    
    # åˆ›å»ºåŸå§‹ç¯å¢ƒ
    env = make_parallel_env()
    
    # ä¿®å¤SuperSuitå…¼å®¹æ€§é—®é¢˜ï¼šæ·»åŠ render_modeå±æ€§
    if not hasattr(env, 'render_mode'):
        env.render_mode = None
    
    # ä½¿ç”¨SuperSuitåŒ…è£…å™¨è½¬æ¢ä¸ºå•æ™ºèƒ½ä½“ç¯å¢ƒ
    # è¿™æ˜¯å®ç°MARLçš„å…³é”®æ­¥éª¤
    env = ss.pettingzoo_env_to_vec_env_v1(env)
    env = ss.concat_vec_envs_v1(env, 1, num_cpus=1, base_class='stable_baselines3')
    
    print(f"âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    print(f"  è§‚æµ‹ç©ºé—´: {env.observation_space}")
    print(f"  åŠ¨ä½œç©ºé—´: {env.action_space}")
    
    return env

def train_sb3_marl(total_timesteps: int = 100000, learning_rate: float = 3e-4):
    """ä½¿ç”¨Stable-Baselines3è®­ç»ƒMARL"""
    
    print("=" * 60)
    print("Wå·¥å‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒ - Stable-Baselines3ç‰ˆæœ¬")
    print("=" * 60)
    print("æ¡†æ¶: Stable-Baselines3 + SuperSuit")
    print("ç®—æ³•: PPO (Proximal Policy Optimization)")
    print("å¤šæ™ºèƒ½ä½“: å‘é‡åŒ–ç¯å¢ƒMARL")
    print("=" * 60)
    
    # éªŒè¯é…ç½®
    if not validate_config():
        print("é…ç½®éªŒè¯å¤±è´¥")
        return None
    
    try:
        # åˆ›å»ºç¯å¢ƒ
        env = create_env()
        
        # åˆ›å»ºPPOæ¨¡å‹
        print("åˆ›å»ºPPOæ¨¡å‹...")
        model = PPO(
            "MlpPolicy",  # å¤šå±‚æ„ŸçŸ¥æœºç­–ç•¥
            env,
            learning_rate=learning_rate,
            n_steps=2048,  # æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ­¥æ•°
            batch_size=64,  # å°æ‰¹é‡å¤§å°
            n_epochs=10,    # æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒè½®æ•°
            gamma=0.99,     # æŠ˜æ‰£å› å­
            gae_lambda=0.95, # GAEå‚æ•°
            clip_range=0.2,  # PPOè£å‰ªå‚æ•°
            ent_coef=0.01,   # ç†µç³»æ•°
            vf_coef=0.5,     # ä»·å€¼å‡½æ•°ç³»æ•°
            max_grad_norm=0.5, # æ¢¯åº¦è£å‰ª
            verbose=1,
            device='cpu'  # ä½¿ç”¨CPUï¼Œé¿å…GPUå…¼å®¹æ€§é—®é¢˜
        )
        
        print("âœ“ PPOæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"  ç­–ç•¥ç½‘ç»œ: {model.policy}")
        print(f"  å­¦ä¹ ç‡: {learning_rate}")
        print(f"  æ€»è®­ç»ƒæ­¥æ•°: {total_timesteps}")
        
        # åˆ›å»ºå›è°ƒå‡½æ•°
        callback = MARLTrainingCallback(eval_freq=2000)
        
        # å¼€å§‹è®­ç»ƒ
        print("\nå¼€å§‹è®­ç»ƒ...")
        start_time = time.time()
        
        model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            progress_bar=True
        )
        
        training_time = time.time() - start_time
        
        print("\n" + "=" * 60)
        print("Stable-Baselines3 MARLè®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        print(f"è®­ç»ƒæ—¶é—´: {training_time/60:.2f} åˆ†é’Ÿ")
        print(f"æ€»æ­¥æ•°: {total_timesteps}")
        
        # å®‰å…¨åœ°è®¡ç®—å¹³å‡å¥–åŠ±
        if len(callback.episode_rewards) > 0:
            recent_rewards = callback.episode_rewards[-10:]
            if len(recent_rewards) > 0:
                avg_reward = np.mean(recent_rewards)
                print(f"å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
            else:
                print("å¹³å‡å¥–åŠ±: æ— æ•°æ®")
        else:
            print("å¹³å‡å¥–åŠ±: æ— æ•°æ®")
        
        # ä¿å­˜æ¨¡å‹
        os.makedirs("models", exist_ok=True)
        model_path = "models/sb3_marl_model"
        model.save(model_path)
        print(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜è®­ç»ƒç»Ÿè®¡
        training_stats = {
            "framework": "Stable-Baselines3",
            "algorithm": "PPO",
            "total_timesteps": total_timesteps,
            "training_time_minutes": training_time / 60,
            "episode_rewards": callback.episode_rewards,
            "episode_lengths": callback.episode_lengths,
            "final_avg_reward": float(np.mean(callback.episode_rewards[-10:])) if len(callback.episode_rewards) > 0 else 0.0,
            "agents": list(WORKSTATIONS.keys()),
            "config": {
                "learning_rate": learning_rate,
                "n_steps": 2048,
                "batch_size": 64,
                "n_epochs": 10
            }
        }
        
        os.makedirs("results", exist_ok=True)
        stats_file = f"results/sb3_training_stats_{int(time.time())}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(training_stats, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜: {stats_file}")
        
        return model, training_stats
        
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def evaluate_model(model, num_episodes: int = 10):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"\nè¯„ä¼°æ¨¡å‹ ({num_episodes} å›åˆ)...")
    
    try:
        env = create_env()
        
        eval_rewards = []
        eval_lengths = []
        
        for episode in range(num_episodes):
            obs = env.reset()
            episode_reward = 0
            episode_length = 0
            done = False
            
            while not done:
                # ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, done, info = env.step(action)
                
                # å¤„ç†å¥–åŠ±ï¼ˆå¯èƒ½æ˜¯æ•°ç»„ï¼‰
                if isinstance(reward, np.ndarray):
                    reward = float(reward.sum())
                elif isinstance(reward, (list, tuple)):
                    reward = float(sum(reward))
                else:
                    reward = float(reward)
                
                # å¤„ç†doneï¼ˆå¯èƒ½æ˜¯æ•°ç»„ï¼‰
                if isinstance(done, np.ndarray):
                    done = bool(done.any())
                elif isinstance(done, (list, tuple)):
                    done = bool(any(done))
                else:
                    done = bool(done)
                
                episode_reward += reward
                episode_length += 1
                
                if episode_length > 1000:  # é˜²æ­¢æ— é™å¾ªç¯
                    break
            
            eval_rewards.append(episode_reward)
            eval_lengths.append(episode_length)
            
            print(f"  å›åˆ {episode+1}: å¥–åŠ±={episode_reward:.2f}, é•¿åº¦={episode_length}")
        
        eval_results = {
            "mean_reward": float(np.mean(eval_rewards)),
            "std_reward": float(np.std(eval_rewards)),
            "mean_length": float(np.mean(eval_lengths)),
            "eval_rewards": eval_rewards,
            "eval_lengths": eval_lengths
        }
        
        print(f"\nè¯„ä¼°ç»“æœ:")
        print(f"  å¹³å‡å¥–åŠ±: {eval_results['mean_reward']:.2f} Â± {eval_results['std_reward']:.2f}")
        print(f"  å¹³å‡é•¿åº¦: {eval_results['mean_length']:.1f}")
        
        return eval_results
        
    except Exception as e:
        print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None

def compare_with_simple_baseline():
    """ä¸ç®€å•åŸºå‡†ç®—æ³•å¯¹æ¯”"""
    print("\nè¿è¡ŒåŸºå‡†ç®—æ³•å¯¹æ¯”...")
    
    try:
        # å°è¯•å¯¼å…¥å¹¶è¿è¡Œç®€å•è®­ç»ƒè„šæœ¬
        try:
            from simple_train import SimpleTrainer
            simple_trainer = SimpleTrainer()
            simple_results = simple_trainer.train(num_episodes=10)
            
            # æ£€æŸ¥ç»“æœæ ¼å¼
            if isinstance(simple_results, dict) and 'episode_rewards' in simple_results:
                print(f"ç®€å•åŸºå‡†ç®—æ³•:")
                print(f"  å¹³å‡å¥–åŠ±: {np.mean(simple_results['episode_rewards']):.2f}")
                print(f"  è®­ç»ƒæ—¶é—´: {simple_results.get('training_time', 0)/60:.2f} åˆ†é’Ÿ")
                return simple_results
            else:
                print("ç®€å•åŸºå‡†ç®—æ³•ç»“æœæ ¼å¼ä¸æ­£ç¡®")
                return None
                
        except ImportError:
            print("æœªæ‰¾åˆ°simple_trainæ¨¡å—ï¼Œè·³è¿‡åŸºå‡†å¯¹æ¯”")
            return None
            
    except Exception as e:
        print(f"åŸºå‡†å¯¹æ¯”å¤±è´¥: {e}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("Wå·¥å‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ")
    print("åŸºäºStable-Baselines3çš„MARLå®ç°")
    print("=" * 60)
    
    try:
        # è®­ç»ƒæ¨¡å‹
        model, training_stats = train_sb3_marl(
            total_timesteps=50000,  # é€‚ä¸­çš„è®­ç»ƒæ­¥æ•°
            learning_rate=3e-4
        )
        
        if model is not None:
            print("\nğŸ‰ SB3 MARLè®­ç»ƒæˆåŠŸå®Œæˆï¼")
            
            # è¯„ä¼°æ¨¡å‹
            eval_results = evaluate_model(model, num_episodes=5)
            
            # åŸºå‡†å¯¹æ¯”
            baseline_results = compare_with_simple_baseline()
            
            # æœ€ç»ˆæ€»ç»“
            print("\n" + "=" * 60)
            print("æœ€ç»ˆç»“æœæ€»ç»“")
            print("=" * 60)
            
            if eval_results:
                print(f"SB3 MARLå¹³å‡å¥–åŠ±: {eval_results['mean_reward']:.2f}")
            
            if baseline_results:
                baseline_avg = np.mean(baseline_results['episode_rewards'])
                print(f"ç®€å•åŸºå‡†å¹³å‡å¥–åŠ±: {baseline_avg:.2f}")
                
                if eval_results:
                    improvement = eval_results['mean_reward'] - baseline_avg
                    print(f"æ€§èƒ½æå‡: {improvement:.2f} ({improvement/baseline_avg*100:.1f}%)")
            
            print("\nâœ… è¿™æ˜¯çœŸæ­£çš„MARLè®­ç»ƒï¼")
            print("âœ… ä½¿ç”¨å·¥ä¸šçº§PPOç®—æ³•")
            print("âœ… å¤šæ™ºèƒ½ä½“ååŒå­¦ä¹ ")
            print("âœ… Windowså®Œå…¨å…¼å®¹")
            print("âœ… æ— Rayä¾èµ–é—®é¢˜")
            
        else:
            print("\nâŒ SB3è®­ç»ƒå¤±è´¥")
            
    except Exception as e:
        print(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 