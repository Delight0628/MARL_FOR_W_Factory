"""
ç®€åŒ–çš„SB3 MARLè®­ç»ƒè„šæœ¬
ä¸ä½¿ç”¨SuperSuitï¼Œç›´æ¥å®ç°å¤šæ™ºèƒ½ä½“è®­ç»ƒ
"""

import os
import sys
import time
import json
import numpy as np
from typing import Dict, Any, List
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ ç¯å¢ƒè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from environments.w_factory_env import make_parallel_env
from environments.w_factory_config import *

# å¯¼å…¥å¼ºåŒ–å­¦ä¹ åº“
try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.vec_env import DummyVecEnv
    from stable_baselines3.common.callbacks import BaseCallback
    import gymnasium as gym
    print("âœ“ Stable-Baselines3 å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–åº“: {e}")
    print("è¯·å®‰è£…: pip install stable-baselines3[extra]")
    sys.exit(1)

class SingleAgentWrapper(gym.Env):
    """å°†å¤šæ™ºèƒ½ä½“ç¯å¢ƒåŒ…è£…ä¸ºå•æ™ºèƒ½ä½“ç¯å¢ƒ"""
    
    def __init__(self, config=None):
        super().__init__()
        self.env = make_parallel_env(config)
        
        # å®šä¹‰è§‚æµ‹å’ŒåŠ¨ä½œç©ºé—´
        # è§‚æµ‹ç©ºé—´ï¼šæ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹æ‹¼æ¥
        single_obs_dim = 2  # æ¯ä¸ªæ™ºèƒ½ä½“çš„è§‚æµ‹ç»´åº¦
        total_obs_dim = len(self.env.possible_agents) * single_obs_dim
        self.observation_space = gym.spaces.Box(
            low=0.0, high=1.0, shape=(total_obs_dim,), dtype=np.float32
        )
        
        # åŠ¨ä½œç©ºé—´ï¼šæ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œæ‹¼æ¥
        total_action_dim = len(self.env.possible_agents)
        self.action_space = gym.spaces.MultiDiscrete([2] * total_action_dim)
        
        self.agents = self.env.possible_agents
        print(f"åŒ…è£…ç¯å¢ƒåˆ›å»ºæˆåŠŸ:")
        print(f"  æ™ºèƒ½ä½“æ•°é‡: {len(self.agents)}")
        print(f"  è§‚æµ‹ç©ºé—´: {self.observation_space}")
        print(f"  åŠ¨ä½œç©ºé—´: {self.action_space}")
    
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        obs, info = self.env.reset(seed=seed, options=options)
        
        # å°†å¤šæ™ºèƒ½ä½“è§‚æµ‹æ‹¼æ¥ä¸ºå•ä¸€è§‚æµ‹
        combined_obs = []
        for agent in self.agents:
            if agent in obs:
                combined_obs.extend(obs[agent])
            else:
                combined_obs.extend([0.0, 0.0])  # é»˜è®¤è§‚æµ‹
        
        return np.array(combined_obs, dtype=np.float32), {}
    
    def step(self, action):
        """æ‰§è¡Œä¸€æ­¥"""
        # å°†å•ä¸€åŠ¨ä½œåˆ†è§£ä¸ºå¤šæ™ºèƒ½ä½“åŠ¨ä½œ
        actions = {}
        for i, agent in enumerate(self.agents):
            if i < len(action):
                actions[agent] = int(action[i])
            else:
                actions[agent] = 0  # é»˜è®¤åŠ¨ä½œ
        
        # æ‰§è¡Œç¯å¢ƒæ­¥éª¤
        obs, rewards, terminations, truncations, infos = self.env.step(actions)
        
        # å¤„ç†è§‚æµ‹
        combined_obs = []
        for agent in self.agents:
            if agent in obs:
                combined_obs.extend(obs[agent])
            else:
                combined_obs.extend([0.0, 0.0])
        
        # å¤„ç†å¥–åŠ±ï¼ˆæ±‚å’Œï¼‰
        total_reward = sum(rewards.values()) if rewards else 0.0
        
        # å¤„ç†å®ŒæˆçŠ¶æ€
        done = any(terminations.values()) or any(truncations.values())
        
        # å¤„ç†ä¿¡æ¯
        info = {}
        if done and infos:
            # è·å–æœ€ç»ˆç»Ÿè®¡
            for agent_info in infos.values():
                if "final_stats" in agent_info:
                    info["final_stats"] = agent_info["final_stats"]
                    break
        
        return np.array(combined_obs, dtype=np.float32), total_reward, done, False, info

class MARLTrainingCallback(BaseCallback):
    """MARLè®­ç»ƒå›è°ƒå‡½æ•°"""
    
    def __init__(self, eval_freq: int = 1000, verbose: int = 1):
        super().__init__(verbose)
        self.eval_freq = eval_freq
        self.episode_rewards = []
        self.episode_lengths = []
        
    def _on_step(self) -> bool:
        # è®°å½•è®­ç»ƒç»Ÿè®¡
        if len(self.model.ep_info_buffer) > 0:
            for info in self.model.ep_info_buffer:
                if 'r' in info:
                    self.episode_rewards.append(info['r'])
                if 'l' in info:
                    self.episode_lengths.append(info['l'])
        
        # å®šæœŸè¾“å‡ºè®­ç»ƒè¿›åº¦
        if self.num_timesteps % self.eval_freq == 0:
            if len(self.episode_rewards) > 0:
                recent_rewards = self.episode_rewards[-10:]
                avg_reward = np.mean(recent_rewards)
                print(f"æ­¥æ•°: {self.num_timesteps:8d} | "
                      f"å¹³å‡å¥–åŠ±: {avg_reward:8.2f} | "
                      f"å›åˆæ•°: {len(self.episode_rewards):4d}")
        
        return True

def create_env():
    """åˆ›å»ºç¯å¢ƒ"""
    def _init():
        return SingleAgentWrapper()
    return _init

def train_simple_sb3_marl(total_timesteps: int = 50000, learning_rate: float = 3e-4):
    """ä½¿ç”¨ç®€åŒ–æ–¹æ³•è®­ç»ƒSB3 MARL"""
    
    print("=" * 60)
    print("Wå·¥å‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒ - ç®€åŒ–SB3ç‰ˆæœ¬")
    print("=" * 60)
    print("æ¡†æ¶: Stable-Baselines3 (æ— SuperSuit)")
    print("ç®—æ³•: PPO")
    print("å¤šæ™ºèƒ½ä½“: è”åˆåŠ¨ä½œç©ºé—´")
    print("=" * 60)
    
    # éªŒè¯é…ç½®
    if not validate_config():
        print("é…ç½®éªŒè¯å¤±è´¥")
        return None, None
    
    try:
        # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
        env = DummyVecEnv([create_env()])
        
        # åˆ›å»ºPPOæ¨¡å‹
        print("åˆ›å»ºPPOæ¨¡å‹...")
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=learning_rate,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            vf_coef=0.5,
            max_grad_norm=0.5,
            verbose=1,
            device='cpu'
        )
        
        print("âœ“ PPOæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"  å­¦ä¹ ç‡: {learning_rate}")
        print(f"  æ€»è®­ç»ƒæ­¥æ•°: {total_timesteps}")
        
        # åˆ›å»ºå›è°ƒå‡½æ•°
        callback = MARLTrainingCallback(eval_freq=2000)
        
        # å¼€å§‹è®­ç»ƒ
        print("\nå¼€å§‹è®­ç»ƒ...")
        start_time = time.time()
        
        model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            progress_bar=True
        )
        
        training_time = time.time() - start_time
        
        print("\n" + "=" * 60)
        print("ç®€åŒ–SB3 MARLè®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        print(f"è®­ç»ƒæ—¶é—´: {training_time/60:.2f} åˆ†é’Ÿ")
        print(f"æ€»æ­¥æ•°: {total_timesteps}")
        
        # å®‰å…¨åœ°è®¡ç®—å¹³å‡å¥–åŠ±
        if len(callback.episode_rewards) > 0:
            recent_rewards = callback.episode_rewards[-10:]
            avg_reward = np.mean(recent_rewards)
            print(f"å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        else:
            print("å¹³å‡å¥–åŠ±: æ— æ•°æ®")
        
        # ä¿å­˜æ¨¡å‹
        os.makedirs("models", exist_ok=True)
        model_path = "models/simple_sb3_marl_model"
        model.save(model_path)
        print(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜è®­ç»ƒç»Ÿè®¡
        training_stats = {
            "framework": "Stable-Baselines3-Simple",
            "algorithm": "PPO",
            "total_timesteps": total_timesteps,
            "training_time_minutes": training_time / 60,
            "episode_rewards": callback.episode_rewards,
            "episode_lengths": callback.episode_lengths,
            "final_avg_reward": float(np.mean(callback.episode_rewards[-10:])) if len(callback.episode_rewards) > 0 else 0.0,
            "agents": list(WORKSTATIONS.keys()),
            "config": {
                "learning_rate": learning_rate,
                "n_steps": 2048,
                "batch_size": 64,
                "n_epochs": 10
            }
        }
        
        os.makedirs("results", exist_ok=True)
        stats_file = f"results/simple_sb3_training_stats_{int(time.time())}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(training_stats, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜: {stats_file}")
        
        return model, training_stats
        
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def evaluate_model(model, num_episodes: int = 5):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"\nè¯„ä¼°æ¨¡å‹ ({num_episodes} å›åˆ)...")
    
    try:
        env = DummyVecEnv([create_env()])
        
        eval_rewards = []
        eval_lengths = []
        
        for episode in range(num_episodes):
            obs = env.reset()
            episode_reward = 0
            episode_length = 0
            done = False
            
            while not done and episode_length < 1000:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, done, info = env.step(action)
                
                episode_reward += reward[0]  # DummyVecEnvè¿”å›æ•°ç»„
                episode_length += 1
                
                done = done[0]  # DummyVecEnvè¿”å›æ•°ç»„
            
            eval_rewards.append(episode_reward)
            eval_lengths.append(episode_length)
            
            print(f"  å›åˆ {episode+1}: å¥–åŠ±={episode_reward:.2f}, é•¿åº¦={episode_length}")
        
        eval_results = {
            "mean_reward": float(np.mean(eval_rewards)),
            "std_reward": float(np.std(eval_rewards)),
            "mean_length": float(np.mean(eval_lengths)),
            "eval_rewards": eval_rewards,
            "eval_lengths": eval_lengths
        }
        
        print(f"\nè¯„ä¼°ç»“æœ:")
        print(f"  å¹³å‡å¥–åŠ±: {eval_results['mean_reward']:.2f} Â± {eval_results['std_reward']:.2f}")
        print(f"  å¹³å‡é•¿åº¦: {eval_results['mean_length']:.1f}")
        
        return eval_results
        
    except Exception as e:
        print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("Wå·¥å‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ")
    print("åŸºäºStable-Baselines3çš„ç®€åŒ–MARLå®ç°")
    print("=" * 60)
    
    try:
        # è®­ç»ƒæ¨¡å‹
        model, training_stats = train_simple_sb3_marl(
            total_timesteps=20000,  # é€‚ä¸­çš„è®­ç»ƒæ­¥æ•°
            learning_rate=3e-4
        )
        
        if model is not None:
            print("\nğŸ‰ ç®€åŒ–SB3 MARLè®­ç»ƒæˆåŠŸå®Œæˆï¼")
            
            # è¯„ä¼°æ¨¡å‹
            eval_results = evaluate_model(model, num_episodes=3)
            
            # æœ€ç»ˆæ€»ç»“
            print("\n" + "=" * 60)
            print("æœ€ç»ˆç»“æœæ€»ç»“")
            print("=" * 60)
            
            if eval_results:
                print(f"SB3 MARLå¹³å‡å¥–åŠ±: {eval_results['mean_reward']:.2f}")
            
            print("\nâœ… è¿™æ˜¯çœŸæ­£çš„MARLè®­ç»ƒï¼")
            print("âœ… ä½¿ç”¨å·¥ä¸šçº§PPOç®—æ³•")
            print("âœ… å¤šæ™ºèƒ½ä½“è”åˆåŠ¨ä½œç©ºé—´")
            print("âœ… Windowså®Œå…¨å…¼å®¹")
            print("âœ… æ— SuperSuitä¾èµ–é—®é¢˜")
            
        else:
            print("\nâŒ SB3è®­ç»ƒå¤±è´¥")
            
    except Exception as e:
        print(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 