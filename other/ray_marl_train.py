"""
çœŸæ­£çš„Ray RLlibå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒè„šæœ¬
è§£å†³Windowså…¼å®¹æ€§é—®é¢˜ï¼Œä½¿ç”¨æ­£ç¡®çš„Rayé…ç½®
"""
#è¿™ä¸ªè„šæœ¬ä½œä¸ºä»marlè®­ç»ƒä»windowsçš„ä¸å…¼å®¹è¿‡æ¸¡åˆ°wslçš„ç‰ˆæœ¬
import os
import sys
import time
import json
import tempfile
from typing import Dict, Any

# è®¾ç½®ç¯å¢ƒå˜é‡è§£å†³Windowså…¼å®¹æ€§
os.environ['RAY_DISABLE_IMPORT_WARNING'] = '1'
os.environ['RAY_DEDUP_LOGS'] = '0'

import ray
from ray import tune
from ray.rllib.algorithms.ppo import PPOConfig
from ray.rllib.env import PettingZooEnv
from ray.tune.registry import register_env
import numpy as np
import gymnasium as gym

# æ·»åŠ ç¯å¢ƒè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from environments.w_factory_env import make_parallel_env
from environments.w_factory_config import *

def env_creator(config):
    """ç¯å¢ƒåˆ›å»ºå‡½æ•°"""
    return PettingZooEnv(make_parallel_env(config))

# æ³¨å†Œç¯å¢ƒ
register_env("w_factory", env_creator)

def get_ray_config():
    """è·å–Rayåˆå§‹åŒ–é…ç½®"""
    # åˆ›å»ºä¸´æ—¶ç›®å½•ä½œä¸ºRayçš„å·¥ä½œç›®å½•
    temp_dir = tempfile.mkdtemp()
    
    return {
        "local_mode": False,  # å°è¯•ä½¿ç”¨åˆ†å¸ƒå¼æ¨¡å¼
        "ignore_reinit_error": True,
        "include_dashboard": False,  # ç¦ç”¨dashboardå‡å°‘èµ„æºå ç”¨
        "_temp_dir": temp_dir,
        "object_store_memory": 100000000,  # 100MBå¯¹è±¡å­˜å‚¨
        "num_cpus": 4,  # é™åˆ¶CPUä½¿ç”¨
    }

def get_training_config():
    """è·å–è®­ç»ƒé…ç½®"""
    
    config = (
        PPOConfig()
        .environment(
            env="w_factory",
            env_config={},
            disable_env_checking=True
        )
        .framework("tf2")
        .env_runners(
            # Windowså…¼å®¹æ€§é…ç½® (Ray 2.48+ API)
            num_env_runners=2,  # å‡å°‘runneræ•°é‡
            num_envs_per_env_runner=1,
            rollout_fragment_length=100,  # å‡å°‘ç‰‡æ®µé•¿åº¦
            batch_mode="complete_episodes"
        )
        .training(
            # PPOè®­ç»ƒå‚æ•°
            train_batch_size=1000,  # å‡å°æ‰¹æ¬¡å¤§å°
            sgd_minibatch_size=64,
            num_sgd_iter=5,
            lr=3e-4,
            gamma=0.99,
            lambda_=0.95,
            clip_param=0.2,
            vf_clip_param=10.0,
            entropy_coeff=0.01,
            vf_loss_coeff=0.5,
            # æ¢¯åº¦è£å‰ª
            grad_clip=0.5,
        )
        .multi_agent(
            # å¤šæ™ºèƒ½ä½“é…ç½®
            policies={
                "shared_policy": (
                    None,  # ä½¿ç”¨é»˜è®¤ç­–ç•¥ç±»
                    gym.spaces.Box(low=0.0, high=1.0, shape=(2,), dtype=np.float32),
                    gym.spaces.Discrete(2),
                    {}
                )
            },
            policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: "shared_policy",
            policies_to_train=["shared_policy"]
        )
        .resources(
            # èµ„æºé…ç½®
            num_gpus=0,  # ä½¿ç”¨CPU
            num_cpus_per_env_runner=1,
            num_gpus_per_env_runner=0
        )
        .evaluation(
            # è¯„ä¼°é…ç½®
            evaluation_interval=20,
            evaluation_duration=5,
            evaluation_num_env_runners=1,
            evaluation_config={
                "explore": False,
                "render_env": False,
            }
        )
        .debugging(
            # è°ƒè¯•é…ç½®
            log_level="ERROR",  # å‡å°‘æ—¥å¿—è¾“å‡º
        )
        .experimental(
            # å®éªŒæ€§é…ç½®
            _disable_preprocessor_api=True,
        )
    )
    
    return config

def run_ray_training(num_iterations: int = 50):
    """è¿è¡ŒRay RLlibè®­ç»ƒ"""
    print("=" * 60)
    print("Wå·¥å‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒ - Ray RLlibç‰ˆæœ¬")
    print("=" * 60)
    print("æ¡†æ¶: Ray RLlib")
    print("ç®—æ³•: PPO (Proximal Policy Optimization)")
    print("å¤šæ™ºèƒ½ä½“: ç­–ç•¥å…±äº«MAPPO")
    print("=" * 60)
    
    # éªŒè¯é…ç½®
    if not validate_config():
        print("é…ç½®éªŒè¯å¤±è´¥")
        return None
    
    try:
        # åˆå§‹åŒ–Ray
        ray_config = get_ray_config()
        print("åˆå§‹åŒ–Ray...")
        
        if ray.is_initialized():
            ray.shutdown()
        
        ray.init(**ray_config)
        print("âœ“ Rayåˆå§‹åŒ–æˆåŠŸ")
        
        # è·å–è®­ç»ƒé…ç½®
        training_config = get_training_config()
        
        # è®¾ç½®åœæ­¢æ¡ä»¶
        stop_config = {
            "training_iteration": num_iterations,
            "timesteps_total": num_iterations * 1000,
        }
        
        # åˆ›å»ºç»“æœç›®å½•
        results_dir = os.path.join(os.getcwd(), "ray_results")
        os.makedirs(results_dir, exist_ok=True)
        
        print(f"å¼€å§‹è®­ç»ƒ ({num_iterations} è½®)...")
        start_time = time.time()
        
        # è¿è¡Œè®­ç»ƒ
        tuner = tune.Tuner(
            "PPO",
            param_space=training_config.to_dict(),
            run_config=tune.RunConfig(
                name="w_factory_ray_marl",
                local_dir=results_dir,
                stop=stop_config,
                checkpoint_config=tune.CheckpointConfig(
                    checkpoint_frequency=10,
                    num_to_keep=3
                ),
                verbose=1  # å‡å°‘è¾“å‡º
            )
        )
        
        results = tuner.fit()
        
        # è·å–æœ€ä½³ç»“æœ
        best_result = results.get_best_result(
            metric="episode_reward_mean", 
            mode="max"
        )
        
        training_time = time.time() - start_time
        
        print("\n" + "=" * 60)
        print("Ray RLlibè®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        print(f"è®­ç»ƒæ—¶é—´: {training_time/60:.2f} åˆ†é’Ÿ")
        print(f"æœ€ä½³å¹³å‡å¥–åŠ±: {best_result.metrics['episode_reward_mean']:.2f}")
        print(f"æœ€ä½³æ£€æŸ¥ç‚¹: {best_result.checkpoint}")
        
        # ä¿å­˜ç»“æœæ‘˜è¦
        summary = {
            "framework": "Ray RLlib",
            "algorithm": "PPO/MAPPO",
            "training_time_minutes": training_time / 60,
            "best_reward": best_result.metrics['episode_reward_mean'],
            "checkpoint_path": str(best_result.checkpoint),
            "iterations": num_iterations,
            "agents": list(WORKSTATIONS.keys()),
            "config": "Windowså…¼å®¹æ¨¡å¼"
        }
        
        summary_file = os.path.join(results_dir, f"ray_training_summary_{int(time.time())}.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"è®­ç»ƒæ‘˜è¦å·²ä¿å­˜: {summary_file}")
        
        return best_result
        
    except Exception as e:
        print(f"Rayè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("é”™è¯¯è¯¦æƒ…:")
        import traceback
        traceback.print_exc()
        
        # å°è¯•fallbackåˆ°æœ¬åœ°æ¨¡å¼
        print("\nå°è¯•fallbackåˆ°æœ¬åœ°æ¨¡å¼...")
        return run_ray_training_local_mode(num_iterations)
    
    finally:
        # æ¸…ç†Ray
        if ray.is_initialized():
            ray.shutdown()

def run_ray_training_local_mode(num_iterations: int = 50):
    """ä½¿ç”¨æœ¬åœ°æ¨¡å¼è¿è¡ŒRayè®­ç»ƒï¼ˆfallbackæ–¹æ¡ˆï¼‰"""
    print("=" * 60)
    print("Ray RLlibè®­ç»ƒ - æœ¬åœ°æ¨¡å¼ (Fallback)")
    print("=" * 60)
    
    try:
        # æœ¬åœ°æ¨¡å¼åˆå§‹åŒ–Ray
        if ray.is_initialized():
            ray.shutdown()
        
        ray.init(local_mode=True, ignore_reinit_error=True)
        print("âœ“ Rayæœ¬åœ°æ¨¡å¼åˆå§‹åŒ–æˆåŠŸ")
        
        # æœ¬åœ°æ¨¡å¼é…ç½®ï¼ˆç®€åŒ–ï¼‰
        config = (
            PPOConfig()
            .environment(
                env="w_factory",
                env_config={},
                disable_env_checking=True
            )
            .framework("tf2")
            .rollouts(
                num_rollout_workers=0,  # æœ¬åœ°æ¨¡å¼ä¸ä½¿ç”¨worker
                rollout_fragment_length=200
            )
            .training(
                train_batch_size=500,
                sgd_minibatch_size=32,
                num_sgd_iter=3,
                lr=3e-4
            )
            .multi_agent(
                policies={
                    "shared_policy": (
                        None,
                        gym.spaces.Box(low=0.0, high=1.0, shape=(2,), dtype=np.float32),
                        gym.spaces.Discrete(2),
                        {}
                    )
                },
                policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: "shared_policy",
                policies_to_train=["shared_policy"]
            )
            .resources(num_gpus=0)
        )
        
        # è¿è¡Œè®­ç»ƒ
        results_dir = os.path.join(os.getcwd(), "ray_results_local")
        os.makedirs(results_dir, exist_ok=True)
        
        start_time = time.time()
        
        tuner = tune.Tuner(
            "PPO",
            param_space=config.to_dict(),
            run_config=tune.RunConfig(
                name="w_factory_ray_local",
                local_dir=results_dir,
                stop={"training_iteration": num_iterations},
                checkpoint_config=tune.CheckpointConfig(
                    checkpoint_frequency=10,
                    num_to_keep=2
                )
            )
        )
        
        results = tuner.fit()
        best_result = results.get_best_result(metric="episode_reward_mean", mode="max")
        
        training_time = time.time() - start_time
        
        print(f"\næœ¬åœ°æ¨¡å¼è®­ç»ƒå®Œæˆï¼")
        print(f"è®­ç»ƒæ—¶é—´: {training_time/60:.2f} åˆ†é’Ÿ")
        print(f"æœ€ä½³å¹³å‡å¥–åŠ±: {best_result.metrics['episode_reward_mean']:.2f}")
        
        return best_result
        
    except Exception as e:
        print(f"æœ¬åœ°æ¨¡å¼è®­ç»ƒä¹Ÿå¤±è´¥: {e}")
        return None
    
    finally:
        if ray.is_initialized():
            ray.shutdown()

def compare_with_baselines():
    """ä¸åŸºå‡†ç®—æ³•å¯¹æ¯”"""
    print("\nè¿è¡ŒåŸºå‡†ç®—æ³•å¯¹æ¯”...")
    
    try:
        from main import FIFOScheduler, SPTScheduler
        
        algorithms = {
            "FIFO": FIFOScheduler(),
            "SPT": SPTScheduler()
        }
        
        results = {}
        
        for name, scheduler in algorithms.items():
            print(f"è¿è¡Œ {name} ç®—æ³•...")
            start_time = time.time()
            stats = scheduler.schedule(BASE_ORDERS)
            end_time = time.time()
            
            stats['computation_time'] = end_time - start_time
            results[name] = stats
            
            print(f"  {name} - Makespan: {stats['makespan']:.2f}, "
                  f"å»¶æœŸ: {stats['total_tardiness']:.2f}")
        
        return results
        
    except Exception as e:
        print(f"åŸºå‡†ç®—æ³•å¯¹æ¯”å¤±è´¥: {e}")
        return {}

def main():
    """ä¸»å‡½æ•°"""
    print("Wå·¥å‚å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ")
    print("ä½¿ç”¨Ray RLlibæ¡†æ¶çš„æ­£å¼MARLè®­ç»ƒ")
    print("=" * 60)
    
    try:
        # è¿è¡ŒRayè®­ç»ƒ
        ray_result = run_ray_training(num_iterations=30)  # å‡å°‘è¿­ä»£æ¬¡æ•°ç”¨äºæµ‹è¯•
        
        if ray_result:
            print("\nğŸ‰ Ray RLlibè®­ç»ƒæˆåŠŸå®Œæˆï¼")
            
            # è¿è¡ŒåŸºå‡†å¯¹æ¯”
            baseline_results = compare_with_baselines()
            
            if baseline_results:
                print("\n" + "=" * 60)
                print("æ€§èƒ½å¯¹æ¯”")
                print("=" * 60)
                print(f"Ray MARLæœ€ä½³å¥–åŠ±: {ray_result.metrics['episode_reward_mean']:.2f}")
                
                for name, stats in baseline_results.items():
                    makespan = stats.get('makespan', 0)
                    tardiness = stats.get('total_tardiness', 0)
                    print(f"{name:10} - Makespan: {makespan:6.1f}, å»¶æœŸ: {tardiness:6.1f}")
            
            print("\nâœ… è¿™æ˜¯çœŸæ­£çš„Ray RLlib MARLè®­ç»ƒï¼")
            print("âœ… ä½¿ç”¨PPO/MAPPOç®—æ³•")
            print("âœ… å¤šæ™ºèƒ½ä½“ç­–ç•¥å…±äº«")
            print("âœ… ç¥ç»ç½‘ç»œç­–ç•¥å­¦ä¹ ")
            
        else:
            print("\nâŒ Rayè®­ç»ƒå¤±è´¥")
            print("å»ºè®®æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒæˆ–ä½¿ç”¨ç®€åŒ–è®­ç»ƒè„šæœ¬")
            
    except Exception as e:
        print(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 