# W工厂生产调度多智能体强化学习系统 - 项目说明文档

## 1. 项目概述 (Project Overview)

### 1.1 项目目标 (Project Goal)
本项目旨在为W工厂设计并实现一个**基于多智能体强化学习（MARL）的智能生产调度系统**。系统的核心目标是解决W工厂在"多品种、小批量"生产模式下面临的复杂调度难题，通过训练一组协同工作的智能体，实现对生产设备的最优调度，从而达到**最小化最大完工时间 (Makespan)**、**最大化设备利用率**和**最小化订单延期 (Tardiness)** 的综合目标。此外，项目还将研究系统在应对设备故障、紧急插单等动态事件时的鲁棒性，并探索将训练好的模型封装为实用决策支持工具的可能性。

### 1.2 核心问题 (Core Problem)
W工厂作为典型的现代家具制造企业，其生产线需要同时处理多种产品的订单，批量小、工艺路线各不相同。传统的静态调度方法（如基于规则的调度）难以适应市场的快速变化和生产过程中的不确定性。本项目要解决的核心问题是：如何利用人工智能方法，让工厂的每个生产单元（设备）能够像一个有经验的调度员一样，根据当前全局的生产状况（排队长度、设备状态、订单紧急程度等）做出实时的、局部的最优决策，从而在全局上涌现出高效、自适应的生产调度策略。

### 1.3 主要特性 (Key Features)
*   **多智能体强化学习核心 (MARL Core):** 以MARL为核心技术，将每个生产设备或工作中心建模为一个独立的智能体，通过协同学习实现分布式智能决策。
*   **高保真离散事件仿真 (High-Fidelity Simulation):** 基于SimPy构建W工厂的数字孪生环境，精确模拟产品工艺流、资源竞争等真实物理过程。
*   **动态事件响应 (Dynamic Event Handling):** 仿真环境内置了对设备随机故障、紧急订单插入等突发事件的模拟，用以训练和评估调度策略的鲁棒性。
*   **模块化与可扩展性 (Modular and Extensible):** 系统采用模块化设计，配置文件、仿真环境、训练脚本分离，便于后续的功能扩展（如引入AGV搬运、更复杂的决策逻辑等）。

---

## 2. 系统架构与技术栈 (System Architecture & Technical Stack)

### 2.1 系统架构 (Architecture)
本系统采用分层架构，实现了强化学习算法、标准环境接口与自定义仿真逻辑的解耦。

```
+------------------------------------------+
|           (训练引擎)            |
|       (PPO/MAPPO Algorithm etc.)       |
+------------------------------------------+
      ^          |
(State, Reward)  | (Action)
      |          v
+------------------------------------------+
|      PettingZoo (多智能体环境接口)        |
|          (w_factory_env.py)            |
+------------------------------------------+
      ^          |
 (SimState,     | (Control Decision)
  SimReward)     |
      |          v
+------------------------------------------+
|         SimPy (离散事件仿真核心)         |
|         (WFactorySim Class)            |
|  (资源、流程、队列、事件的物理模拟)       |
+------------------------------------------+
```

*   **Ray:** 作为最上层的"大脑"，负责执行MARL算法（如PPO），管理策略网络、执行训练循环、更新模型参数。
*   **PettingZoo (`w_factory_env.py`):** 作为中间的"桥梁"，将我们自定义的仿真环境封装成一个标准的多智能体接口，供RLlib直接调用。它负责转换状态、动作和奖励的格式。
*   **SimPy (`WFactorySim`):** 作为最底层的"物理世界"，负责模拟工厂的实际运作。它管理着时间、设备（资源）、队列和所有生产事件的发生。

### 2.2 技术栈 (Technical Stack)
*   **核心算法库:** `Ray`
*   **多智能体环境:** `pettingzoo`
*   **离散事件仿真:** `simpy`
*   **基础科学计算:** `numpy`
*   **强化学习环境标准:** `gymnasium`
*   **配置文件:** `environments/w_factory_config.py` - 所有仿真参数、设备信息、产品工艺路线的单一数据源。

---

## 3. 问题建模与仿真环境 (Problem Modeling & Simulation Environment)

我们将真实的工厂调度问题抽象为一个数字化的仿真环境。

*   **生产设备 (Resources):** 每个设备类型（如'带锯机'、'五轴加工中心'）在SimPy中被建模为一个 `simpy.Resource` 对象，其容量（`capacity`）由`w_factory_config.py`中的`count`决定。
*   **设备队列 (Queues):** 每个设备前都有一个输入队列，被建模为 `simpy.Store`，用于缓存等待加工的零件。
*   **产品与工艺流程 (Products & Processes):** 每个产品（如'黑胡桃木餐桌'）的生产过程由一系列预定义的工序构成。一个零件（`part`）在仿真中会按照其产品的工艺路线，依次进入对应设备的队列，请求设备资源，完成加工，然后前往下一个工序。
*   **动态事件 (Dynamic Events):**
    *   **设备故障:** 通过一个独立的SimPy进程实现，它会在仿真过程中随机选择一个设备，使其在一段时间内变为不可用状态。
    *   **紧急插单:** 通过在仿真中途向订单列表动态添加新订单来实现。

---

## 4. MARL智能体与策略设计 (MARL Agent & Policy Design)

*   **智能体 (Agents):** 每个工作站/设备类型被定义为一个独立的智能体（如`agent_带锯机`）。系统中的智能体总数由`WORKSTATIONS`的数量决定。
*   **状态空间 (State Space) :** 每个智能体的观测状态现在是一个丰富的15维向量，包含：
    *   **自身设备状态 (2维)**: 设备忙碌比率、故障状态
    *   **队列详细信息 (12维)**: 前3个零件的详细信息，每个零件包含4个特征：
        - 归一化剩余处理时间
        - 延期紧迫性 
        - 优先级
        - 是否为最后工序
    *   **下游工作站信息 (1维)**: 下游队列拥堵情况
*   **动作空间 (Action Space) :** 每个智能体的动作空间现在是 `Discrete(4)`：
    *   `0`: **空闲 (IDLE)** - 保持等待状态
    *   `1`: **加工第1个零件** - 处理队列中的第1个零件
    *   `2`: **加工第2个零件** - 处理队列中的第2个零件  
    *   `3`: **加工第3个零件** - 处理队列中的第3个零件
*   **奖励函数 (Reward Function):** 经过多版本迭代，我们设计了一套以“订单思维”为核心的、多层次的复杂奖励系统，旨在引导智能体关注最终的业务价值——准时交付完整订单。
    *   **核心奖励 (订单导向):**
        - **订单完成奖励 (`order_completion_reward`):** 给予一个巨大的正奖励 (5000)，其权重远超其他所有奖励，是引导策略收敛的最强信号。
        - **订单进度里程碑奖励 (`order_progress_bonus`):** 当订单完成度跨越25%、50%、75%等里程碑时给予显著奖励，解决了最终奖励过于稀疏的问题。
    *   **过程引导奖励 (启发式):**
        - **零件和工序奖励 (`part_completion_reward`, `step_reward`):** 给予非常微小的正奖励，仅用于在训练初期提供基本的探索方向。
        - **关键任务奖励 (`critical_path_bonus`, `bottleneck_priority_bonus`):** 对处理瓶颈工位或关键路径上（如即将到期）的零件给予额外奖励，将专家知识融入奖励函数。
    *   **严厉惩罚 (风险规避):**
        - **订单延期惩罚 (`order_tardiness_penalty`):** 对超出交付日期的订单给予与其延期时间相关的惩罚。
        - **订单遗弃惩罚 (`order_abandonment_penalty`):** 对长时间没有进展的订单给予巨额惩罚，防止智能体“饥饿”某些难处理的订单。
        - **未完成惩罚 (`incomplete_order_final_penalty`):** 在仿真结束时，对每一个未完成的订单都施以最严厉的惩罚。
*   **策略共享 (Policy Sharing):** 所有智能体共享同一套策略网络参数。这意味着我们只需要训练一个模型，但每个智能体可以根据自己独特的观测状态做出不同的决策，这极大地提高了训练效率。
---

## 5. 训练与评估流程 (Training & Evaluation Workflow)

*   **核心算法 (Algorithm):** 选用Ray中实现的**PPO (Proximal Policy Optimization)** 算法，其多智能体版本通常被称为**MAPPO**。
*   **训练流程 (Training Process):** 采用"从理想（静态）到现实（动态）"的递进式训练方法。
    1.  **静态策略训练:** 在一个没有随机事件的、确定的环境中进行训练，目标是让智能体学会基本的协同调度逻辑，并得到一个性能基准模型。
    2.  **动态策略训练:** 在静态模型的基础上，引入设备故障、紧急插单等动态事件，对策略进行**微调（Fine-tuning）**，以增强其应对不确定性的鲁棒性和泛化能力。
*   **评估指标 (Evaluation Metrics):**
    *   **最大完工时间 (Makespan):** 完成所有订单所需的总时长。
    *   **设备平均利用率 (Mean Equipment Utilization):** 所有设备在仿真期间的平均工作时间占比。
    *   **最大延迟 (Max Tardiness):** 所有订单中，延期最长的时间。

---

    ``` 